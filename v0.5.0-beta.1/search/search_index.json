{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"obstore","text":"<p>The simplest, highest-throughput <sup>1</sup> Python interface to S3, GCS, Azure Storage, &amp; other S3-compliant APIs, powered by Rust.</p> <ul> <li>Sync and async API with full type hinting.</li> <li>Streaming downloads with configurable chunking.</li> <li>Streaming uploads from files or async or sync iterators.</li> <li>Streaming list, with no need to paginate.</li> <li>Automatic multipart uploads for large file objects.</li> <li>File-like object API and fsspec integration.</li> <li>Easy to install with no required Python dependencies.</li> <li>Support for conditional put (\"put if not exists\"), as well as custom tags and attributes.</li> <li>Optionally return list results in Apache Arrow format, which is faster and more memory-efficient than materializing Python <code>dict</code>s.</li> <li>Zero-copy data exchange between Rust and Python via the buffer protocol.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install obstore using pip:</p> <pre><code>pip install obstore\n</code></pre> <p>Obstore is on conda-forge and can be installed using conda, mamba, or pixi. To install obstore using conda:</p> <pre><code>conda install -c conda-forge obstore\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation is available on the website.</p> <p>Head to Getting Started to dig in.</p> <ol> <li> <p>Benchmarking is ongoing, but preliminary results indicate roughly 9x higher throughput than fsspec and 2.8x higher throughput than aioboto3 for many concurrent, small, get requests from an async context.\u00a0\u21a9</p> </li> </ol>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#050-unreleased","title":"[0.5.0] - unreleased","text":""},{"location":"CHANGELOG/#breaking-changes","title":"Breaking changes","text":"<ul> <li>Removed <code>S3Store.from_session</code> and <code>S3Store._from_native</code>. Use credential providers instead.</li> </ul>"},{"location":"CHANGELOG/#040-2025-02-10","title":"[0.4.0] - 2025-02-10","text":""},{"location":"CHANGELOG/#new-features","title":"New Features","text":"<ul> <li>Support for pickling &amp; always manage store prefix by @kylebarron in developmentseed/obstore!185, developmentseed/obstore!239, developmentseed/obstore!223</li> <li>Add top-level <code>obstore.store.from_url</code> function, which delegates to each store's <code>from_url</code> constructor by @kylebarron in developmentseed/obstore!179, developmentseed/obstore!201</li> <li>Add option to return Arrow from <code>list_with_delimiter</code> by @kylebarron in developmentseed/obstore!238, developmentseed/obstore!244</li> <li>(Provisional) Enhanced loading of s3 credentials using <code>aws-config</code> crate by @kylebarron in developmentseed/obstore!203</li> <li>Access config values out from stores by @kylebarron in developmentseed/obstore!210</li> <li>LocalStore updates:</li> <li>Enable automatic cleanup for local store, when deleting directories by @kylebarron in developmentseed/obstore!175</li> <li>Optionally create root dir in LocalStore by @kylebarron in developmentseed/obstore!177</li> <li>File-like object updates:</li> <li>Add support for writable file-like objects by @kylebarron in developmentseed/obstore!167</li> <li> <p>Updates to readable file API:</p> <ul> <li>Support user-specified capacity in readable file-like objects by @kylebarron in developmentseed/obstore!174</li> <li>Expose <code>ObjectMeta</code> from readable file API by @kylebarron in developmentseed/obstore!176</li> <li>Merge <code>config</code> and <code>kwargs</code> and validate that no configuration parameters have been passed multiple times. (developmentseed/obstore!180, developmentseed/obstore!182, developmentseed/obstore!218)</li> <li>Add <code>__repr__</code> to <code>Bytes</code> class by @jessekrubin in developmentseed/obstore!173</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#breaking-changes_1","title":"Breaking changes","text":"<ul> <li><code>get_range</code>, <code>get_range_async</code>, <code>get_ranges</code>, and <code>get_ranges_async</code> now require named parameters for <code>start</code>, <code>end</code>, and <code>length</code> to make the semantics of the range request fully explicit. by @kylebarron in developmentseed/obstore!156</li> <li>Previously, individual stores did not manage a prefix path within the remote resource and <code>PrefixStore</code> was used to enable this. As of 0.4.0, <code>PrefixStore</code> was removed and all stores manage an optional mount prefix natively.</li> <li><code>obstore.open</code> has been renamed to <code>obstore.open_reader</code>.</li> <li>The <code>from_env</code> constructor has been removed from <code>S3Store</code>, <code>GCSStore</code>, and <code>AzureStore</code>. Now all constructors will read from environment variables. Use <code>__init__</code> or <code>from_url</code> instead. developmentseed/obstore!189</li> <li><code>obstore.exceptions.ObstoreError</code> renamed to <code>obstore.exceptions.BaseError</code> developmentseed/obstore!200</li> </ul>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug fixes","text":"<ul> <li>Fix pylance finding exceptions module by @kylebarron in developmentseed/obstore!183</li> <li>Allow passing in partial retry/backoff config by @kylebarron in developmentseed/obstore!205</li> <li>Fix returning None from async functions by @kylebarron in developmentseed/obstore!245</li> <li>Fix LocalStore range request past end of file, by @kylebarron in developmentseed/obstore!230</li> </ul>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Update wording for fsspec docstring by @kylebarron in developmentseed/obstore!195</li> <li>Add documentation about AWS region by @kylebarron in developmentseed/obstore!213</li> <li>Add developer documentation for functional API choice by @kylebarron in developmentseed/obstore!215</li> <li>Add <code>tqdm</code> progress bar example by @kylebarron in developmentseed/obstore!237</li> <li>Add contributor, performance, integrations docs by @kylebarron in developmentseed/obstore!227</li> <li>Add minio example by @kylebarron in developmentseed/obstore!241</li> </ul>"},{"location":"CHANGELOG/#other","title":"Other","text":"<ul> <li>Use manylinux 2_24 for aarch64 linux wheels by @kylebarron in developmentseed/obstore!225</li> </ul>"},{"location":"CHANGELOG/#new-contributors","title":"New Contributors","text":"<ul> <li>@vincentsarago made their first contribution in developmentseed/obstore!168</li> <li>@jessekrubin made their first contribution in developmentseed/obstore!173</li> </ul> <p>Full Changelog: github.com/developmentseed/obstore/compare/py-v0.3.0...py-v0.4.0</p>"},{"location":"CHANGELOG/#030-2025-01-16","title":"[0.3.0] - 2025-01-16","text":""},{"location":"CHANGELOG/#new-features_1","title":"New Features","text":"<ul> <li>Streaming uploads. <code>obstore.put</code> now supports iterable input, and <code>obstore.put_async</code> now supports async iterable input. This means you can pass the output of <code>obstore.get_async</code> directly into <code>obstore.put_async</code>. by @kylebarron in developmentseed/obstore!54</li> <li>Allow passing config options directly as keyword arguments. Previously, you had to pass all options as a <code>dict</code> into the <code>config</code> parameter. Now you can pass the elements directly to the store constructor. by @kylebarron in developmentseed/obstore!144</li> <li>Readable file-like objects. Open a readable file-like object with <code>obstore.open</code> and <code>obstore.open_async</code>. by @kylebarron in developmentseed/obstore!33</li> <li>Fsspec integration by @martindurant in developmentseed/obstore!63</li> <li>Prefix store by @kylebarron in developmentseed/obstore!117</li> <li>Python 3.13 wheels by @kylebarron in developmentseed/obstore!95</li> <li>Support python timedelta objects as duration config values by @kylebarron in developmentseed/obstore!146</li> <li>Add class constructors for store builders. Each store now has an <code>__init__</code> method, for easier construction. by @kylebarron in developmentseed/obstore!141</li> </ul>"},{"location":"CHANGELOG/#breaking-changes_2","title":"Breaking changes","text":"<ul> <li> <p><code>get_range</code>, <code>get_range_async</code>, <code>get_ranges</code>, and <code>get_ranges_async</code> now use start/end instead of offset/length. This is for consistency with the <code>range</code> option of <code>obstore.get</code>. by @kylebarron in developmentseed/obstore!71</p> </li> <li> <p>Return <code>Bytes</code> from <code>GetResult.bytes()</code> by @kylebarron in developmentseed/obstore!134</p> </li> </ul>"},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>boto3 region name can be None by @kylebarron in developmentseed/obstore!59</li> <li>add missing py.typed file by @gruebel in developmentseed/obstore!115</li> </ul>"},{"location":"CHANGELOG/#documentation_1","title":"Documentation","text":"<ul> <li>FastAPI/Starlette example by @kylebarron in developmentseed/obstore!145</li> <li>Add conda installation doc to README by @kylebarron in developmentseed/obstore!78</li> <li>Document suggested lifecycle rules for aborted multipart uploads by @kylebarron in developmentseed/obstore!139</li> <li>Add type hint and documentation for requester pays by @kylebarron in developmentseed/obstore!131</li> <li>Add note that S3Store can be constructed without boto3 by @kylebarron in developmentseed/obstore!108</li> <li>HTTP Store usage example by @kylebarron in developmentseed/obstore!142</li> </ul>"},{"location":"CHANGELOG/#whats-changed","title":"What's Changed","text":"<ul> <li>Improved docs for from_url by @kylebarron in developmentseed/obstore!138</li> <li>Implement read_all for async iterable by @kylebarron in developmentseed/obstore!140</li> </ul>"},{"location":"CHANGELOG/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@willemarcel made their first contribution in developmentseed/obstore!64</li> <li>@martindurant made their first contribution in developmentseed/obstore!63</li> <li>@norlandrhagen made their first contribution in developmentseed/obstore!107</li> <li>@gruebel made their first contribution in developmentseed/obstore!115</li> </ul> <p>Full Changelog: github.com/developmentseed/obstore/compare/py-v0.2.0...py-v0.3.0</p>"},{"location":"CHANGELOG/#020-2024-10-25","title":"[0.2.0] - 2024-10-25","text":""},{"location":"CHANGELOG/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Streaming list results. <code>list</code> now returns an async or sync generator. by @kylebarron in developmentseed/obstore!35</li> <li>Optionally return list result as arrow. The <code>return_arrow</code> keyword argument returns chunks from <code>list</code> as Arrow RecordBatches, which is faster than materializing Python dicts/lists. by @kylebarron in developmentseed/obstore!38</li> <li>Return buffer protocol object from <code>get_range</code> and <code>get_ranges</code>. Enables zero-copy data exchange from Rust into Python. by @kylebarron in developmentseed/obstore!39</li> <li>Add put options. Enables custom tags and attributes, as well as \"put if not exists\". by @kylebarron in developmentseed/obstore!50</li> <li>Rename to obstore by @kylebarron in developmentseed/obstore!45</li> <li>Add custom exceptions. by @kylebarron in developmentseed/obstore!48</li> </ul> <p>Full Changelog: github.com/developmentseed/obstore/compare/py-v0.1.0...py-v0.2.0</p>"},{"location":"CHANGELOG/#010-2024-10-21","title":"[0.1.0] - 2024-10-21","text":"<ul> <li>Initial release.</li> </ul>"},{"location":"alternatives/","title":"Alternatives to Obstore","text":""},{"location":"alternatives/#obstore-vs-fsspec","title":"Obstore vs fsspec","text":"<p>Fsspec is a generic specification for pythonic filesystems. It includes implementations for several cloud storage providers, including s3fs for Amazon S3, gcsfs for Google Cloud Storage, and adlfs for Azure Storage.</p>"},{"location":"alternatives/#api-differences","title":"API Differences","text":"<p>Like Obstore, fsspec presents an abstraction layer that allows you to write code once to interface to multiple cloud providers. However, the abstracted API each presents is different. Obstore tries to mirror native object store APIs while fsspec tries to mirror a file-like API.</p> <p>The upstream Rust library powering obstore, <code>object_store</code>, documents why it intentionally avoids a primary file-like API:</p> <p>The <code>ObjectStore</code> interface is designed to mirror the APIs of object stores and not filesystems, and thus has stateless APIs instead of cursor based interfaces such as <code>Read</code> or <code>Seek</code> available in filesystems.</p> <p>This design provides the following advantages:</p> <ul> <li>All operations are atomic, and readers cannot observe partial and/or failed writes</li> <li>Methods map directly to object store APIs, providing both efficiency and predictability</li> <li>Abstracts away filesystem and operating system specific quirks, ensuring portability</li> <li>Allows for functionality not native to filesystems, such as operation preconditions and atomic multipart uploads</li> </ul> <p>Obstore's primary APIs, like <code>get</code>, <code>put</code>, and <code>list</code>, mirror such object store APIs. However, if you still need to use a file-like API, Obstore provides such APIs with <code>open_reader</code> and <code>open_writer</code>.</p> <p>Obstore also includes a best-effort fsspec compatibility layer, which allows you to use obstore in applications that expect an fsspec-compatible API.</p>"},{"location":"alternatives/#performance","title":"Performance","text":"<p>Beyond API design, performance can also be a consideration. Initial benchmarks show that obstore's async API can provide 9x higher throughput than fsspec's async API.</p>"},{"location":"alternatives/#obstore-vs-boto3","title":"Obstore vs boto3","text":"<p>boto3 is the official Python client for working with AWS services, including S3.</p> <p>boto3 supports all features of S3, including some features that obstore doesn't provide, like creating or deleting buckets.</p> <p>However, boto3 is synchronous and specific to AWS. To support multiple clouds you'd need to use boto3 and another library and abstract away those differences yourself. With obstore you can interface with data in multiple clouds, changing only configuration settings.</p>"},{"location":"alternatives/#obstore-vs-aioboto3","title":"Obstore vs aioboto3","text":"<p>aioboto3 is an async Python client for S3, wrapping boto3 and aiobotocore.</p> <p>aioboto3 presents largely the same API as boto3, but async. As above, this means that it may support more S3-specific features than what obstore supports.</p> <p>But it's still specific to AWS, and in early benchmarks we've measured obstore to provide significantly higher throughput than aioboto3.</p>"},{"location":"alternatives/#obstore-vs-google-cloud-storage-python-client","title":"Obstore vs Google Cloud Storage Python Client","text":"<p>The official Google Cloud Storage Python client uses requests as its HTTP client. This means that the GCS Python client supports only synchronous requests.</p> <p>It also presents a Google-specific API, so you'd need to re-implement your code if you want to use multiple cloud providers.</p>"},{"location":"authentication/","title":"Authentication","text":"<p>Authentication tends to be among the trickiest but most important elements of connecting to object storage. Obstore supports various native and custom authentication methods.</p>"},{"location":"authentication/#native-authentication","title":"Native Authentication","text":"<p>\"Native\" authentication refers to authentication methods that are natively supported by the underlying Rust <code>object_store</code> library.</p> <p>Native authentication is most efficient, as obstore never needs to call into Python to update credentials.</p>"},{"location":"authentication/#order-of-application","title":"Order of application","text":"<p>Native authentication is checked in order:</p> <ul> <li>Environment variables</li> <li>Passed in via <code>config</code>/keyword parameters.</li> </ul> <p>So any parameters passed by the <code>config</code> parameter or by keyword parameters will override any values found from environment variables.</p>"},{"location":"authentication/#native-authentication-variants","title":"Native Authentication Variants","text":"<p>Note that many authentication variants are already supported natively.</p>"},{"location":"authentication/#aws","title":"AWS","text":"<ul> <li>Basic authentication, where an access key ID, secret access key, and optionally token are passed in via environment variables or configuration parameters.</li> <li>WebIdentity. This requires the <code>AWS_WEB_IDENTITY_TOKEN_FILE</code> and <code>AWS_ROLE_ARN</code> environment variables to be set. Additionally, <code>AWS_ROLE_SESSION_NAME</code> can be set to specify a session name.</li> <li>Container credentials. Ensure you pass <code>aws_container_credentials_relative_uri</code> to the <code>S3Store</code>.</li> <li>Instance credentials.</li> </ul> <p>(A transcription of this underlying code).</p>"},{"location":"authentication/#google-cloud-storage","title":"Google Cloud Storage","text":"<ul> <li>Service account credentials</li> <li>Application default credentials</li> <li>Instance credentials</li> </ul> <p>(A transcription of this underlying code).</p>"},{"location":"authentication/#azure","title":"Azure","text":"<ul> <li>Fabric OAuth2, using <code>fabric_token_service_url</code>, <code>fabric_workload_host</code>, <code>fabric_session_token</code>, and <code>fabric_cluster_identifier</code> passed in by the user</li> <li>Workload identity OAuth2, using a <code>client_id</code>, <code>tenant_id</code>, and <code>federated_token_file</code> passed in by the user</li> <li>OAuth2, using a <code>client_id</code>, <code>client_secret</code>, and <code>tenant_id</code> passed in by the user</li> <li>A SAS key passed in by the user.</li> <li>Azure CLI. (If you want to ensure the IMDS authentication is used below, pass <code>use_azure_cli=False</code> to <code>AzureStore</code>.)</li> <li>IMDS Managed Identity Provider.</li> </ul> <p>(A transcription of this underlying code).</p>"},{"location":"authentication/#credential-providers","title":"Credential Providers","text":"<p>Obstore allows for passing in credential providers. These are Python callbacks that allow for full control over credential generation. Passing in a credential provider will override any native credentials.</p>"},{"location":"authentication/#official-sdk-credential-providers","title":"\"Official\" SDK credential providers","text":""},{"location":"authentication/#boto3","title":"boto3","text":"<p>You can use the <code>Boto3CredentialProvider</code> to use <code>boto3.Session</code> to handle credentials.</p> <pre><code>from boto3 import Session\nfrom obstore.auth.boto3 import Boto3CredentialProvider\nfrom obstore.store import S3Store\n\nsession = Session(...)\ncredential_provider = Boto3CredentialProvider(session)\nstore = S3Store(\"bucket_name\", credential_provider=credential_provider)\n</code></pre> <p>Refer to <code>obstore.auth.boto3</code>.</p>"},{"location":"authentication/#googleauth","title":"google.auth","text":"<p>You can use the <code>GoogleAuthCredentialProvider</code> to use <code>google.auth</code> to handle credentials.</p> <pre><code>from obstore.auth.google import GoogleAuthCredentialProvider\nfrom obstore.store import GCSStore\n\ncredential_provider = GoogleAuthCredentialProvider(credentials=...)\nstore = GCSStore(\"bucket_name\", credential_provider=credential_provider)\n</code></pre> <p>Refer to <code>obstore.auth.google</code>.</p>"},{"location":"authentication/#custom-authentication","title":"Custom Authentication","text":"<p>There's a long tail of possible authentication mechanisms. Obstore allows you to provide your own custom authentication callback.</p> <p>You can provide either a synchronous or asynchronous callback for your custom authentication function.</p> <p>Note</p> <p>Provide an asynchronous credential provider for optimal performance.</p> <ul> <li>A custom AWS credential provider, passed in to <code>S3Store</code> must return an <code>S3Credential</code>.</li> <li>A custom GCS credential provider, passed in to <code>GCSStore</code> must return a <code>GCSCredential</code>.</li> <li>A custom Azure credential provider, passed in to <code>AzureStore</code> must return an <code>AzureCredential</code>.</li> </ul>"},{"location":"authentication/#basic-example","title":"Basic Example","text":"<p>The simplest custom credential provider can be just a synchronous or asynchronous function callback:</p> <pre><code>from datetime import datetime, timedelta, UTC\n\ndef get_credentials() -&gt; S3Credential:\n    return {\n        \"access_key_id\": \"...\",\n        \"secret_access_key\": \"...\",\n        # Not always required\n        \"token\": \"...\",\n        \"expires_at\": datetime.now(UTC) + timedelta(minutes=30),\n    }\n</code></pre> <p>Then just pass that function into <code>credential_provider</code>:</p> <pre><code>S3Store(..., credential_provider=get_credentials)\n</code></pre>"},{"location":"authentication/#advanced-example","title":"Advanced Example","text":"<p>More advanced credential providers can be class based. A class can act as a callable, just like a function callback, as long as it implements a <code>__call__</code> method.</p> <p>Below is an example custom credential provider for accessing NASA Earthdata.</p> <p>NASA Earthdata supports public in-region direct S3 access. This credential provider automatically manages refreshing the S3 credentials before they expire.</p> <p>Note that you must be in the same AWS region (<code>us-west-2</code>) to use this provider.</p> <pre><code>from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING\n\nimport requests\n\nif TYPE_CHECKING:\n    from obstore.store import S3Credential\n\nCREDENTIALS_API = \"https://archive.podaac.earthdata.nasa.gov/s3credentials\"\n\n\nclass NasaEarthdataCredentialProvider:\n    \"\"\"A credential provider for accessing [NASA Earthdata].\n\n    NASA Earthdata supports public [in-region direct S3\n    access](https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME). This\n    credential provider automatically manages the S3 credentials.\n\n    !!! note\n\n        Note that you must be in the same AWS region (`us-west-2`) to use the\n        credentials returned from this provider.\n\n    [NASA Earthdata]: https://www.earthdata.nasa.gov/\n    \"\"\"\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n    ) -&gt; None:\n        \"\"\"Create a new NasaEarthdataCredentialProvider.\n\n        Args:\n            username: Username to NASA Earthdata.\n            password: Password to NASA Earthdata.\n\n        \"\"\"\n        self.session = requests.Session()\n        self.session.auth = (username, password)\n\n    def __call__(self) -&gt; S3Credential:\n        \"\"\"Request updated credentials.\"\"\"\n        resp = self.session.get(CREDENTIALS_API, allow_redirects=True, timeout=15)\n        auth_resp = self.session.get(resp.url, allow_redirects=True, timeout=15)\n        creds = auth_resp.json()\n        return {\n            \"access_key_id\": creds[\"accessKeyId\"],\n            \"secret_access_key\": creds[\"secretAccessKey\"],\n            \"token\": creds[\"sessionToken\"],\n            \"expires_at\": datetime.fromisoformat(creds[\"expiration\"]),\n        }\n</code></pre> <p>Or asynchronously:</p> <pre><code>from __future__ import annotations\n\nimport json\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING\n\nfrom aiohttp import BasicAuth, ClientSession\n\nif TYPE_CHECKING:\n    from obstore.store import S3Credential\n\nCREDENTIALS_API = \"https://archive.podaac.earthdata.nasa.gov/s3credentials\"\n\n\nclass NasaEarthdataAsyncCredentialProvider:\n    \"\"\"A credential provider for accessing [NASA Earthdata].\n\n    NASA Earthdata supports public [in-region direct S3\n    access](https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME). This\n    credential provider automatically manages the S3 credentials.\n\n    !!! note\n\n        Note that you must be in the same AWS region (`us-west-2`) to use the\n        credentials returned from this provider.\n\n    [NASA Earthdata]: https://www.earthdata.nasa.gov/\n    \"\"\"\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n    ) -&gt; None:\n        \"\"\"Create a new NasaEarthdataAsyncCredentialProvider.\n\n        Args:\n            username: Username to NASA Earthdata.\n            password: Password to NASA Earthdata.\n\n        \"\"\"\n        self.session = ClientSession(auth=BasicAuth(username, password))\n\n    async def __call__(self) -&gt; S3Credential:\n        \"\"\"Request updated credentials.\"\"\"\n        async with self.session.get(CREDENTIALS_API, allow_redirects=True) as resp:\n            auth_url = resp.url\n        async with self.session.get(auth_url, allow_redirects=True) as auth_resp:\n            # Note: We parse the JSON manually instead of using `resp.json()` because\n            # the response mimetype is incorrectly set to text/html.\n            creds = json.loads(await auth_resp.text())\n        return {\n            \"access_key_id\": creds[\"accessKeyId\"],\n            \"secret_access_key\": creds[\"secretAccessKey\"],\n            \"token\": creds[\"sessionToken\"],\n            \"expires_at\": datetime.fromisoformat(creds[\"expiration\"]),\n        }\n\n    async def close(self):\n        \"\"\"Close the underlying session.\n\n        You should call this method after you've finished all obstore calls.\n        \"\"\"\n        await self.session.close()\n</code></pre> <p>Then call it with</p> <pre><code>credential_provider = NasaEarthdataCredentialProvider(username=\"...\", password=\"...\")\nstore = S3Store(\"bucket_name\", credential_provider=credential_provider)\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#list-objects","title":"List objects","text":"<p>Use the <code>obstore.list</code> method.</p> <pre><code>import obstore as obs\n\nstore = ... # store of your choice\n\n# Recursively list all files below the 'data' path.\n# 1. On AWS S3 this would be the 'data/' prefix\n# 2. On a local filesystem, this would be the 'data' directory\nprefix = \"data\"\n\n# Get a stream of metadata objects:\nlist_stream = obs.list(store, prefix)\n\n# Print info\nfor batch in list_stream:\n    for meta in batch:\n        print(f\"Name: {meta.path}, size: {meta.size}\")\n</code></pre>"},{"location":"cookbook/#list-objects-as-arrow","title":"List objects as Arrow","text":"<p>The default <code>list</code> behavior creates many small Python <code>dict</code>s. When listing a large bucket, generating these Python objects can add up to a lot of overhead.</p> <p>Instead, you may consider passing <code>return_arrow=True</code> to <code>obstore.list</code> to return each chunk of list results as an Arrow <code>RecordBatch</code>. This can be much faster than materializing Python objects for each row because Arrow can be shared zero-copy between Rust and Python.</p> <p>This Arrow integration requires the <code>arro3-core</code> dependency, a lightweight Arrow implementation. You can pass the emitted <code>RecordBatch</code> to <code>pyarrow</code> (zero-copy) by passing it to <code>pyarrow.record_batch</code> or to <code>polars</code> (also zero-copy) by passing it to <code>polars.DataFrame</code>.</p> <pre><code>import obstore as obs\n\nstore = ... # store of your choice\n\n# Get a stream of Arrow RecordBatches of metadata\nlist_stream = obs.list(store, prefix=\"data\", return_arrow=True)\nfor record_batch in list_stream:\n    print(record_batch.num_rows)\n</code></pre> <p>Here's a working example with the <code>sentinel-cogs</code> bucket in AWS Open Data:</p> <pre><code>import obstore as obs\nimport pandas as pd\nimport pyarrow as pa\nfrom obstore.store import S3Store\n\nstore = S3Store(\"sentinel-cogs\", region=\"us-west-2\", skip_signature=True)\nstream = obs.list(store, chunk_size=20, return_arrow=True)\n\nfor record_batch in stream:\n    # Convert to pyarrow (zero-copy), then to pandas for easy export to a\n    # Markdown table\n    df = pa.record_batch(record_batch).to_pandas()\n    print(df.iloc[:5].to_markdown(index=False))\n    break\n</code></pre> <p>The Arrow record batch looks like the following:</p> path last_modified size e_tag version sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/AOT.tif 2020-09-30 20:25:56+00:00 50510 \"2e24c2ee324ea478f2f272dbd3f5ce69\" sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/B01.tif 2020-09-30 20:22:48+00:00 1455332 \"a31b78e96748ccc2b21b827bef9850c1\" sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/B02.tif 2020-09-30 20:23:19+00:00 38149405 \"d7a92f88ad19761081323165649ce799-5\" sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/B03.tif 2020-09-30 20:23:52+00:00 38123224 \"4b938b6969f1c16e5dd685e6599f115f-5\" sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/B04.tif 2020-09-30 20:24:21+00:00 39033591 \"4781b581cd32b2169d0b3d22bf40a8ef-5\""},{"location":"cookbook/#fetch-objects","title":"Fetch objects","text":"<p>Use the <code>obstore.get</code> function to fetch data bytes from remote storage or files in the local filesystem.</p> <pre><code>import obstore as obs\n\nstore = ... # store of your choice\n\n# Retrieve a specific file\npath = \"data/file01.parquet\"\n\n# Fetch just the file metadata\nmeta = obs.head(store, path)\nprint(meta)\n\n# Fetch the object including metadata\nresult = obs.get(store, path)\nassert result.meta == meta\n\n# Buffer the entire object in memory\nbuffer = result.bytes()\nassert len(buffer) == meta.size\n\n# Alternatively stream the bytes from object storage\nstream = obs.get(store, path).stream()\n\n# We can now iterate over the stream\ntotal_buffer_len = 0\nfor chunk in stream:\n    total_buffer_len += len(chunk)\n\nassert total_buffer_len == meta.size\n</code></pre>"},{"location":"cookbook/#download-to-disk","title":"Download to disk","text":"<p>Using the response as an iterator ensures that we don't buffer the entire file into memory.</p> <pre><code>import obstore as obs\n\nresp = obs.get(store, path)\n\nwith open(\"output/file\", \"wb\") as f:\n    for chunk in resp:\n        f.write(chunk)\n</code></pre>"},{"location":"cookbook/#put-object","title":"Put object","text":"<p>Use the <code>obstore.put</code> function to atomically write data. <code>obstore.put</code> will automatically use multipart uploads for large input data.</p> <pre><code>import obstore as obs\n\nstore = ... # store of your choice\npath = \"data/file1\"\ncontent = b\"hello\"\nobs.put(store, path, content)\n</code></pre> <p>You can also upload local files:</p> <pre><code>from pathlib import Path\nimport obstore as obs\n\nstore = ... # store of your choice\npath = \"data/file1\"\ncontent = Path(\"path/to/local/file\")\nobs.put(store, path, content)\n</code></pre> <p>Or file-like objects:</p> <pre><code>import obstore as obs\n\nstore = ... # store of your choice\npath = \"data/file1\"\nwith open(\"path/to/local/file\", \"rb\") as content:\n    obs.put(store, path, content)\n</code></pre> <p>Or iterables:</p> <pre><code>import obstore as obs\n\ndef bytes_iter():\n    for i in range(5):\n        yield b\"foo\"\n\nstore = ... # store of your choice\npath = \"data/file1\"\ncontent = bytes_iter()\nobs.put(store, path, content)\n</code></pre> <p>Or async iterables:</p> <pre><code>import obstore as obs\n\nasync def bytes_stream():\n    for i in range(5):\n        yield b\"foo\"\n\nstore = ... # store of your choice\npath = \"data/file1\"\ncontent = bytes_stream()\nobs.put(store, path, content)\n</code></pre>"},{"location":"cookbook/#copy-objects-from-one-store-to-another","title":"Copy objects from one store to another","text":"<p>Perhaps you have data in one store, say AWS S3, that you need to copy to another, say Google Cloud Storage.</p>"},{"location":"cookbook/#in-memory","title":"In memory","text":"<p>Download the file, collect its bytes in memory, then upload it. Note that this will materialize the entire file in memory.</p> <pre><code>import obstore as obs\n\nstore1 = ... # store of your choice\nstore2 = ... # store of your choice\n\npath1 = \"data/file1\"\npath2 = \"data/file2\"\n\nbuffer = obs.get(store1, path1).bytes()\nobs.put(store2, path2, buffer)\n</code></pre>"},{"location":"cookbook/#local-file","title":"Local file","text":"<p>First download the file to disk, then upload it.</p> <pre><code>from pathlib import Path\nimport obstore as obs\n\nstore1 = ... # store of your choice\nstore2 = ... # store of your choice\n\npath1 = \"data/file1\"\npath2 = \"data/file2\"\n\nresp = obs.get(store1, path1)\n\nwith open(\"temporary_file\", \"wb\") as f:\n    for chunk in resp:\n        f.write(chunk)\n\n# Upload the path\nobs.put(store2, path2, Path(\"temporary_file\"))\n</code></pre>"},{"location":"cookbook/#streaming","title":"Streaming","text":"<p>It's easy to stream a download from one store directly as the upload to another. Only the given</p> <p>Note</p> <p>Using the async API is currently required to use streaming copies.</p> <pre><code>import obstore as obs\n\nstore1 = ... # store of your choice\nstore2 = ... # store of your choice\n\npath1 = \"data/file1\"\npath2 = \"data/file2\"\n\n# This only constructs the stream, it doesn't materialize the data in memory\nresp = await obs.get_async(store1, path1)\n# A streaming upload is created to copy the file to path2\nawait obs.put_async(store2, path2, resp, chunk_size=chunk_size)\n</code></pre> <p>Or, by customizing the chunk size and the upload concurrency you can control memory overhead.</p> <pre><code>resp = await obs.get_async(store1, path1)\nchunk_size = 5 * 1024 * 1024 # 5MB\nstream = resp.stream(min_chunk_size=chunk_size)\n\n# A streaming upload is created to copy the file to path2\nawait obs.put_async(\n    store2,\n    path2,\n    stream,\n    chunk_size=chunk_size,\n    max_concurrency=12\n)\n</code></pre> <p>This will start up to 12 concurrent uploads, each with around 5MB chunks, giving a total memory usage of up to roughly 60MB for this copy.</p> <p>Note</p> <p>You may need to increase the download timeout for large source files. The timeout defaults to 30 seconds, which may not be long enough to upload the file to the destination.</p> <p>You may set the <code>timeout</code> parameter in the <code>client_options</code> passed when creating the store.</p>"},{"location":"fsspec/","title":"fsspec Integration","text":"<p>Obstore provides native integration with the fsspec ecosystem.</p> <p>The fsspec integration is best effort and may not provide the same performance as the rest of obstore. Where possible, implementations should use the underlying <code>obstore</code> APIs directly. If you find any bugs with this integration, please file an issue.</p>"},{"location":"fsspec/#usage","title":"Usage","text":""},{"location":"fsspec/#direct-class-usage","title":"Direct class usage","text":"<p>Construct an fsspec-compatible filesystem with <code>FsspecStore</code>. This implements <code>AbstractFileSystem</code>, so you can use it wherever an API expects an fsspec-compatible filesystem.</p> <pre><code>from obstore.fsspec import FsspecStore\n\nfs = FsspecStore(\"s3\", region=\"us-west-2\", skip_signature=True)\nprefix = (\n    \"s3://sentinel-cogs/sentinel-s2-l2a-cogs/12/S/UF/2022/6/S2B_12SUF_20220609_0_L2A/\"\n)\nitems = fs.ls(prefix)\n# [{'name': 'sentinel-cogs/sentinel-s2-l2a-cogs/12/S/UF/2022/6/S2B_12SUF_20220609_0_L2A/AOT.tif',\n#   'size': 80689,\n#   'type': 'file',\n#   'e_tag': '\"c93b0f6b0e2cf8e375968f41161f9df7\"'},\n#   ...\n</code></pre> <p>If you need a readable or writable file-like object, you can call the <code>open</code> method provided on <code>FsspecStore</code>, or you may construct a <code>BufferedFile</code> directly.</p> <pre><code>from obstore.fsspec import FsspecStore\n\nfs = FsspecStore(\"s3\", region=\"us-west-2\", skip_signature=True)\n\nwith fs.open(\n    \"s3://sentinel-cogs/sentinel-s2-l2a-cogs/12/S/UF/2022/6/S2B_12SUF_20220609_0_L2A/thumbnail.jpg\",\n) as file:\n    content = file.read()\n</code></pre> <p>Using the <code>FsspecStore</code> class directly may be preferred because the type hinting should work automatically, which may help IDEs like VSCode suggest valid keyword parameters.</p>"},{"location":"fsspec/#register-as-a-global-handler","title":"Register as a global handler","text":"<p>Use <code>register</code> to register obstore as the default handler for various protocols. Then use <code>fsspec.filesystem</code> to create an fsspec filesystem object for a specific protocol. Or use <code>fsspec.open</code> to open a file given a URL.</p> <pre><code>import fsspec\nfrom obstore.fsspec import register\n\n# Register obstore as the default handler for all protocols supported by\n# obstore.\n# You may wish to register only specific protocols, instead.\nregister()\n\n# Create a new fsspec filesystem for the given protocol\nfs = fsspec.filesystem(\"https\")\ncontent = fs.cat_file(\"https://example.com/\")\n\n# Or, open the file directly\nurl = \"https://github.com/opengeospatial/geoparquet/raw/refs/heads/main/examples/example.parquet\"\nwith fsspec.open(url) as file:\n    content = file.read()\n</code></pre>"},{"location":"fsspec/#store-configuration","title":"Store configuration","text":"<p>Some stores may require configuration. You may pass configuration parameters to the <code>FsspecStore</code> constructor directly. Or, if you're using <code>fsspec.filesystem</code>, you may pass configuration parameters to that call, which will pass parameters down to the <code>FsspecStore</code> constructor internally.</p> <pre><code>from obstore.fsspec import FsspecStore\n\nfs = FsspecStore(\"s3\", region=\"us-west-2\", skip_signature=True)\n</code></pre> <p>Or, with <code>fsspec.filesystem</code>:</p> <pre><code>import fsspec\n\nfrom obstore.fsspec import register\n\nregister(\"s3\")\n\nfs = fsspec.filesystem(\"s3\", region=\"us-west-2\", skip_signature=True)\n</code></pre>"},{"location":"fsspec/#type-hinting","title":"Type hinting","text":"<p>The fsspec API is not conducive to type checking. The easiest way to get type hinting for parameters is to use <code>FsspecStore</code> to construct fsspec-compatible stores instead of <code>fsspec.filesystem</code>.</p> <p><code>fsspec.open</code> and <code>fsspec.filesystem</code> take arbitrary keyword arguments that they pass down to the underlying store, and these pass-through arguments are not typed.</p> <p>However, it is possible to get type checking of store configuration by defining config parameters as a dictionary:</p> <pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport fsspec\n\nfrom obstore.fsspec import register\n\nif TYPE_CHECKING:\n    from obstore.store import S3ConfigInput\n\nregister(\"s3\")\n\nconfig: S3ConfigInput = {\"region\": \"us-west-2\", \"skip_signature\": True}\nfs = fsspec.filesystem(\"s3\", config=config)\n</code></pre> <p>Then your type checker will validate that the <code>config</code> dictionary is compatible with <code>S3ConfigInput</code>. VSCode also provides auto suggestions for parameters:</p> <p></p> <p>Note</p> <p><code>S3ConfigInput</code> is a \"type-only\" construct, and so it needs to be imported from within an <code>if TYPE_CHECKING</code> block. Additionally, <code>from __future__ import annotations</code> must be at the top of the file.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>There are two parts to <code>obstore</code>:</p> <ol> <li>Constructing a <code>Store</code>, a representation of a remote object store with configuration and credentials.</li> <li>Interacting with the <code>Store</code> to download, upload, move, and delete objects.</li> </ol>"},{"location":"getting-started/#constructing-a-store","title":"Constructing a store","text":"<p>You can use the top-level <code>from_url</code> function to construct a store from a top-level URL. Among others this supports <code>s3://bucket/path</code>, <code>gs://bucket/path</code>, <code>az://account/container/path</code>, and <code>https://mydomain/path</code>.</p> <p>Alternatively, you can construct a store directly:</p> <ul> <li><code>S3Store</code>: Configure a connection to Amazon S3.</li> <li><code>GCSStore</code>: Configure a connection to Google Cloud Storage.</li> <li><code>AzureStore</code>: Configure a connection to Microsoft Azure Blob Storage.</li> <li><code>HTTPStore</code>: Configure a connection to a generic HTTP server</li> <li><code>LocalStore</code>: Local filesystem storage providing the same object store interface.</li> <li><code>MemoryStore</code>: A fully in-memory implementation of ObjectStore.</li> </ul> <p>Each store concept has a variety of constructors, and a host of configuration options.</p> <p>Note that each store is scoped to one bucket, so you'll have to create a separate store instance per bucket, even if they're in the same region.</p> <p>Example:</p> <p>For example, multiple ways to create an anonymous <code>S3Store</code> client (without any credentials, for use with fully public buckets):</p> <pre><code>from obstore.store import S3Store, from_url\n\nfrom_url(\"s3://bucket-name\", region=\"us-east-1\", skip_signature=True)\nfrom_url(\"https://bucket-name.s3.us-east-1.amazonaws.com\", skip_signature=True)\nstore = S3Store(\"bucket-name\", region=\"us-east-1\", skip_signature=True)\n</code></pre> <p>The process is similar for <code>GCSStore</code> and <code>AzureStore</code>.</p>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>Each store class above has its own store-specific configuration. Elements of the store configuration can be passed as keyword arguments or as a dictionary through the <code>config</code> named parameter.</p> <ul> <li><code>S3Config</code>: Configuration parameters for Amazon S3.</li> <li><code>GCSConfig</code>: Configuration parameters for Google Cloud Storage.</li> <li><code>AzureConfig</code>: Configuration parameters for Microsoft Azure Blob Storage.</li> </ul> <p>Additionally, each store accepts parameters for the underlying HTTP client (<code>ClientConfig</code>) and parameters for retrying requests that error (<code>RetryConfig</code>).</p>"},{"location":"getting-started/#interacting-with-a-store","title":"Interacting with a store","text":"<p>All operations for interacting with a store are exported as top-level functions (not methods on the <code>store</code> object):</p> <ul> <li><code>copy</code>: Copy an object from one path to another in the same object store.</li> <li><code>delete</code>: Delete the object at the specified location.</li> <li><code>get</code>: Return the bytes that are stored at the specified location.</li> <li><code>head</code>: Return the metadata for the specified location</li> <li><code>list</code>: List all the objects with the given prefix.</li> <li><code>put</code>: Save the provided buffer to the specified location.</li> <li><code>rename</code>: Move an object from one path to another in the same object store.</li> </ul> <p>There are a few additional APIs useful for specific use cases:</p> <ul> <li><code>get_range</code>: Get a specific byte range from a file.</li> <li><code>get_ranges</code>: Get multiple byte ranges from a single file.</li> <li><code>list_with_delimiter</code>: List objects within a specific directory.</li> <li><code>sign</code>: Create a signed URL.</li> </ul> <p>File-like object support is also provided:</p> <ul> <li><code>open_reader</code>: Open a remote object as a readable file-like object, similar to a Python <code>BufferedReader</code>.</li> <li><code>open_writer</code>: Open a remote object as a writable file-like object, similar to a Python <code>BufferedWriter</code></li> <li><code>FsspecStore</code> adapter for use with <code>fsspec</code>.</li> </ul> <p>All operations have a comparable async method with the same name plus an <code>_async</code> suffix.</p>"},{"location":"getting-started/#example","title":"Example","text":"<pre><code>import obstore as obs\n\nstore = obs.store.MemoryStore()\n\nobs.put(store, \"file.txt\", b\"hello world!\")\nresponse = obs.get(store, \"file.txt\")\nresponse.meta\n# {'path': 'file.txt',\n#  'last_modified': datetime.datetime(2024, 10, 21, 16, 19, 45, 102620, tzinfo=datetime.timezone.utc),\n#  'size': 12,\n#  'e_tag': '0',\n#  'version': None}\nassert response.bytes() == b\"hello world!\"\n\nbyte_range = obs.get_range(store, \"file.txt\", start=0, end=5)\nassert byte_range == b\"hello\"\n\nobs.copy(store, \"file.txt\", \"other.txt\")\nassert obs.get(store, \"other.txt\").bytes() == b\"hello world!\"\n</code></pre> <p>All of these methods also have <code>async</code> counterparts, suffixed with <code>_async</code>.</p> <pre><code>import obstore as obs\n\nstore = obs.store.MemoryStore()\n\nawait obs.put_async(store, \"file.txt\", b\"hello world!\")\nresponse = await obs.get_async(store, \"file.txt\")\nresponse.meta\n# {'path': 'file.txt',\n#  'last_modified': datetime.datetime(2024, 10, 21, 16, 20, 36, 477418, tzinfo=datetime.timezone.utc),\n#  'size': 12,\n#  'e_tag': '0',\n#  'version': None}\nassert await response.bytes_async() == b\"hello world!\"\n\nbyte_range = await obs.get_range_async(store, \"file.txt\", start=0, end=5)\nassert byte_range == b\"hello\"\n\nawait obs.copy_async(store, \"file.txt\", \"other.txt\")\nresp = await obs.get_async(store, \"other.txt\")\nassert await resp.bytes_async() == b\"hello world!\"\n</code></pre>"},{"location":"integrations/","title":"External Integrations","text":"<p>Various integrations with external libraries exist:</p> <ul> <li><code>dagster</code>: Refer to <code>dagster-obstore</code>.</li> <li><code>fsspec</code>: Use the <code>obstore.fsspec</code> module.</li> <li><code>zarr-python</code>: In progress.</li> </ul> <p>Know of an integration that doesn't exist here? Edit this document.</p>"},{"location":"performance/","title":"Performance","text":"<p>Last edited 2025-02-05.</p> <p>Performance is a primary goal of Obstore. Benchmarking is still ongoing, so this document is a mix of what we've learned so far and our untested expectations.</p> <p>tl;dr: Obstore can't magically make your networking hardware faster, but it can reduce overhead, and in cases where that overhead is the limiting factor it can better utilize your available hardware and improve performance.</p>"},{"location":"performance/#non-performance-benefits","title":"Non-performance benefits","text":"<p>Before we get into the weeds of performance, keep in mind that performance is not the only feature of Obstore. There's a strong focus on developer experience as well:</p> <ul> <li>Simple to install with no required Python dependencies.</li> <li>Works the same across AWS S3, Google Cloud Storage, and Azure Storage.</li> <li>Full type hinting, including all store configuration and operations.</li> <li>Downloads that automatically act as iterators and uploads that automatically accept iterators.</li> <li>Automatic pagination of <code>list</code> calls behind the scenes</li> </ul> <p>So you might enjoy using Obstore even in a case where it only marginally improves your performance.</p>"},{"location":"performance/#defining-performance","title":"Defining performance","text":"<p>\"Fast\" can have several definitions in a networking context.</p> <ul> <li>Download latency: the time until the first byte of a download is received.</li> <li>Single-request throughput: the download or upload bytes per second of a single request.</li> <li>Many-request throughput: the combined download or upload bytes per second of multiple concurrent requests.</li> </ul> <p>Furthermore, performance can be different when using obstore's synchronous or asynchronous API.</p> <p>Let's consider the areas where we expect improved, possibly-improved, and equal performance.</p>"},{"location":"performance/#improved-performance","title":"Improved performance","text":"<p>Many-request throughput with the asynchronous API is the primary place where we expect significantly improved performance. Especially when making many requests of relatively small files, we find that obstore can provide significantly higher throughput.</p> <p>For example, preliminary results indicate roughly 9x higher throughput than fsspec and 2.8x higher throughput than aioboto3. That specific benchmark considered fetching the first 16KB of a file many times from an async context.</p>"},{"location":"performance/#possibly-improved-performance","title":"Possibly improved performance","text":"<p>Using the synchronous API. We haven't benchmarked the synchronous API. However, we do release the Python Global Interpreter Lock (GIL) for all synchronous operations, so it may perform better in a thread pool than other Python request libraries.</p>"},{"location":"performance/#equal-performance","title":"Equal performance","text":"<ul> <li> <p>Single-request throughput: if you're making one request, the limiting factor is likely network conditions, not Python overhead, so it's unlikely that obstore will be faster.</p> <p>Keep in mind, however, that what looks like a single request may actually be multiple requests under the hood. <code>obstore.put</code> will use multipart uploads by default, meaning that various parts of a file will be uploaded concurrently, and there may be efficiency gains here. - Latency: this is primarily driven by hardware and network conditions, and we expect Obstore to have similar latency as other Python request libraries.</p> </li> </ul>"},{"location":"performance/#future-research","title":"Future research","text":"<p>In the future, we'd like to benchmark:</p> <ul> <li>Alternate Python event loops, e.g. <code>uvloop</code></li> <li>The obstore synchronous API</li> </ul> <p>If you have any interest in collaborating on this, open an issue.</p>"},{"location":"advanced/pickle/","title":"Pickle Support","text":"<p>Obstore supports pickle, which is commonly used from inside Dask and similar libraries to manage state across distributed workers.</p>"},{"location":"advanced/pickle/#not-for-persistence","title":"Not for persistence","text":"<p>The format used to pickle stores may change across versions. Pickle support is intended for execution frameworks like Dask that need to share state across workers that are using the same environments, including the same version of Python and obstore.</p>"},{"location":"advanced/pickle/#middlewares","title":"Middlewares","text":"<p>Obstore expects to support some sort of middleware in the future, such as for recording request metrics. It's unlikely that middlewares will support pickle.</p>"},{"location":"advanced/pickle/#memorystore-not-implemented","title":"MemoryStore not implemented","text":"<p>Pickling isn't supported for <code>MemoryStore</code> because we don't have a way to access the raw state of the store.</p>"},{"location":"advanced/pickle/#custom-authentication","title":"Custom authentication","text":"<p>As of obstore 0.5.0, custom authentication is supported.</p> <p>Pickling works with a custom authentication provider so long as that Python callback can itself be pickled.</p> <p>So, for example, the boto3 provider cannot be pickled, because a <code>boto3.session.Session</code> cannot be pickled, but a simple function can be.</p>"},{"location":"api/attributes/","title":"Attributes","text":""},{"location":"api/attributes/#obstore.Attribute","title":"obstore.Attribute  <code>module-attribute</code>","text":"<pre><code>Attribute: TypeAlias = (\n    Literal[\n        \"Content-Disposition\",\n        \"Content-Encoding\",\n        \"Content-Language\",\n        \"Content-Type\",\n        \"Cache-Control\",\n    ]\n    | str\n)\n</code></pre> <p>Additional object attribute types.</p> <ul> <li> <p><code>\"Content-Disposition\"</code>: Specifies how the object should be handled by a browser.</p> <p>See Content-Disposition.</p> </li> <li> <p><code>\"Content-Encoding\"</code>: Specifies the encodings applied to the object.</p> <p>See Content-Encoding.</p> </li> <li> <p><code>\"Content-Language\"</code>: Specifies the language of the object.</p> <p>See Content-Language.</p> </li> <li> <p><code>\"Content-Type\"</code>: Specifies the MIME type of the object.</p> <p>This takes precedence over any client configuration.</p> <p>See Content-Type.</p> </li> <li> <p><code>\"Cache-Control\"</code>: Overrides cache control policy of the object.</p> <p>See Cache-Control.</p> </li> </ul> <p>Any other string key specifies a user-defined metadata field for the object.</p>"},{"location":"api/attributes/#obstore.Attributes","title":"obstore.Attributes  <code>module-attribute</code>","text":"<pre><code>Attributes: TypeAlias = dict[Attribute, str]\n</code></pre> <p>Additional attributes of an object</p> <p>Attributes can be specified in <code>put</code>/<code>put_async</code> and retrieved from <code>get</code>/<code>get_async</code>.</p> <p>Unlike ObjectMeta, Attributes are not returned by listing APIs</p>"},{"location":"api/copy/","title":"Copy","text":""},{"location":"api/copy/#obstore.copy","title":"obstore.copy","text":"<pre><code>copy(\n    store: ObjectStore, from_: str, to: str, *, overwrite: bool = True\n) -&gt; None\n</code></pre> <p>Copy an object from one path to another in the same object store.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>from_</code>               (<code>str</code>)           \u2013            <p>Source path</p> </li> <li> <code>to</code>               (<code>str</code>)           \u2013            <p>Destination path</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>overwrite</code>               (<code>bool</code>)           \u2013            <p>If <code>True</code>, if there exists an object at the destination, it will     be overwritten.</p> <p>If <code>False</code>: will copy only if destination is empty. Performs an atomic operation if the underlying object storage supports it. If atomic operations are not supported by the underlying object storage (like S3) it will return an error.</p> <p>Will return an error if the destination already has an object.</p> </li> </ul>"},{"location":"api/copy/#obstore.copy_async","title":"obstore.copy_async  <code>async</code>","text":"<pre><code>copy_async(\n    store: ObjectStore, from_: str, to: str, *, overwrite: bool = True\n) -&gt; None\n</code></pre> <p>Call <code>copy</code> asynchronously.</p> <p>Refer to the documentation for copy.</p>"},{"location":"api/delete/","title":"Delete","text":""},{"location":"api/delete/#obstore.delete","title":"obstore.delete","text":"<pre><code>delete(store: ObjectStore, paths: str | Sequence[str]) -&gt; None\n</code></pre> <p>Delete the object at the specified location(s).</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>paths</code>               (<code>str | Sequence[str]</code>)           \u2013            <p>The path or paths within the store to delete.</p> <p>When supported by the underlying store, this method will use bulk operations that delete more than one object per a request.</p> <p>If the object did not exist, the result may be an error or a success, depending on the behavior of the underlying store. For example, local filesystems, GCP, and Azure return an error, while S3 and in-memory will return Ok.</p> </li> </ul>"},{"location":"api/delete/#obstore.delete_async","title":"obstore.delete_async  <code>async</code>","text":"<pre><code>delete_async(store: ObjectStore, paths: str | Sequence[str]) -&gt; None\n</code></pre> <p>Call <code>delete</code> asynchronously.</p> <p>Refer to the documentation for delete.</p>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#obstore.exceptions","title":"obstore.exceptions","text":""},{"location":"api/exceptions/#obstore.exceptions.AlreadyExistsError","title":"AlreadyExistsError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the object already exists.</p>"},{"location":"api/exceptions/#obstore.exceptions.BaseError","title":"BaseError","text":"<p>               Bases: <code>Exception</code></p> <p>The base exception class.</p>"},{"location":"api/exceptions/#obstore.exceptions.GenericError","title":"GenericError","text":"<p>               Bases: <code>BaseError</code></p> <p>A fallback error type when no variant matches.</p>"},{"location":"api/exceptions/#obstore.exceptions.InvalidPathError","title":"InvalidPathError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error for invalid path.</p>"},{"location":"api/exceptions/#obstore.exceptions.JoinError","title":"JoinError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when <code>tokio::spawn</code> failed.</p>"},{"location":"api/exceptions/#obstore.exceptions.NotFoundError","title":"NotFoundError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the object is not found at given location.</p>"},{"location":"api/exceptions/#obstore.exceptions.NotModifiedError","title":"NotModifiedError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the object at the location isn't modified.</p>"},{"location":"api/exceptions/#obstore.exceptions.NotSupportedError","title":"NotSupportedError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the attempted operation is not supported.</p>"},{"location":"api/exceptions/#obstore.exceptions.PermissionDeniedError","title":"PermissionDeniedError","text":"<p>               Bases: <code>BaseError</code></p> <p>Permission denied.</p> <p>Error when the used credentials don't have enough permission to perform the requested operation.</p>"},{"location":"api/exceptions/#obstore.exceptions.PreconditionError","title":"PreconditionError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the required conditions failed for the operation.</p>"},{"location":"api/exceptions/#obstore.exceptions.UnauthenticatedError","title":"UnauthenticatedError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when the used credentials lack valid authentication.</p>"},{"location":"api/exceptions/#obstore.exceptions.UnknownConfigurationKeyError","title":"UnknownConfigurationKeyError","text":"<p>               Bases: <code>BaseError</code></p> <p>Error when a configuration key is invalid for the store used.</p>"},{"location":"api/file/","title":"File-like Object","text":"<p>Native support for reading from object stores as a file-like object.</p> <p>Use <code>obstore.open_reader</code> or <code>obstore.open_reader_async</code> to open readable files. Use <code>obstore.open_writer</code> or <code>obstore.open_writer_async</code> to open writable files.</p>"},{"location":"api/file/#obstore.open_reader","title":"obstore.open_reader","text":"<pre><code>open_reader(\n    store: ObjectStore, path: str, *, buffer_size: int = 1024 * 1024\n) -&gt; ReadableFile\n</code></pre> <p>Open a readable file object from the specified location.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>buffer_size</code>               (<code>int</code>)           \u2013            <p>The minimum number of bytes to read in a single request. Up to <code>buffer_size</code> bytes will be buffered in memory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ReadableFile</code>           \u2013            <p>ReadableFile</p> </li> </ul>"},{"location":"api/file/#obstore.open_reader_async","title":"obstore.open_reader_async  <code>async</code>","text":"<pre><code>open_reader_async(\n    store: ObjectStore, path: str, *, buffer_size: int = 1024 * 1024\n) -&gt; AsyncReadableFile\n</code></pre> <p>Call <code>open_reader</code> asynchronously, returning a readable file object with asynchronous operations.</p> <p>Refer to the documentation for open_reader.</p>"},{"location":"api/file/#obstore.open_writer","title":"obstore.open_writer","text":"<pre><code>open_writer(\n    store: ObjectStore,\n    path: str,\n    *,\n    attributes: Attributes | None = None,\n    buffer_size: int = 10 * 1024 * 1024,\n    tags: dict[str, str] | None = None,\n    max_concurrency: int = 12,\n) -&gt; WritableFile\n</code></pre> <p>Open a writable file object at the specified location.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Attributes | None</code>)           \u2013            <p>Provide a set of <code>Attributes</code>. Defaults to <code>None</code>.</p> </li> <li> <code>buffer_size</code>               (<code>int</code>)           \u2013            <p>The underlying buffer size to use. Up to <code>buffer_size</code> bytes will be buffered in memory. If <code>buffer_size</code> is exceeded, data will be uploaded as a multipart upload in chunks of <code>buffer_size</code>.</p> </li> <li> <code>tags</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Provide tags for this object. Defaults to <code>None</code>.</p> </li> <li> <code>max_concurrency</code>               (<code>int</code>)           \u2013            <p>The maximum number of chunks to upload concurrently. Defaults to 12.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>WritableFile</code>           \u2013            <p>ReadableFile</p> </li> </ul>"},{"location":"api/file/#obstore.open_writer_async","title":"obstore.open_writer_async","text":"<pre><code>open_writer_async(\n    store: ObjectStore,\n    path: str,\n    *,\n    attributes: Attributes | None = None,\n    buffer_size: int = 10 * 1024 * 1024,\n    tags: dict[str, str] | None = None,\n    max_concurrency: int = 12,\n) -&gt; AsyncWritableFile\n</code></pre> <p>Open an asynchronous writable file object at the specified location.</p> <p>Refer to the documentation for open_writer.</p>"},{"location":"api/file/#obstore.ReadableFile","title":"obstore.ReadableFile","text":"<p>A synchronous-buffered reader that implements a similar interface as a Python <code>BufferedReader</code>.</p> <p>Internally this maintains a buffer of the requested size, and uses <code>get_range</code> to populate its internal buffer once depleted. This buffer is cleared on seek.</p> <p>Whilst simple, this interface will typically be outperformed by the native <code>obstore</code> methods that better map to the network APIs. This is because most object stores have very high first-byte latencies, on the order of 100-200ms, and so avoiding unnecessary round-trips is critical to throughput.</p> <p>Systems looking to sequentially scan a file should instead consider using <code>get</code>, or <code>get_range</code> to read a particular range.</p> <p>Systems looking to read multiple ranges of a file should instead consider using <code>get_ranges</code>, which will optimise the vectored IO.</p>"},{"location":"api/file/#obstore.ReadableFile.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: ObjectMeta\n</code></pre> <p>Access the metadata of the underlying file.</p>"},{"location":"api/file/#obstore.ReadableFile.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>The size in bytes of the object.</p>"},{"location":"api/file/#obstore.ReadableFile.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the current file.</p> <p>This is currently a no-op.</p>"},{"location":"api/file/#obstore.ReadableFile.read","title":"read","text":"<pre><code>read(size: int | None = None) -&gt; Bytes\n</code></pre> <p>Read up to <code>size</code> bytes from the object and return them.</p> <p>As a convenience, if size is unspecified or <code>None</code>, all bytes until EOF are returned.</p>"},{"location":"api/file/#obstore.ReadableFile.readall","title":"readall","text":"<pre><code>readall() -&gt; Bytes\n</code></pre> <p>Read and return all the bytes from the stream until EOF.</p>"},{"location":"api/file/#obstore.ReadableFile.readline","title":"readline","text":"<pre><code>readline() -&gt; Bytes\n</code></pre> <p>Read a single line of the file, up until the next newline character.</p>"},{"location":"api/file/#obstore.ReadableFile.readlines","title":"readlines","text":"<pre><code>readlines(hint: int = -1) -&gt; list[Bytes]\n</code></pre> <p>Read all remaining lines into a list of buffers.</p>"},{"location":"api/file/#obstore.ReadableFile.seek","title":"seek","text":"<pre><code>seek(offset: int, whence: int = ...) -&gt; int\n</code></pre> <p>Change the stream position.</p> <p>Change the stream position to the given byte <code>offset</code>, interpreted relative to the position indicated by <code>whence</code>, and return the new absolute position. Values for <code>whence</code> are:</p> <ul> <li><code>os.SEEK_SET</code> or 0: start of the stream (the default); <code>offset</code> should be zero or positive</li> <li><code>os.SEEK_CUR</code> or 1: current stream position; <code>offset</code> may be negative</li> <li><code>os.SEEK_END</code> or 2: end of the stream; <code>offset</code> is usually negative</li> </ul>"},{"location":"api/file/#obstore.ReadableFile.seekable","title":"seekable","text":"<pre><code>seekable() -&gt; bool\n</code></pre> <p>Return True if the stream supports random access.</p>"},{"location":"api/file/#obstore.ReadableFile.tell","title":"tell","text":"<pre><code>tell() -&gt; int\n</code></pre> <p>Return the current stream position.</p>"},{"location":"api/file/#obstore.AsyncReadableFile","title":"obstore.AsyncReadableFile","text":"<p>An async-buffered reader that implements a similar interface as a Python <code>BufferedReader</code>.</p> <p>Internally this maintains a buffer of the requested size, and uses <code>get_range</code> to populate its internal buffer once depleted. This buffer is cleared on seek.</p> <p>Whilst simple, this interface will typically be outperformed by the native <code>obstore</code> methods that better map to the network APIs. This is because most object stores have very high first-byte latencies, on the order of 100-200ms, and so avoiding unnecessary round-trips is critical to throughput.</p> <p>Systems looking to sequentially scan a file should instead consider using <code>get</code>, or <code>get_range</code> to read a particular range.</p> <p>Systems looking to read multiple ranges of a file should instead consider using <code>get_ranges</code>, which will optimise the vectored IO.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: ObjectMeta\n</code></pre> <p>Access the metadata of the underlying file.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>The size in bytes of the object.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the current file.</p> <p>This is currently a no-op.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.read","title":"read  <code>async</code>","text":"<pre><code>read(size: int | None = None) -&gt; Bytes\n</code></pre> <p>Read up to <code>size</code> bytes from the object and return them.</p> <p>As a convenience, if size is unspecified or <code>None</code>, all bytes until EOF are returned.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.readall","title":"readall  <code>async</code>","text":"<pre><code>readall() -&gt; Bytes\n</code></pre> <p>Read and return all the bytes from the stream until EOF.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.readline","title":"readline  <code>async</code>","text":"<pre><code>readline() -&gt; Bytes\n</code></pre> <p>Read a single line of the file, up until the next newline character.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.readlines","title":"readlines  <code>async</code>","text":"<pre><code>readlines(hint: int = -1) -&gt; list[Bytes]\n</code></pre> <p>Read all remaining lines into a list of buffers.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.seek","title":"seek  <code>async</code>","text":"<pre><code>seek(offset: int, whence: int = ...) -&gt; int\n</code></pre> <p>Change the stream position.</p> <p>Change the stream position to the given byte <code>offset</code>, interpreted relative to the position indicated by <code>whence</code>, and return the new absolute position. Values for <code>whence</code> are:</p> <ul> <li><code>os.SEEK_SET</code> or 0: start of the stream (the default); <code>offset</code> should be zero or positive</li> <li><code>os.SEEK_CUR</code> or 1: current stream position; <code>offset</code> may be negative</li> <li><code>os.SEEK_END</code> or 2: end of the stream; <code>offset</code> is usually negative</li> </ul>"},{"location":"api/file/#obstore.AsyncReadableFile.seekable","title":"seekable","text":"<pre><code>seekable() -&gt; bool\n</code></pre> <p>Return True if the stream supports random access.</p>"},{"location":"api/file/#obstore.AsyncReadableFile.tell","title":"tell  <code>async</code>","text":"<pre><code>tell() -&gt; int\n</code></pre> <p>Return the current stream position.</p>"},{"location":"api/file/#obstore.WritableFile","title":"obstore.WritableFile","text":"<p>               Bases: <code>AbstractContextManager</code></p> <p>A buffered writable file object with synchronous operations.</p> <p>This implements a similar interface as a Python <code>BufferedWriter</code>.</p>"},{"location":"api/file/#obstore.WritableFile.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the current file.</p>"},{"location":"api/file/#obstore.WritableFile.closed","title":"closed","text":"<pre><code>closed() -&gt; bool\n</code></pre> <p>Check whether this file has been closed.</p> <p>Note that this is a method, not an attribute.</p>"},{"location":"api/file/#obstore.WritableFile.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flushes this output stream, ensuring that all intermediately buffered contents reach their destination.</p>"},{"location":"api/file/#obstore.WritableFile.write","title":"write","text":"<pre><code>write(buffer: bytes | Buffer) -&gt; int\n</code></pre> <p>Write the bytes-like object, <code>buffer</code>, and return the number of bytes written.</p>"},{"location":"api/file/#obstore.AsyncWritableFile","title":"obstore.AsyncWritableFile","text":"<p>               Bases: <code>AbstractAsyncContextManager</code></p> <p>A buffered writable file object with asynchronous operations.</p>"},{"location":"api/file/#obstore.AsyncWritableFile.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the current file.</p>"},{"location":"api/file/#obstore.AsyncWritableFile.closed","title":"closed  <code>async</code>","text":"<pre><code>closed() -&gt; bool\n</code></pre> <p>Check whether this file has been closed.</p> <p>Note that this is an async method, not an attribute.</p>"},{"location":"api/file/#obstore.AsyncWritableFile.flush","title":"flush  <code>async</code>","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flushes this output stream, ensuring that all intermediately buffered contents reach their destination.</p>"},{"location":"api/file/#obstore.AsyncWritableFile.write","title":"write  <code>async</code>","text":"<pre><code>write(buffer: bytes | Buffer) -&gt; int\n</code></pre> <p>Write the bytes-like object, <code>buffer</code>, and return the number of bytes written.</p>"},{"location":"api/fsspec/","title":"obstore.fsspec","text":""},{"location":"api/fsspec/#obstore.fsspec","title":"obstore.fsspec","text":"<p>Integration with the fsspec library.</p> <p>The fsspec integration is best effort and may not provide the same performance as the rest of obstore. If you find any bugs with this integration, please file an issue.</p> <p>The underlying <code>object_store</code> Rust crate cautions against relying too strongly on stateful filesystem representations of object stores:</p> <p>The ObjectStore interface is designed to mirror the APIs of object stores and not filesystems, and thus has stateless APIs instead of cursor based interfaces such as Read or Seek available in filesystems.</p> <p>This design provides the following advantages:</p> <ul> <li>All operations are atomic, and readers cannot observe partial and/or failed writes</li> <li>Methods map directly to object store APIs, providing both efficiency and   predictability</li> <li>Abstracts away filesystem and operating system specific quirks, ensuring portability</li> <li>Allows for functionality not native to filesystems, such as operation preconditions   and atomic multipart uploads</li> </ul> <p>Where possible, implementations should use the underlying <code>obstore</code> APIs directly. Only where this is not possible should users fall back to this fsspec integration.</p>"},{"location":"api/fsspec/#obstore.fsspec.SUPPORTED_PROTOCOLS","title":"SUPPORTED_PROTOCOLS  <code>module-attribute</code>","text":"<pre><code>SUPPORTED_PROTOCOLS: set[str] = {\n    \"abfs\",\n    \"abfss\",\n    \"adl\",\n    \"az\",\n    \"azure\",\n    \"file\",\n    \"gcs\",\n    \"gs\",\n    \"http\",\n    \"https\",\n    \"memory\",\n    \"s3\",\n    \"s3a\",\n}\n</code></pre> <p>All supported protocols.</p>"},{"location":"api/fsspec/#obstore.fsspec.SUPPORTED_PROTOCOLS_T","title":"SUPPORTED_PROTOCOLS_T  <code>module-attribute</code>","text":"<pre><code>SUPPORTED_PROTOCOLS_T = Literal[\n    \"abfs\",\n    \"abfss\",\n    \"adl\",\n    \"az\",\n    \"azure\",\n    \"file\",\n    \"gcs\",\n    \"gs\",\n    \"http\",\n    \"https\",\n    \"memory\",\n    \"s3\",\n    \"s3a\",\n]\n</code></pre> <p>A type hint for all supported protocols.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile","title":"BufferedFile","text":"<p>               Bases: <code>AbstractBufferedFile</code></p> <p>A buffered readable or writable file.</p> <p>This is a wrapper around <code>obstore.ReadableFile</code> and <code>obstore.WritableFile</code>. If you don't have a need to use the fsspec integration, you may be better served by using <code>open_reader</code> or <code>open_writer</code> directly.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.loc","title":"loc  <code>property</code> <code>writable</code>","text":"<pre><code>loc: int\n</code></pre> <p>Get current file location.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.__init__","title":"__init__","text":"<pre><code>__init__(\n    fs: FsspecStore,\n    path: str,\n    mode: Literal[\"rb\"] = \"rb\",\n    *,\n    buffer_size: int = 1024 * 1024,\n    **kwargs: Any,\n) -&gt; None\n</code></pre><pre><code>__init__(\n    fs: FsspecStore,\n    path: str,\n    mode: Literal[\"wb\"],\n    *,\n    buffer_size: int = 10 * 1024 * 1024,\n    attributes: Attributes | None = None,\n    tags: dict[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; None\n</code></pre> <pre><code>__init__(\n    fs: FsspecStore,\n    path: str,\n    mode: Literal[\"rb\", \"wb\"] = \"rb\",\n    *,\n    buffer_size: int | None = None,\n    attributes: Attributes | None = None,\n    tags: dict[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; None\n</code></pre> <p>Create new buffered file.</p> <p>Parameters:</p> <ul> <li> <code>fs</code>               (<code>FsspecStore</code>)           \u2013            <p>The underlying fsspec store to read from.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within the store to use.</p> </li> <li> <code>mode</code>               (<code>Literal['rb', 'wb']</code>, default:                   <code>'rb'</code> )           \u2013            <p><code>\"rb\"</code> for a readable binary file or <code>\"wb\"</code> for a writable binary file. Defaults to \"rb\".</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Attributes | None</code>)           \u2013            <p>Provide a set of <code>Attributes</code>. Only used when writing. Defaults to <code>None</code>.</p> </li> <li> <code>buffer_size</code>               (<code>int | None</code>)           \u2013            <p>Up to <code>buffer_size</code> bytes will be buffered in memory. When reading: The minimum number of bytes to read in a single request. When writing:  If <code>buffer_size</code> is exceeded, data will be uploaded as a multipart upload in chunks of <code>buffer_size</code>. Defaults to None.</p> </li> <li> <code>tags</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Provide tags for this object. Only used when writing. Defaults to <code>None</code>.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>)           \u2013            <p>Keyword arguments passed on to <code>fsspec.spec.AbstractBufferedFile</code>.</p> </li> </ul>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close file. Ensure flushing the buffer.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.flush","title":"flush","text":"<pre><code>flush(force: bool = False) -&gt; None\n</code></pre> <p>Write buffered data to backend store.</p> <p>Writes the current buffer, if it is larger than the block-size, or if the file is being closed.</p> <p>Parameters:</p> <ul> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Unused.</p> </li> </ul>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.read","title":"read","text":"<pre><code>read(length: int = -1) -&gt; bytes\n</code></pre> <p>Return bytes from the remote file.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>if positive, returns up to this many bytes; if negative, return all remaining bytes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bytes</code>           \u2013            <p>Data in bytes</p> </li> </ul>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.readline","title":"readline","text":"<pre><code>readline() -&gt; bytes\n</code></pre> <p>Read until first occurrence of newline character.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.readlines","title":"readlines","text":"<pre><code>readlines() -&gt; list[bytes]\n</code></pre> <p>Return all data, split by the newline character.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.seek","title":"seek","text":"<pre><code>seek(loc: int, whence: int = 0) -&gt; int\n</code></pre> <p>Set current file location.</p> <p>Parameters:</p> <ul> <li> <code>loc</code>               (<code>int</code>)           \u2013            <p>byte location</p> </li> <li> <code>whence</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Either - <code>0</code>: from start of file - <code>1</code>: current location - <code>2</code>: end of file</p> </li> </ul>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.tell","title":"tell","text":"<pre><code>tell() -&gt; int\n</code></pre> <p>Get current file location.</p>"},{"location":"api/fsspec/#obstore.fsspec.BufferedFile.write","title":"write","text":"<pre><code>write(data: bytes) -&gt; int\n</code></pre> <p>Write data to buffer.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>bytes</code>)           \u2013            <p>Set of bytes to be written.</p> </li> </ul>"},{"location":"api/fsspec/#obstore.fsspec.FsspecStore","title":"FsspecStore","text":"<p>               Bases: <code>AsyncFileSystem</code></p> <p>An fsspec implementation based on a obstore Store.</p> <p>You should be able to pass an instance of this class into any API that expects an fsspec-style object.</p>"},{"location":"api/fsspec/#obstore.fsspec.FsspecStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    protocol: Literal[\"s3\", \"s3a\"],\n    *args: Any,\n    config: S3Config | S3ConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    asynchronous: bool = False,\n    max_cache_size: int = 10,\n    loop: Any = None,\n    batch_size: int | None = None,\n    **kwargs: Unpack[S3ConfigInput],\n) -&gt; None\n</code></pre><pre><code>__init__(\n    protocol: Literal[\"gs\"],\n    *args: Any,\n    config: GCSConfig | GCSConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    asynchronous: bool = False,\n    max_cache_size: int = 10,\n    loop: Any = None,\n    batch_size: int | None = None,\n    **kwargs: Unpack[GCSConfigInput],\n) -&gt; None\n</code></pre><pre><code>__init__(\n    protocol: Literal[\"az\", \"adl\", \"azure\", \"abfs\", \"abfss\"],\n    *args: Any,\n    config: AzureConfig | AzureConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    asynchronous: bool = False,\n    max_cache_size: int = 10,\n    loop: Any = None,\n    batch_size: int | None = None,\n    **kwargs: Unpack[AzureConfigInput],\n) -&gt; None\n</code></pre> <pre><code>__init__(\n    protocol: SUPPORTED_PROTOCOLS_T | str | None = None,\n    *args: Any,\n    config: S3Config\n    | S3ConfigInput\n    | GCSConfig\n    | GCSConfigInput\n    | AzureConfig\n    | AzureConfigInput\n    | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    asynchronous: bool = False,\n    max_cache_size: int = 10,\n    loop: Any = None,\n    batch_size: int | None = None,\n    **kwargs: Any,\n) -&gt; None\n</code></pre> <p>Construct a new FsspecStore.</p> <p>Parameters:</p> <ul> <li> <code>protocol</code>               (<code>SUPPORTED_PROTOCOLS_T | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The storage protocol to use, such as \"s3\", \"gcs\", or \"abfs\". If <code>None</code>, the default class-level protocol is used. Default to None.</p> </li> <li> <code>config</code>               (<code>S3Config | S3ConfigInput | GCSConfig | GCSConfigInput | AzureConfig | AzureConfigInput | None</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for the cloud storage provider, which can be one of S3Config, S3ConfigInput, GCSConfig, GCSConfigInput, AzureConfig, or AzureConfigInput. Any of these values will be applied after checking for environment variables. If <code>None</code>, no cloud storage configuration is applied beyond what is found in environment variables.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for configuring the client.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for handling request errors.</p> </li> <li> <code>args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>positional arguments passed on to the <code>fsspec.asyn.AsyncFileSystem</code> constructor.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>asynchronous</code>               (<code>bool</code>)           \u2013            <p>Set to <code>True</code> if this instance is meant to be be called using the fsspec async API. This should only be set to true when running within a coroutine.</p> </li> <li> <code>max_cache_size</code>               (<code>int</code>)           \u2013            <p>The maximum number of stores the cache should keep. A cached store is kept internally for each bucket name. Defaults to 10.</p> </li> <li> <code>loop</code>               (<code>Any</code>)           \u2013            <p>since both fsspec/python and tokio/rust may be using loops, this should be kept <code>None</code> for now, and will not be used.</p> </li> <li> <code>batch_size</code>               (<code>int | None</code>)           \u2013            <p>some operations on many files will batch their requests; if you are seeing timeouts, you may want to set this number smaller than the defaults, which are determined in <code>fsspec.asyn._get_batch_size</code>.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>)           \u2013            <p>per-store configuration passed down to store-specific builders.</p> </li> </ul> <p>Examples:</p> <pre><code>from obstore.fsspec import FsspecStore\n\nstore = FsspecStore(\"https\")\nresp = store.cat_file(\"https://raw.githubusercontent.com/developmentseed/obstore/refs/heads/main/README.md\")\nassert resp.startswith(b\"# obstore\")\n</code></pre>"},{"location":"api/fsspec/#obstore.fsspec.register","title":"register","text":"<pre><code>register(\n    protocol: SUPPORTED_PROTOCOLS_T\n    | str\n    | Iterable[SUPPORTED_PROTOCOLS_T]\n    | Iterable[str]\n    | None = None,\n    *,\n    asynchronous: bool = False,\n) -&gt; None\n</code></pre> <p>Dynamically register a subclass of FsspecStore for the given protocol(s).</p> <p>This function creates a new subclass of FsspecStore with the specified protocol and registers it with fsspec. If multiple protocols are provided, the function registers each one individually.</p> <p>Parameters:</p> <ul> <li> <code>protocol</code>               (<code>SUPPORTED_PROTOCOLS_T | str | Iterable[SUPPORTED_PROTOCOLS_T] | Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single protocol (e.g., \"s3\", \"gcs\", \"abfs\") or a list of protocols to register FsspecStore for.</p> <p>Defaults to <code>None</code>, which will register <code>obstore</code> as the provider for all supported protocols except for <code>file://</code> and <code>memory://</code>. If you wish to use <code>obstore</code> via fsspec for <code>file://</code> or <code>memory://</code> URLs, list them explicitly.</p> </li> <li> <code>asynchronous</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, the registered store will support asynchronous operations. Defaults to <code>False</code>.</p> </li> </ul> <p>Example: <pre><code># Register obstore as the default handler for all supported protocols except for\n# `memory://` and `file://`\nregister()\n\nregister(\"s3\")\n\n# Registers an async store for \"s3\"\nregister(\"s3\", asynchronous=True)\n\n# Registers both \"gcs\" and \"abfs\"\nregister([\"gcs\", \"abfs\"])\n</code></pre></p> Notes <ul> <li>Each protocol gets a dynamically generated subclass named   <code>FsspecStore_&lt;protocol&gt;</code>. This avoids modifying the original   FsspecStore class.</li> </ul>"},{"location":"api/get/","title":"Get","text":""},{"location":"api/get/#obstore.get","title":"obstore.get","text":"<pre><code>get(\n    store: ObjectStore, path: str, *, options: GetOptions | None = None\n) -&gt; GetResult\n</code></pre> <p>Return the bytes that are stored at the specified location.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> <li> <code>options</code>               (<code>GetOptions | None</code>, default:                   <code>None</code> )           \u2013            <p>options for accessing the file. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GetResult</code>           \u2013            <p>GetResult</p> </li> </ul>"},{"location":"api/get/#obstore.get_async","title":"obstore.get_async  <code>async</code>","text":"<pre><code>get_async(\n    store: ObjectStore, path: str, *, options: GetOptions | None = None\n) -&gt; GetResult\n</code></pre> <p>Call <code>get</code> asynchronously.</p> <p>Refer to the documentation for get.</p>"},{"location":"api/get/#obstore.get_range","title":"obstore.get_range","text":"<pre><code>get_range(\n    store: ObjectStore,\n    path: str,\n    *,\n    start: int,\n    end: int | None = None,\n    length: int | None = None,\n) -&gt; Bytes\n</code></pre> <p>Return the bytes that are stored at the specified location in the given byte range.</p> <p>If the given range is zero-length or starts after the end of the object, an error will be returned. Additionally, if the range ends after the end of the object, the entire remainder of the object will be returned. Otherwise, the exact requested range will be returned.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>start</code>               (<code>int</code>)           \u2013            <p>The start of the byte range.</p> </li> <li> <code>end</code>               (<code>int | None</code>)           \u2013            <p>The end of the byte range (exclusive). Either <code>end</code> or <code>length</code> must be non-None.</p> </li> <li> <code>length</code>               (<code>int | None</code>)           \u2013            <p>The number of bytes of the byte range. Either <code>end</code> or <code>length</code> must be non-None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bytes</code>           \u2013            <p>A <code>Bytes</code> object implementing the Python buffer protocol, allowing zero-copy access to the underlying memory provided by Rust.</p> </li> </ul>"},{"location":"api/get/#obstore.get_range_async","title":"obstore.get_range_async  <code>async</code>","text":"<pre><code>get_range_async(\n    store: ObjectStore,\n    path: str,\n    *,\n    start: int,\n    end: int | None = None,\n    length: int | None = None,\n) -&gt; Bytes\n</code></pre> <p>Call <code>get_range</code> asynchronously.</p> <p>Refer to the documentation for get_range.</p>"},{"location":"api/get/#obstore.get_ranges","title":"obstore.get_ranges","text":"<pre><code>get_ranges(\n    store: ObjectStore,\n    path: str,\n    *,\n    starts: Sequence[int],\n    ends: Sequence[int] | None = None,\n    lengths: Sequence[int] | None = None,\n) -&gt; list[Bytes]\n</code></pre> <p>Return the bytes stored at the specified location in the given byte ranges.</p> <p>To improve performance this will:</p> <ul> <li>Transparently combine ranges less than 1MB apart into a single underlying request</li> <li>Make multiple <code>fetch</code> requests in parallel (up to maximum of 10)</li> </ul> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>starts</code>               (<code>Sequence[int]</code>)           \u2013            <p>A sequence of <code>int</code> where each offset starts.</p> </li> <li> <code>ends</code>               (<code>Sequence[int] | None</code>)           \u2013            <p>A sequence of <code>int</code> where each offset ends (exclusive). Either <code>ends</code> or <code>lengths</code> must be non-None.</p> </li> <li> <code>lengths</code>               (<code>Sequence[int] | None</code>)           \u2013            <p>A sequence of <code>int</code> with the number of bytes of each byte range. Either <code>ends</code> or <code>lengths</code> must be non-None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Bytes]</code>           \u2013            <p>A sequence of <code>Bytes</code>, one for each range. This <code>Bytes</code> object implements the Python buffer protocol, allowing zero-copy access to the underlying memory provided by Rust.</p> </li> </ul>"},{"location":"api/get/#obstore.get_ranges_async","title":"obstore.get_ranges_async  <code>async</code>","text":"<pre><code>get_ranges_async(\n    store: ObjectStore,\n    path: str,\n    *,\n    starts: Sequence[int],\n    ends: Sequence[int] | None = None,\n    lengths: Sequence[int] | None = None,\n) -&gt; list[Bytes]\n</code></pre> <p>Call <code>get_ranges</code> asynchronously.</p> <p>Refer to the documentation for get_ranges.</p>"},{"location":"api/get/#obstore.GetOptions","title":"obstore.GetOptions","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options for a get request.</p> <p>All options are optional.</p>"},{"location":"api/get/#obstore.GetOptions.head","title":"head  <code>instance-attribute</code>","text":"<pre><code>head: bool\n</code></pre> <p>Request transfer of no content</p> <p>datatracker.ietf.org/doc/html/rfc9110#name-head</p>"},{"location":"api/get/#obstore.GetOptions.if_match","title":"if_match  <code>instance-attribute</code>","text":"<pre><code>if_match: str | None\n</code></pre> <p>Request will succeed if the <code>ObjectMeta::e_tag</code> matches otherwise returning <code>PreconditionError</code>.</p> <p>See datatracker.ietf.org/doc/html/rfc9110#name-if-match</p> <p>Examples:</p> <pre><code>If-Match: \"xyzzy\"\nIf-Match: \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"\nIf-Match: *\n</code></pre>"},{"location":"api/get/#obstore.GetOptions.if_modified_since","title":"if_modified_since  <code>instance-attribute</code>","text":"<pre><code>if_modified_since: datetime | None\n</code></pre> <p>Request will succeed if the object has not been modified since otherwise returning <code>PreconditionError</code>.</p> <p>Some stores, such as S3, will only return <code>NotModified</code> for exact timestamp matches, instead of for any timestamp greater than or equal.</p> <p>datatracker.ietf.org/doc/html/rfc9110#section-13.1.4</p>"},{"location":"api/get/#obstore.GetOptions.if_none_match","title":"if_none_match  <code>instance-attribute</code>","text":"<pre><code>if_none_match: str | None\n</code></pre> <p>Request will succeed if the <code>ObjectMeta::e_tag</code> does not match otherwise returning <code>NotModifiedError</code>.</p> <p>See datatracker.ietf.org/doc/html/rfc9110#section-13.1.2</p> <p>Examples:</p> <pre><code>If-None-Match: \"xyzzy\"\nIf-None-Match: \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"\nIf-None-Match: *\n</code></pre>"},{"location":"api/get/#obstore.GetOptions.if_unmodified_since","title":"if_unmodified_since  <code>instance-attribute</code>","text":"<pre><code>if_unmodified_since: datetime | None\n</code></pre> <p>Request will succeed if the object has been modified since</p> <p>datatracker.ietf.org/doc/html/rfc9110#section-13.1.3</p>"},{"location":"api/get/#obstore.GetOptions.range","title":"range  <code>instance-attribute</code>","text":"<pre><code>range: tuple[int, int] | list[int] | OffsetRange | SuffixRange\n</code></pre> <p>Request transfer of only the specified range of bytes otherwise returning <code>NotModifiedError</code>.</p> <p>The semantics of this tuple are:</p> <ul> <li> <p><code>(int, int)</code>: Request a specific range of bytes <code>(start, end)</code>.</p> <p>If the given range is zero-length or starts after the end of the object, an error will be returned. Additionally, if the range ends after the end of the object, the entire remainder of the object will be returned. Otherwise, the exact requested range will be returned.</p> <p>The <code>end</code> offset is exclusive.</p> </li> <li> <p><code>{\"offset\": int}</code>: Request all bytes starting from a given byte offset.</p> <p>This is equivalent to <code>bytes={int}-</code> as an HTTP header.</p> </li> <li> <p><code>{\"suffix\": int}</code>: Request the last <code>int</code> bytes. Note that here, <code>int</code> is the     size of the request, not the byte offset. This is equivalent to <code>bytes=-{int}</code>     as an HTTP header.</p> </li> </ul> <p>datatracker.ietf.org/doc/html/rfc9110#name-range</p>"},{"location":"api/get/#obstore.GetOptions.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str | None\n</code></pre> <p>Request a particular object version</p>"},{"location":"api/get/#obstore.GetResult","title":"obstore.GetResult","text":"<p>Result for a get request.</p> <p>You can materialize the entire buffer by using either <code>bytes</code> or <code>bytes_async</code>, or you can stream the result using <code>stream</code>. <code>__iter__</code> and <code>__aiter__</code> are implemented as aliases to <code>stream</code>, so you can alternatively call <code>iter()</code> or <code>aiter()</code> on <code>GetResult</code> to start an iterator.</p> <p>Using as an async iterator: <pre><code>resp = await obs.get_async(store, path)\n# 5MB chunk size in stream\nstream = resp.stream(min_chunk_size=5 * 1024 * 1024)\nasync for buf in stream:\n    print(len(buf))\n</code></pre></p> <p>Using as a sync iterator: <pre><code>resp = obs.get(store, path)\n# 20MB chunk size in stream\nstream = resp.stream(min_chunk_size=20 * 1024 * 1024)\nfor buf in stream:\n    print(len(buf))\n</code></pre></p> <p>Note that after calling <code>bytes</code>, <code>bytes_async</code>, or <code>stream</code>, you will no longer be able to call other methods on this object, such as the <code>meta</code> attribute.</p>"},{"location":"api/get/#obstore.GetResult.attributes","title":"attributes  <code>property</code>","text":"<pre><code>attributes: Attributes\n</code></pre> <p>Additional object attributes.</p> <p>This must be accessed before calling <code>stream</code>, <code>bytes</code>, or <code>bytes_async</code>.</p>"},{"location":"api/get/#obstore.GetResult.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: ObjectMeta\n</code></pre> <p>The ObjectMeta for this object.</p> <p>This must be accessed before calling <code>stream</code>, <code>bytes</code>, or <code>bytes_async</code>.</p>"},{"location":"api/get/#obstore.GetResult.range","title":"range  <code>property</code>","text":"<pre><code>range: tuple[int, int]\n</code></pre> <p>The range of bytes returned by this request.</p> <p>Note that this is <code>(start, stop)</code> not <code>(start, length)</code>.</p> <p>This must be accessed before calling <code>stream</code>, <code>bytes</code>, or <code>bytes_async</code>.</p>"},{"location":"api/get/#obstore.GetResult.__aiter__","title":"__aiter__","text":"<pre><code>__aiter__() -&gt; BytesStream\n</code></pre> <p>Return a chunked stream over the result's bytes.</p> <p>Uses the default (10MB) chunk size.</p>"},{"location":"api/get/#obstore.GetResult.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; BytesStream\n</code></pre> <p>Return a chunked stream over the result's bytes.</p> <p>Uses the default (10MB) chunk size.</p>"},{"location":"api/get/#obstore.GetResult.bytes","title":"bytes","text":"<pre><code>bytes() -&gt; Bytes\n</code></pre> <p>Collect the data into a <code>Bytes</code> object.</p> <p>This implements the Python buffer protocol. You can copy the buffer to Python memory by passing to <code>bytes</code>.</p>"},{"location":"api/get/#obstore.GetResult.bytes_async","title":"bytes_async  <code>async</code>","text":"<pre><code>bytes_async() -&gt; Bytes\n</code></pre> <p>Collect the data into a <code>Bytes</code> object.</p> <p>This implements the Python buffer protocol. You can copy the buffer to Python memory by passing to <code>bytes</code>.</p>"},{"location":"api/get/#obstore.GetResult.stream","title":"stream","text":"<pre><code>stream(min_chunk_size: int = 10 * 1024 * 1024) -&gt; BytesStream\n</code></pre> <p>Return a chunked stream over the result's bytes.</p> <p>Parameters:</p> <ul> <li> <code>min_chunk_size</code>               (<code>int</code>, default:                   <code>10 * 1024 * 1024</code> )           \u2013            <p>The minimum size in bytes for each chunk in the returned <code>BytesStream</code>. All chunks except for the last chunk will be at least this size. Defaults to 10*1024*1024 (10MB).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BytesStream</code>           \u2013            <p>A chunked stream</p> </li> </ul>"},{"location":"api/get/#obstore.BytesStream","title":"obstore.BytesStream","text":"<p>An async stream of bytes.</p> <p>Request timeouts</p> <p>The underlying stream needs to stay alive until the last chunk is polled. If the file is large, it may exceed the default timeout of 30 seconds. In this case, you may see an error like:</p> <pre><code>GenericError: Generic {\n    store: \"HTTP\",\n    source: reqwest::Error {\n        kind: Decode,\n        source: reqwest::Error {\n            kind: Body,\n            source: TimedOut,\n        },\n    },\n}\n</code></pre> <p>To fix this, set the <code>timeout</code> parameter in the <code>client_options</code> passed when creating the store.</p>"},{"location":"api/get/#obstore.BytesStream.__aiter__","title":"__aiter__","text":"<pre><code>__aiter__() -&gt; BytesStream\n</code></pre> <p>Return <code>Self</code> as an async iterator.</p>"},{"location":"api/get/#obstore.BytesStream.__anext__","title":"__anext__  <code>async</code>","text":"<pre><code>__anext__() -&gt; bytes\n</code></pre> <p>Return the next chunk of bytes in the stream.</p>"},{"location":"api/get/#obstore.BytesStream.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; BytesStream\n</code></pre> <p>Return <code>Self</code> as an async iterator.</p>"},{"location":"api/get/#obstore.BytesStream.__next__","title":"__next__","text":"<pre><code>__next__() -&gt; bytes\n</code></pre> <p>Return the next chunk of bytes in the stream.</p>"},{"location":"api/get/#obstore.Bytes","title":"obstore.Bytes","text":"<p>               Bases: <code>Buffer</code></p> <p>A <code>bytes</code>-like buffer.</p> <p>This implements the Python buffer protocol, allowing zero-copy access to underlying Rust memory.</p> <p>You can pass this to <code>memoryview</code> for a zero-copy view into the underlying data or to <code>bytes</code> to copy the underlying data into a Python <code>bytes</code>.</p> <p>Many methods from the Python <code>bytes</code> class are implemented on this,</p>"},{"location":"api/get/#obstore.Bytes.__init__","title":"__init__","text":"<pre><code>__init__(buf: Buffer = b'') -&gt; None\n</code></pre> <p>Construct a new Bytes object.</p> <p>This will be a zero-copy view on the Python byte slice.</p>"},{"location":"api/get/#obstore.Bytes.isalnum","title":"isalnum","text":"<pre><code>isalnum() -&gt; bool\n</code></pre> <p>Return <code>True</code> if all bytes in the sequence are alphabetical ASCII characters or ASCII decimal digits and the sequence is not empty, <code>False</code> otherwise.</p> <p>Alphabetic ASCII characters are those byte values in the sequence <code>b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'</code>. ASCII decimal digits are those byte values in the sequence <code>b'0123456789'</code>.</p>"},{"location":"api/get/#obstore.Bytes.isalpha","title":"isalpha","text":"<pre><code>isalpha() -&gt; bool\n</code></pre> <p>Return <code>True</code> if all bytes in the sequence are alphabetic ASCII characters and the sequence is not empty, <code>False</code> otherwise.</p> <p>Alphabetic ASCII characters are those byte values in the sequence <code>b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'</code>.</p>"},{"location":"api/get/#obstore.Bytes.isascii","title":"isascii","text":"<pre><code>isascii() -&gt; bool\n</code></pre> <p>Return <code>True</code> if the sequence is empty or all bytes in the sequence are ASCII, <code>False</code> otherwise.</p> <p>ASCII bytes are in the range <code>0-0x7F</code>.</p>"},{"location":"api/get/#obstore.Bytes.isdigit","title":"isdigit","text":"<pre><code>isdigit() -&gt; bool\n</code></pre> <p>Return <code>True</code> if all bytes in the sequence are ASCII decimal digits and the sequence is not empty, <code>False</code> otherwise.</p> <p>ASCII decimal digits are those byte values in the sequence <code>b'0123456789'</code>.</p>"},{"location":"api/get/#obstore.Bytes.islower","title":"islower","text":"<pre><code>islower() -&gt; bool\n</code></pre> <p>Return <code>True</code> if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, <code>False</code> otherwise.</p>"},{"location":"api/get/#obstore.Bytes.isspace","title":"isspace","text":"<pre><code>isspace() -&gt; bool\n</code></pre> <p>Return <code>True</code> if all bytes in the sequence are ASCII whitespace and the sequence is not empty, <code>False</code> otherwise.</p> <p>ASCII whitespace characters are those byte values in the sequence <code>b' \\t\\n\\r\\x0b\\f'</code> (space, tab, newline, carriage return, vertical tab, form feed).</p>"},{"location":"api/get/#obstore.Bytes.isupper","title":"isupper","text":"<pre><code>isupper() -&gt; bool\n</code></pre> <p>Return <code>True</code> if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII characters, <code>False</code> otherwise.</p>"},{"location":"api/get/#obstore.Bytes.lower","title":"lower","text":"<pre><code>lower() -&gt; Bytes\n</code></pre> <p>Return a copy of the sequence with all the uppercase ASCII characters converted to their corresponding lowercase counterpart.</p>"},{"location":"api/get/#obstore.Bytes.removeprefix","title":"removeprefix","text":"<pre><code>removeprefix(prefix: Buffer) -&gt; Bytes\n</code></pre> <p>If the binary data starts with the prefix string, return <code>bytes[len(prefix):]</code>. Otherwise, return the original binary data.</p>"},{"location":"api/get/#obstore.Bytes.removesuffix","title":"removesuffix","text":"<pre><code>removesuffix(suffix: Buffer) -&gt; Bytes\n</code></pre> <p>If the binary data ends with the suffix string and that suffix is not empty, return <code>bytes[:-len(suffix)]</code>. Otherwise, return the original binary data.</p>"},{"location":"api/get/#obstore.Bytes.to_bytes","title":"to_bytes","text":"<pre><code>to_bytes() -&gt; bytes\n</code></pre> <p>Copy this buffer's contents into a Python <code>bytes</code> object.</p>"},{"location":"api/get/#obstore.Bytes.upper","title":"upper","text":"<pre><code>upper() -&gt; Bytes\n</code></pre> <p>Return a copy of the sequence with all the lowercase ASCII characters converted to their corresponding uppercase counterpart.</p>"},{"location":"api/get/#obstore.OffsetRange","title":"obstore.OffsetRange","text":"<p>               Bases: <code>TypedDict</code></p> <p>Request all bytes starting from a given byte offset.</p>"},{"location":"api/get/#obstore.OffsetRange.offset","title":"offset  <code>instance-attribute</code>","text":"<pre><code>offset: int\n</code></pre> <p>The byte offset for the offset range request.</p>"},{"location":"api/get/#obstore.SuffixRange","title":"obstore.SuffixRange","text":"<p>               Bases: <code>TypedDict</code></p> <p>Request up to the last <code>n</code> bytes.</p>"},{"location":"api/get/#obstore.SuffixRange.suffix","title":"suffix  <code>instance-attribute</code>","text":"<pre><code>suffix: int\n</code></pre> <p>The number of bytes from the suffix to request.</p>"},{"location":"api/head/","title":"Head","text":""},{"location":"api/head/#obstore.head","title":"obstore.head","text":"<pre><code>head(store: ObjectStore, path: str) -&gt; ObjectMeta\n</code></pre> <p>Return the metadata for the specified location.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ObjectMeta</code>           \u2013            <p>ObjectMeta</p> </li> </ul>"},{"location":"api/head/#obstore.head_async","title":"obstore.head_async  <code>async</code>","text":"<pre><code>head_async(store: ObjectStore, path: str) -&gt; ObjectMeta\n</code></pre> <p>Call <code>head</code> asynchronously.</p> <p>Refer to the documentation for head.</p>"},{"location":"api/list/","title":"List","text":""},{"location":"api/list/#obstore.list","title":"obstore.list","text":"<pre><code>list(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    offset: str | None = None,\n    chunk_size: int = 50,\n    return_arrow: Literal[True],\n) -&gt; ListStream[RecordBatch]\n</code></pre><pre><code>list(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    offset: str | None = None,\n    chunk_size: int = 50,\n    return_arrow: Literal[False] = False,\n) -&gt; ListStream[List[ObjectMeta]]\n</code></pre> <pre><code>list(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    offset: str | None = None,\n    chunk_size: int = 50,\n    return_arrow: bool = False,\n) -&gt; ListStream[RecordBatch] | ListStream[List[ObjectMeta]]\n</code></pre> <p>List all the objects with the given prefix.</p> <p>Prefixes are evaluated on a path segment basis, i.e. <code>foo/bar/</code> is a prefix of <code>foo/bar/x</code> but not of <code>foo/bar_baz/x</code>. List is recursive, i.e. <code>foo/bar/more/x</code> will be included.</p> <p>Examples:</p> <p>Synchronously iterate through list results:</p> <pre><code>import obstore as obs\nfrom obstore.store import MemoryStore\n\nstore = MemoryStore()\nfor i in range(100):\n    obs.put(store, f\"file{i}.txt\", b\"foo\")\n\nstream = obs.list(store, chunk_size=10)\nfor list_result in stream:\n    print(list_result[0])\n    # {'path': 'file0.txt', 'last_modified': datetime.datetime(2024, 10, 23, 19, 19, 28, 781723, tzinfo=datetime.timezone.utc), 'size': 3, 'e_tag': '0', 'version': None}\n    break\n</code></pre> <p>Asynchronously iterate through list results. Just change <code>for</code> to <code>async for</code>:</p> <pre><code>stream = obs.list(store, chunk_size=10)\nasync for list_result in stream:\n    print(list_result[2])\n    # {'path': 'file10.txt', 'last_modified': datetime.datetime(2024, 10, 23, 19, 21, 46, 224725, tzinfo=datetime.timezone.utc), 'size': 3, 'e_tag': '10', 'version': None}\n    break\n</code></pre> <p>Return large list results as Arrow. This is most useful with large list operations. In this case you may want to increase the <code>chunk_size</code> parameter.</p> <pre><code>stream = obs.list(store, chunk_size=1000, return_arrow=True)\n# Stream is now an iterable/async iterable of `RecordBatch`es\nfor batch in stream:\n    print(batch.num_rows) # 100\n\n    # If desired, convert to a pyarrow RecordBatch (zero-copy) with\n    # `pyarrow.record_batch(batch)`\n    break\n</code></pre> <p>Collect all list results into a single Arrow <code>RecordBatch</code>.</p> <pre><code>stream = obs.list(store, return_arrow=True)\nbatch = stream.collect()\n</code></pre> <p>Note</p> <p>The order of returned <code>ObjectMeta</code> is not guaranteed</p> <p>Note</p> <p>There is no async version of this method, because <code>list</code> is not async under the hood, rather it only instantiates a stream, which can be polled in synchronous or asynchronous fashion. See <code>ListStream</code>.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The prefix within ObjectStore to use for listing. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>offset</code>               (<code>str | None</code>)           \u2013            <p>If provided, list all the objects with the given prefix and a location greater than <code>offset</code>. Defaults to <code>None</code>.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The number of items to collect per chunk in the returned (async) iterator. All chunks except for the last one will have this many items. This is ignored in the <code>collect</code> and <code>collect_async</code> methods of <code>ListStream</code>.</p> </li> <li> <code>return_arrow</code>               (<code>bool</code>)           \u2013            <p>If <code>True</code>, return each batch of list items as an Arrow <code>RecordBatch</code>, not as a list of Python <code>dict</code>s. Arrow removes serialization overhead between Rust and Python and so this can be significantly faster for large list operations. Defaults to <code>False</code>.</p> <p>If this is <code>True</code>, the <code>arro3-core</code> Python package must be installed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ListStream[RecordBatch] | ListStream[List[ObjectMeta]]</code>           \u2013            <p>A ListStream, which you can iterate through to access list results.</p> </li> </ul>"},{"location":"api/list/#obstore.list_with_delimiter","title":"obstore.list_with_delimiter","text":"<pre><code>list_with_delimiter(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    return_arrow: Literal[True],\n) -&gt; ListResult[Table]\n</code></pre><pre><code>list_with_delimiter(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    return_arrow: Literal[False] = False,\n) -&gt; ListResult[List[ObjectMeta]]\n</code></pre> <pre><code>list_with_delimiter(\n    store: ObjectStore, prefix: str | None = None, *, return_arrow: bool = False\n) -&gt; ListResult[Table] | ListResult[List[ObjectMeta]]\n</code></pre> <p>List objects with the given prefix and an implementation specific delimiter.</p> <p>Returns common prefixes (directories) in addition to object metadata.</p> <p>Prefixes are evaluated on a path segment basis, i.e. <code>foo/bar/</code> is a prefix of <code>foo/bar/x</code> but not of <code>foo/bar_baz/x</code>. This list is not recursive, i.e. <code>foo/bar/more/x</code> will not be included.</p> <p>Note</p> <p>Any prefix supplied to this <code>prefix</code> parameter will not be stripped off the paths in the result.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The prefix within ObjectStore to use for listing. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>return_arrow</code>               (<code>bool</code>)           \u2013            <p>If <code>True</code>, return list results as an Arrow <code>Table</code>, not as a list of Python <code>dict</code>s. Arrow removes serialization overhead between Rust and Python and so this can be significantly faster for large list operations. Defaults to <code>False</code>.</p> <p>If this is <code>True</code>, the <code>arro3-core</code> Python package must be installed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ListResult[Table] | ListResult[List[ObjectMeta]]</code>           \u2013            <p>ListResult</p> </li> </ul>"},{"location":"api/list/#obstore.list_with_delimiter_async","title":"obstore.list_with_delimiter_async  <code>async</code>","text":"<pre><code>list_with_delimiter_async(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    return_arrow: Literal[True],\n) -&gt; ListResult[Table]\n</code></pre><pre><code>list_with_delimiter_async(\n    store: ObjectStore,\n    prefix: str | None = None,\n    *,\n    return_arrow: Literal[False] = False,\n) -&gt; ListResult[List[ObjectMeta]]\n</code></pre> <pre><code>list_with_delimiter_async(\n    store: ObjectStore, prefix: str | None = None, *, return_arrow: bool = False\n) -&gt; ListResult[Table] | ListResult[List[ObjectMeta]]\n</code></pre> <p>Call <code>list_with_delimiter</code> asynchronously.</p> <p>Refer to the documentation for list_with_delimiter.</p>"},{"location":"api/list/#obstore.ObjectMeta","title":"obstore.ObjectMeta","text":"<p>               Bases: <code>TypedDict</code></p> <p>The metadata that describes an object.</p>"},{"location":"api/list/#obstore.ObjectMeta.e_tag","title":"e_tag  <code>instance-attribute</code>","text":"<pre><code>e_tag: str | None\n</code></pre> <p>The unique identifier for the object</p> <p>datatracker.ietf.org/doc/html/rfc9110#name-etag</p>"},{"location":"api/list/#obstore.ObjectMeta.last_modified","title":"last_modified  <code>instance-attribute</code>","text":"<pre><code>last_modified: datetime\n</code></pre> <p>The last modified time</p>"},{"location":"api/list/#obstore.ObjectMeta.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>The full path to the object</p>"},{"location":"api/list/#obstore.ObjectMeta.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre> <p>The size in bytes of the object</p>"},{"location":"api/list/#obstore.ObjectMeta.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str | None\n</code></pre> <p>A version indicator for this object</p>"},{"location":"api/list/#obstore.ListResult","title":"obstore.ListResult","text":"<p>               Bases: <code>TypedDict</code>, <code>Generic[ListChunkType]</code></p> <p>Result of a list call.</p> <p>Includes objects, prefixes (directories) and a token for the next set of results. Individual result sets may be limited to 1,000 objects based on the underlying object storage's limitations.</p>"},{"location":"api/list/#obstore.ListResult.common_prefixes","title":"common_prefixes  <code>instance-attribute</code>","text":"<pre><code>common_prefixes: list[str]\n</code></pre> <p>Prefixes that are common (like directories)</p>"},{"location":"api/list/#obstore.ListResult.objects","title":"objects  <code>instance-attribute</code>","text":"<pre><code>objects: ListChunkType\n</code></pre> <p>Object metadata for the listing</p>"},{"location":"api/list/#obstore.ListStream","title":"obstore.ListStream","text":"<p>               Bases: <code>Generic[ListChunkType]</code></p> <p>A stream of ObjectMeta that can be polled in a sync or async fashion.</p>"},{"location":"api/list/#obstore.ListStream.__aiter__","title":"__aiter__","text":"<pre><code>__aiter__() -&gt; Self\n</code></pre> <p>Return <code>Self</code> as an async iterator.</p>"},{"location":"api/list/#obstore.ListStream.__anext__","title":"__anext__  <code>async</code>","text":"<pre><code>__anext__() -&gt; ListChunkType\n</code></pre> <p>Return the next chunk of ObjectMeta in the stream.</p>"},{"location":"api/list/#obstore.ListStream.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Self\n</code></pre> <p>Return <code>Self</code> as an async iterator.</p>"},{"location":"api/list/#obstore.ListStream.__next__","title":"__next__","text":"<pre><code>__next__() -&gt; ListChunkType\n</code></pre> <p>Return the next chunk of ObjectMeta in the stream.</p>"},{"location":"api/list/#obstore.ListStream.collect","title":"collect","text":"<pre><code>collect() -&gt; ListChunkType\n</code></pre> <p>Collect all remaining ObjectMeta objects in the stream.</p> <p>This ignores the <code>chunk_size</code> parameter from the <code>list</code> call and collects all remaining data into a single chunk.</p>"},{"location":"api/list/#obstore.ListStream.collect_async","title":"collect_async  <code>async</code>","text":"<pre><code>collect_async() -&gt; ListChunkType\n</code></pre> <p>Collect all remaining ObjectMeta objects in the stream.</p> <p>This ignores the <code>chunk_size</code> parameter from the <code>list</code> call and collects all remaining data into a single chunk.</p>"},{"location":"api/list/#obstore.ListChunkType","title":"obstore.ListChunkType  <code>module-attribute</code>","text":"<pre><code>ListChunkType = TypeVar('ListChunkType', list[ObjectMeta], RecordBatch, Table)\n</code></pre> <p>The data structure used for holding list results.</p> <p>By default, listing APIs return a <code>list</code> of <code>ObjectMeta</code>. However for improved performance when listing large buckets, you can pass <code>return_arrow=True</code>. Then an Arrow <code>RecordBatch</code> will be returned instead.</p>"},{"location":"api/put/","title":"Put","text":""},{"location":"api/put/#obstore.put","title":"obstore.put","text":"<pre><code>put(\n    store: ObjectStore,\n    path: str,\n    file: IO[bytes]\n    | Path\n    | bytes\n    | Buffer\n    | Iterator[Buffer]\n    | Iterable[Buffer],\n    *,\n    attributes: Attributes | None = None,\n    tags: dict[str, str] | None = None,\n    mode: PutMode | None = None,\n    use_multipart: bool | None = None,\n    chunk_size: int = ...,\n    max_concurrency: int = 12,\n) -&gt; PutResult\n</code></pre> <p>Save the provided bytes to the specified location.</p> <p>The operation is guaranteed to be atomic, it will either successfully write the entirety of <code>file</code> to <code>location</code>, or fail. No clients should be able to observe a partially written object.</p> <p>Aborted multipart uploads</p> <p>This function will automatically use multipart uploads under the hood for large file objects (whenever the length of the file is greater than <code>chunk_size</code>) or for iterable or async iterable input.</p> <p>Multipart uploads have a variety of advantages, including performance and reliability.</p> <p>However, aborted or incomplete multipart uploads can leave partial content in a hidden state in your bucket, silently adding to your storage costs. It's recommended to configure lifecycle rules to automatically delete aborted multipart uploads. See here for the AWS S3 documentation, for example.</p> <p>You can turn off multipart uploads by passing <code>use_multipart=False</code>.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path within ObjectStore for where to save the file.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | bytes | Buffer | Iterator[Buffer] | Iterable[Buffer]</code>)           \u2013            <p>The object to upload. Supports various input:</p> <ul> <li>A file-like object opened in binary read mode</li> <li>A <code>Path</code> to a local file</li> <li>A <code>bytes</code> object.</li> <li>Any object implementing the Python buffer   protocol (includes <code>bytes</code>   but also <code>memoryview</code>, numpy arrays, and more).</li> <li>An iterator or iterable of objects implementing the Python buffer   protocol.</li> </ul> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>mode</code>               (<code>PutMode | None</code>)           \u2013            <p>Configure the <code>PutMode</code> for this operation. Refer to the <code>PutMode</code> docstring for more information.</p> <p>If this provided and is not <code>\"overwrite\"</code>, a non-multipart upload will be performed. Defaults to <code>\"overwrite\"</code>.</p> </li> <li> <code>attributes</code>               (<code>Attributes | None</code>)           \u2013            <p>Provide a set of <code>Attributes</code>. Defaults to <code>None</code>.</p> </li> <li> <code>tags</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Provide tags for this object. Defaults to <code>None</code>.</p> </li> <li> <code>use_multipart</code>               (<code>bool | None</code>)           \u2013            <p>Whether to use a multipart upload under the hood. Defaults using a multipart upload if the length of the file is greater than <code>chunk_size</code>. When <code>use_multipart</code> is <code>False</code>, the entire input will be materialized in memory as part of the upload.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of chunks to use within each part of the multipart upload. Defaults to 5 MB.</p> </li> <li> <code>max_concurrency</code>               (<code>int</code>)           \u2013            <p>The maximum number of chunks to upload concurrently. Defaults to 12.</p> </li> </ul>"},{"location":"api/put/#obstore.put_async","title":"obstore.put_async  <code>async</code>","text":"<pre><code>put_async(\n    store: ObjectStore,\n    path: str,\n    file: IO[bytes]\n    | Path\n    | bytes\n    | Buffer\n    | AsyncIterator[Buffer]\n    | AsyncIterable[Buffer]\n    | Iterator[Buffer]\n    | Iterable[Buffer],\n    *,\n    attributes: Attributes | None = None,\n    tags: dict[str, str] | None = None,\n    mode: PutMode | None = None,\n    use_multipart: bool | None = None,\n    chunk_size: int = ...,\n    max_concurrency: int = 12,\n) -&gt; PutResult\n</code></pre> <p>Call <code>put</code> asynchronously.</p> <p>Refer to the documentation for <code>put</code>. In addition to what the synchronous <code>put</code> allows for the <code>file</code> parameter, this also supports an async iterator or iterable of objects implementing the Python buffer protocol.</p> <p>This means, for example, you can pass the result of <code>get_async</code> directly to <code>put_async</code>, and the request will be streamed through Python during the put operation:</p> <pre><code>import obstore as obs\n\n# This only constructs the stream, it doesn't materialize the data in memory\nresp = await obs.get_async(store1, path1)\n# A streaming upload is created to copy the file to path2\nawait obs.put_async(store2, path2)\n</code></pre>"},{"location":"api/put/#obstore.PutResult","title":"obstore.PutResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>Result for a put request.</p>"},{"location":"api/put/#obstore.PutResult.e_tag","title":"e_tag  <code>instance-attribute</code>","text":"<pre><code>e_tag: str | None\n</code></pre> <p>The unique identifier for the newly created object</p> <p>datatracker.ietf.org/doc/html/rfc9110#name-etag</p>"},{"location":"api/put/#obstore.PutResult.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str | None\n</code></pre> <p>A version indicator for the newly created object.</p>"},{"location":"api/put/#obstore.UpdateVersion","title":"obstore.UpdateVersion","text":"<p>               Bases: <code>TypedDict</code></p> <p>Uniquely identifies a version of an object to update.</p> <p>Stores will use differing combinations of <code>e_tag</code> and <code>version</code> to provide conditional updates, and it is therefore recommended applications preserve both</p>"},{"location":"api/put/#obstore.UpdateVersion.e_tag","title":"e_tag  <code>instance-attribute</code>","text":"<pre><code>e_tag: str | None\n</code></pre> <p>The unique identifier for the newly created object.</p> <p>datatracker.ietf.org/doc/html/rfc9110#name-etag</p>"},{"location":"api/put/#obstore.UpdateVersion.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str | None\n</code></pre> <p>A version indicator for the newly created object.</p>"},{"location":"api/put/#obstore.PutMode","title":"obstore.PutMode  <code>module-attribute</code>","text":"<pre><code>PutMode: TypeAlias = Literal['create', 'overwrite'] | UpdateVersion\n</code></pre> <p>Configure preconditions for the put operation</p> <p>There are three modes:</p> <ul> <li>Overwrite: Perform an atomic write operation, overwriting any object present at the   provided path.</li> <li>Create: Perform an atomic write operation, returning   <code>AlreadyExistsError</code> if an object already   exists at the provided path.</li> <li>Update: Perform an atomic write operation if the current version of the object matches   the provided <code>UpdateVersion</code>, returning   <code>PreconditionError</code> otherwise.</li> </ul> <p>If a string is provided, it must be one of:</p> <ul> <li><code>\"overwrite\"</code></li> <li><code>\"create\"</code></li> </ul> <p>If a <code>dict</code> is provided, it must meet the criteria of <code>UpdateVersion</code>.</p>"},{"location":"api/rename/","title":"Rename","text":""},{"location":"api/rename/#obstore.rename","title":"obstore.rename","text":"<pre><code>rename(\n    store: ObjectStore, from_: str, to: str, *, overwrite: bool = True\n) -&gt; None\n</code></pre> <p>Move an object from one path to another in the same object store.</p> <p>By default, this is implemented as a copy and then delete source. It may not check when deleting source that it was the same object that was originally copied.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>ObjectStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>from_</code>               (<code>str</code>)           \u2013            <p>Source path</p> </li> <li> <code>to</code>               (<code>str</code>)           \u2013            <p>Destination path</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>overwrite</code>               (<code>bool</code>)           \u2013            <p>If <code>True</code>, if there exists an object at the destination, it will be overwritten. If <code>False</code>, will return an error if the destination already has an object.</p> </li> </ul>"},{"location":"api/rename/#obstore.rename_async","title":"obstore.rename_async  <code>async</code>","text":"<pre><code>rename_async(\n    store: ObjectStore, from_: str, to: str, *, overwrite: bool = True\n) -&gt; None\n</code></pre> <p>Call <code>rename</code> asynchronously.</p> <p>Refer to the documentation for rename.</p>"},{"location":"api/sign/","title":"Sign","text":""},{"location":"api/sign/#obstore.sign","title":"obstore.sign","text":"<pre><code>sign(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: str,\n    expires_in: timedelta,\n) -&gt; str\n</code></pre><pre><code>sign(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: Sequence[str],\n    expires_in: timedelta,\n) -&gt; list[str]\n</code></pre> <pre><code>sign(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: str | Sequence[str],\n    expires_in: timedelta,\n) -&gt; str | list[str]\n</code></pre> <p>Create a signed URL.</p> <p>Given the intended <code>method</code> and <code>paths</code> to use and the desired length of time for which the URL should be valid, return a signed URL created with the object store implementation's credentials such that the URL can be handed to something that doesn't have access to the object store's credentials, to allow limited access to the object store.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>SignCapableStore</code>)           \u2013            <p>The ObjectStore instance to use.</p> </li> <li> <code>method</code>               (<code>HTTP_METHOD</code>)           \u2013            <p>The HTTP method to use.</p> </li> <li> <code>paths</code>               (<code>str | Sequence[str]</code>)           \u2013            <p>The path(s) within ObjectStore to retrieve. If</p> </li> <li> <code>expires_in</code>               (<code>timedelta</code>)           \u2013            <p>How long the signed URL(s) should be valid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | list[str]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/sign/#obstore.sign_async","title":"obstore.sign_async  <code>async</code>","text":"<pre><code>sign_async(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: str,\n    expires_in: timedelta,\n) -&gt; str\n</code></pre><pre><code>sign_async(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: Sequence[str],\n    expires_in: timedelta,\n) -&gt; list[str]\n</code></pre> <pre><code>sign_async(\n    store: SignCapableStore,\n    method: HTTP_METHOD,\n    paths: str | Sequence[str],\n    expires_in: timedelta,\n) -&gt; str | list[str]\n</code></pre> <p>Call <code>sign</code> asynchronously.</p> <p>Refer to the documentation for sign.</p>"},{"location":"api/sign/#obstore.SignCapableStore","title":"obstore.SignCapableStore  <code>module-attribute</code>","text":"<pre><code>SignCapableStore: TypeAlias = AzureStore | GCSStore | S3Store\n</code></pre> <p>ObjectStore instances that are capable of signing.</p>"},{"location":"api/sign/#obstore.HTTP_METHOD","title":"obstore.HTTP_METHOD  <code>module-attribute</code>","text":"<pre><code>HTTP_METHOD: TypeAlias = Literal[\n    \"GET\",\n    \"PUT\",\n    \"POST\",\n    \"HEAD\",\n    \"PATCH\",\n    \"TRACE\",\n    \"DELETE\",\n    \"OPTIONS\",\n    \"CONNECT\",\n]\n</code></pre> <p>Allowed HTTP Methods for signing.</p>"},{"location":"api/auth/boto3/","title":"Boto3","text":""},{"location":"api/auth/boto3/#obstore.auth.boto3.Boto3CredentialProvider","title":"obstore.auth.boto3.Boto3CredentialProvider","text":"<p>A CredentialProvider for S3Store that uses <code>boto3.session.Session</code>.</p> <p>If the provided session has a <code>region_name</code> set, that will be passed down to the store.</p>"},{"location":"api/auth/boto3/#obstore.auth.boto3.Boto3CredentialProvider.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; S3Credential\n</code></pre> <p>Fetch credentials.</p>"},{"location":"api/auth/boto3/#obstore.auth.boto3.Boto3CredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    session: Session | Session | None = None,\n    *,\n    ttl: timedelta = timedelta(minutes=30),\n) -&gt; None\n</code></pre> <p>Create a new Boto3CredentialProvider.</p> <p>This will call <code>session.get_credentials</code> to get a <code>botocore.credentials.Credentials</code> object. Each token refresh will call <code>credentials.get_frozen_credentials</code>.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session | Session | None</code>, default:                   <code>None</code> )           \u2013            <p>A boto3 session to use for providing credentials. Defaults to None, in which case a new <code>boto3.Session</code> will be used.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>ttl</code>               (<code>timedelta</code>)           \u2013            <p>The length of time each result from <code>get_frozen_credentials</code> should live. Defaults to timedelta(minutes=30).</p> </li> </ul>"},{"location":"api/auth/boto3/#obstore.auth.boto3.StsCredentialProvider","title":"obstore.auth.boto3.StsCredentialProvider","text":"<p>A CredentialProvider for S3Store that uses <code>STS.Client.assume_role</code>.</p> <p>If the provided session has a <code>region_name</code> set, that will be passed down to the store.</p>"},{"location":"api/auth/boto3/#obstore.auth.boto3.StsCredentialProvider.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; S3Credential\n</code></pre> <p>Fetch credentials.</p>"},{"location":"api/auth/boto3/#obstore.auth.boto3.StsCredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    session: Session | None = None,\n    **kwargs: Unpack[AssumeRoleRequestRequestTypeDef],\n) -&gt; None\n</code></pre> <p>Create a new StsCredentialProvider.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session | None</code>, default:                   <code>None</code> )           \u2013            <p>A boto3 session to use for providing credentials. Defaults to None, in which case a new <code>boto3.Session</code> will be used.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>kwargs</code>               (<code>Unpack[AssumeRoleRequestRequestTypeDef]</code>)           \u2013            <p>arguments passed on to <code>STS.Client.assume_role</code>.</p> </li> </ul>"},{"location":"api/auth/earthdata/","title":"NASA Earthdata","text":""},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataCredentialProvider","title":"obstore.auth.earthdata.NasaEarthdataCredentialProvider","text":"<p>A credential provider for accessing NASA Earthdata to be used with S3Store.</p> <p>This credential provider uses <code>requests</code>, and will error if that cannot be imported.</p> <p>NASA Earthdata supports public in-region direct S3 access. This credential provider automatically manages the S3 credentials.</p> <p>Note</p> <p>Note that you must be in the same AWS region (<code>us-west-2</code>) to use the credentials returned from this provider.</p> <p>Example:</p> <pre><code>from obstore.auth.earthdata import NasaEarthdataCredentialProvider\nfrom obstore.store import S3Store\n\ncredential_provider = NasaEarthdataCredentialProvider(username=\"...\", password=\"...\")\nstore = S3Store(\"bucket_name\", credential_provider=credential_provider)\n</code></pre>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataCredentialProvider.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; S3Credential\n</code></pre> <p>Request updated credentials.</p>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataCredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(username: str, password: str) -&gt; None\n</code></pre> <p>Create a new NasaEarthdataCredentialProvider.</p> <p>Parameters:</p> <ul> <li> <code>username</code>               (<code>str</code>)           \u2013            <p>Username to NASA Earthdata.</p> </li> <li> <code>password</code>               (<code>str</code>)           \u2013            <p>Password to NASA Earthdata.</p> </li> </ul>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataCredentialProvider.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the underlying session.</p> <p>You should call this method after you've finished all obstore calls to close the underlying requests.Session.</p>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataAsyncCredentialProvider","title":"obstore.auth.earthdata.NasaEarthdataAsyncCredentialProvider","text":"<p>An async credential provider for accessing NASA Earthdata to be used with S3Store.</p> <p>This credential provider should be preferred over the synchronous NasaEarthdataCredentialProvider whenever you're using async obstore methods.</p> <p>This credential provider uses <code>aiohttp</code>, and will error if that cannot be imported.</p> <p>NASA Earthdata supports public in-region direct S3 access. This credential provider automatically manages the S3 credentials.</p> <p>Note</p> <p>Note that you must be in the same AWS region (<code>us-west-2</code>) to use the credentials returned from this provider.</p> <p>Example:</p> <pre><code>from obstore.auth.earthdata import NasaEarthdataAsyncCredentialProvider\nfrom obstore.store import S3Store\n\ncredential_provider = NasaEarthdataAsyncCredentialProvider(\n    username=\"...\",\n    password=\"...\",\n)\nstore = S3Store(\"bucket_name\", credential_provider=credential_provider)\n</code></pre>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataAsyncCredentialProvider.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__() -&gt; S3Credential\n</code></pre> <p>Request updated credentials.</p>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataAsyncCredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(username: str, password: str) -&gt; None\n</code></pre> <p>Create a new NasaEarthdataAsyncCredentialProvider.</p> <p>Parameters:</p> <ul> <li> <code>username</code>               (<code>str</code>)           \u2013            <p>Username to NASA Earthdata.</p> </li> <li> <code>password</code>               (<code>str</code>)           \u2013            <p>Password to NASA Earthdata.</p> </li> </ul>"},{"location":"api/auth/earthdata/#obstore.auth.earthdata.NasaEarthdataAsyncCredentialProvider.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the underlying session.</p> <p>You should call this method after you've finished all obstore calls to close the underlying aiohttp.ClientSession.</p>"},{"location":"api/auth/google/","title":"Google Auth","text":""},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthCredentialProvider","title":"obstore.auth.google.GoogleAuthCredentialProvider","text":"<p>A CredentialProvider for GCSStore that uses <code>google.auth</code>.</p> <p>This credential provider uses <code>google-auth</code> and <code>requests</code>, and will error if those cannot be imported.</p> <p>Example:</p> <pre><code>from obstore.auth.google import GoogleAuthCredentialProvider\nfrom obstore.store import GCSStore\n\ncredential_provider = GoogleAuthCredentialProvider(credentials=...)\nstore = GCSStore(\"bucket_name\", credential_provider=credential_provider)\n</code></pre>"},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthCredentialProvider.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; GCSCredential\n</code></pre> <p>Fetch the credentials.</p>"},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthCredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    credentials: Credentials | None = None,\n    *,\n    request: Request | None = None,\n    refresh_threshold: timedelta = timedelta(minutes=3, seconds=45),\n) -&gt; None\n</code></pre> <p>Create a new GoogleAuthCredentialProvider.</p> <p>Parameters:</p> <ul> <li> <code>credentials</code>               (<code>Credentials | None</code>, default:                   <code>None</code> )           \u2013            <p>Credentials to use for this provider. Defaults to <code>None</code>, in which case <code>google.auth.default</code> will be called to find application default credentials.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>request</code>               (<code>Request | None</code>)           \u2013            <p>The Request instance to use for refreshing the token. This can be set to reuse an existing <code>requests.Session</code>. Defaults to <code>None</code>, in which case a new <code>Request</code> will be instantiated.</p> </li> <li> <code>refresh_threshold</code>               (<code>timedelta</code>)           \u2013            <p>The length of time before the token timeout when a new token should be requested. Defaults to <code>timedelta(minutes=3, seconds=45)</code> (suggested here).</p> </li> </ul>"},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthAsyncCredentialProvider","title":"obstore.auth.google.GoogleAuthAsyncCredentialProvider","text":"<p>An async CredentialProvider for GCSStore that uses <code>google.auth</code>.</p> <p>This credential provider should be preferred over the synchronous GoogleAuthCredentialProvider whenever you're using async obstore methods.</p> <p>This credential provider uses <code>google-auth</code> and <code>aiohttp</code>, and will error if those cannot be imported.</p> <p>Example:</p> <pre><code>from obstore.auth.google import GoogleAuthAsyncCredentialProvider\nfrom obstore.store import GCSStore\n\ncredential_provider = GoogleAuthAsyncCredentialProvider(credentials=...)\nstore = GCSStore(\"bucket_name\", credential_provider=credential_provider)\n</code></pre>"},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthAsyncCredentialProvider.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__() -&gt; GCSCredential\n</code></pre> <p>Fetch the credentials.</p>"},{"location":"api/auth/google/#obstore.auth.google.GoogleAuthAsyncCredentialProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    credentials: Credentials | None = None,\n    *,\n    request: Request | None = None,\n    refresh_threshold: timedelta = timedelta(minutes=3, seconds=45),\n) -&gt; None\n</code></pre> <p>Create a new GoogleAuthCredentialProvider.</p> <p>Parameters:</p> <ul> <li> <code>credentials</code>               (<code>Credentials | None</code>, default:                   <code>None</code> )           \u2013            <p>Credentials to use for this provider. Defaults to <code>None</code>, in which case <code>google.auth._default_async.default_async</code> will be called to find application default credentials.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>request</code>               (<code>Request | None</code>)           \u2013            <p>The Request instance to use for refreshing the token. This can be set to reuse an existing <code>aiohttp.ClientSession</code>. Defaults to <code>None</code>, in which case a new <code>google.auth.transport._aiohttp_requests.Request</code> will be instantiated.</p> </li> <li> <code>refresh_threshold</code>               (<code>timedelta</code>)           \u2013            <p>The length of time before the token timeout when a new token should be requested. Defaults to <code>timedelta(minutes=3, seconds=45)</code> (suggested here).</p> </li> </ul>"},{"location":"api/store/","title":"ObjectStore","text":""},{"location":"api/store/#obstore.store.from_url","title":"obstore.store.from_url","text":"<pre><code>from_url(\n    url: str,\n    *,\n    config: S3Config | S3ConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: S3CredentialProvider | None = None,\n    **kwargs: Unpack[S3ConfigInput],\n) -&gt; ObjectStore\n</code></pre><pre><code>from_url(\n    url: str,\n    *,\n    config: GCSConfig | GCSConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: GCSCredentialProvider | None = None,\n    **kwargs: Unpack[GCSConfigInput],\n) -&gt; ObjectStore\n</code></pre><pre><code>from_url(\n    url: str,\n    *,\n    config: AzureConfig | AzureConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: AzureCredentialProvider | None = None,\n    **kwargs: Unpack[AzureConfigInput],\n) -&gt; ObjectStore\n</code></pre><pre><code>from_url(\n    url: str,\n    *,\n    config: None = None,\n    client_options: None = None,\n    retry_config: None = None,\n    automatic_cleanup: bool = False,\n    mkdir: bool = False,\n) -&gt; ObjectStore\n</code></pre> <pre><code>from_url(\n    url: str,\n    *,\n    config: S3ConfigInput | GCSConfigInput | AzureConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: Callable | None = None,\n    **kwargs: Any,\n) -&gt; ObjectStore\n</code></pre> <p>Easy construction of store by URL, identifying the relevant store.</p> <p>This will defer to a store-specific <code>from_url</code> constructor based on the provided <code>url</code>. E.g. passing <code>\"s3://bucket/path\"</code> will defer to <code>S3Store.from_url</code>.</p> <p>Supported formats:</p> <ul> <li><code>file:///path/to/my/file</code> -&gt; <code>LocalStore</code></li> <li><code>memory:///</code> -&gt; <code>MemoryStore</code></li> <li><code>s3://bucket/path</code> -&gt; <code>S3Store</code> (also supports <code>s3a</code>)</li> <li><code>gs://bucket/path</code> -&gt; <code>GCSStore</code></li> <li><code>az://account/container/path</code> -&gt; <code>AzureStore</code> (also   supports <code>adl</code>, <code>azure</code>, <code>abfs</code>, <code>abfss</code>)</li> <li><code>http://mydomain/path</code> -&gt; <code>HTTPStore</code></li> <li><code>https://mydomain/path</code> -&gt; <code>HTTPStore</code></li> </ul> <p>There are also special cases for AWS and Azure for <code>https://{host?}/path</code> paths:</p> <ul> <li><code>dfs.core.windows.net</code>, <code>blob.core.windows.net</code>, <code>dfs.fabric.microsoft.com</code>,   <code>blob.fabric.microsoft.com</code> -&gt; <code>AzureStore</code></li> <li><code>amazonaws.com</code> -&gt; <code>S3Store</code></li> <li><code>r2.cloudflarestorage.com</code> -&gt; <code>S3Store</code></li> </ul> <p>Note</p> <p>For best static typing, use the constructors on individual store classes directly.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>well-known storage URL.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>config</code>               (<code>S3ConfigInput | GCSConfigInput | AzureConfigInput | None</code>)           \u2013            <p>per-store Configuration. Values in this config will override values inferred from the url. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>Callable | None</code>)           \u2013            <p>A callback to provide custom credentials to the underlying store classes.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>)           \u2013            <p>per-store configuration passed down to store-specific builders.</p> </li> </ul>"},{"location":"api/store/#obstore.store.ObjectStore","title":"obstore.store.ObjectStore  <code>module-attribute</code>","text":"<pre><code>ObjectStore: TypeAlias = (\n    AzureStore | GCSStore | HTTPStore | S3Store | LocalStore | MemoryStore\n)\n</code></pre> <p>All supported ObjectStore implementations.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import ObjectStore\n</code></pre>"},{"location":"api/store/aws/","title":"AWS S3","text":""},{"location":"api/store/aws/#obstore.store.S3Store","title":"obstore.store.S3Store","text":"<p>Interface to an Amazon S3 bucket.</p> <p>All constructors will check for environment variables. All environment variables starting with <code>AWS_</code> will be evaluated. Names must match keys from <code>S3ConfigInput</code>. Only upper-case environment variables are accepted.</p> <p>Some examples of variables extracted from environment:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> -&gt; access_key_id</li> <li><code>AWS_SECRET_ACCESS_KEY</code> -&gt; secret_access_key</li> <li><code>AWS_DEFAULT_REGION</code> -&gt; region</li> <li><code>AWS_ENDPOINT</code> -&gt; endpoint</li> <li><code>AWS_SESSION_TOKEN</code> -&gt; token</li> <li><code>AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</code> -&gt; docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</li> <li><code>AWS_REQUEST_PAYER</code> -&gt; set to \"true\" to permit requester-pays connections.</li> </ul> <p>Examples:</p> <p>Using requester-pays buckets:</p> <p>Pass <code>request_payer=True</code> as a keyword argument or have <code>AWS_REQUESTER_PAYS=True</code> set in the environment.</p> <p>Anonymous requests:</p> <p>Pass <code>skip_signature=True</code> as a keyword argument or have <code>AWS_SKIP_SIGNATURE=True</code> set in the environment.</p>"},{"location":"api/store/aws/#obstore.store.S3Store.client_options","title":"client_options  <code>property</code>","text":"<pre><code>client_options: ClientConfig | None\n</code></pre> <p>Get the store's client configuration.</p>"},{"location":"api/store/aws/#obstore.store.S3Store.config","title":"config  <code>property</code>","text":"<pre><code>config: S3Config\n</code></pre> <p>Get the underlying S3 config parameters.</p>"},{"location":"api/store/aws/#obstore.store.S3Store.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix: str | None\n</code></pre> <p>Get the prefix applied to all operations in this store, if any.</p>"},{"location":"api/store/aws/#obstore.store.S3Store.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig | None\n</code></pre> <p>Get the store's retry configuration.</p>"},{"location":"api/store/aws/#obstore.store.S3Store.__init__","title":"__init__","text":"<pre><code>__init__(\n    bucket: str | None = None,\n    *,\n    prefix: str | None = None,\n    config: S3Config | S3ConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: S3CredentialProvider | None = None,\n    **kwargs: Unpack[S3ConfigInput],\n) -&gt; None\n</code></pre> <p>Create a new S3Store.</p> <p>Parameters:</p> <ul> <li> <code>bucket</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The AWS bucket to use.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | None</code>)           \u2013            <p>A prefix within the bucket to use for all operations.</p> </li> <li> <code>config</code>               (<code>S3Config | S3ConfigInput | None</code>)           \u2013            <p>AWS configuration. Values in this config will override values inferred from the environment. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>S3CredentialProvider | None</code>)           \u2013            <p>A callback to provide custom S3 credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[S3ConfigInput]</code>)           \u2013            <p>AWS configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>S3Store</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3Store.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    *,\n    config: S3Config | S3ConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: S3CredentialProvider | None = None,\n    **kwargs: Unpack[S3ConfigInput],\n) -&gt; S3Store\n</code></pre> <p>Parse available connection info from a well-known storage URL.</p> <p>The supported url schemes are:</p> <ul> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>s3a://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>https://s3.&lt;region&gt;.amazonaws.com/&lt;bucket&gt;</code></li> <li><code>https://&lt;bucket&gt;.s3.&lt;region&gt;.amazonaws.com</code></li> <li><code>https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket</code></li> </ul> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>well-known storage URL.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>config</code>               (<code>S3Config | S3ConfigInput | None</code>)           \u2013            <p>AWS Configuration. Values in this config will override values inferred from the url. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>S3CredentialProvider | None</code>)           \u2013            <p>A callback to provide custom S3 credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[S3ConfigInput]</code>)           \u2013            <p>AWS configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>S3Store</code>           \u2013            <p>S3Store</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput","title":"obstore.store.S3ConfigInput","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters for S3Store.</p> <p>There are duplicates of many parameters, and parameters can be either upper or lower case. Not all parameters are required.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import S3ConfigInput\n</code></pre>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.ACCESS_KEY_ID","title":"ACCESS_KEY_ID  <code>instance-attribute</code>","text":"<pre><code>ACCESS_KEY_ID: str\n</code></pre> <p>AWS Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_ACCESS_KEY_ID","title":"AWS_ACCESS_KEY_ID  <code>instance-attribute</code>","text":"<pre><code>AWS_ACCESS_KEY_ID: str\n</code></pre> <p>AWS Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_BUCKET","title":"AWS_BUCKET  <code>instance-attribute</code>","text":"<pre><code>AWS_BUCKET: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_BUCKET_NAME","title":"AWS_BUCKET_NAME  <code>instance-attribute</code>","text":"<pre><code>AWS_BUCKET_NAME: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_CHECKSUM_ALGORITHM","title":"AWS_CHECKSUM_ALGORITHM  <code>instance-attribute</code>","text":"<pre><code>AWS_CHECKSUM_ALGORITHM: S3ChecksumAlgorithm | str\n</code></pre> <p>Sets the checksum algorithm which has to be used for object integrity check during upload.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_CONDITIONAL_PUT","title":"AWS_CONDITIONAL_PUT  <code>instance-attribute</code>","text":"<pre><code>AWS_CONDITIONAL_PUT: str\n</code></pre> <p>Configure how to provide conditional put support</p> <p>Supported values:</p> <ul> <li> <p><code>\"etag\"</code> (default): Supported for S3-compatible stores that support conditional     put using the standard HTTP precondition headers <code>If-Match</code> and     <code>If-None-Match</code>.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>: The name of a DynamoDB table to use for coordination.</p> <p>This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI","title":"AWS_CONTAINER_CREDENTIALS_RELATIVE_URI  <code>instance-attribute</code>","text":"<pre><code>AWS_CONTAINER_CREDENTIALS_RELATIVE_URI: str\n</code></pre> <p>Set the container credentials relative URI</p> <p>docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_COPY_IF_NOT_EXISTS","title":"AWS_COPY_IF_NOT_EXISTS  <code>instance-attribute</code>","text":"<pre><code>AWS_COPY_IF_NOT_EXISTS: Literal['multipart'] | str\n</code></pre> <p>Configure how to provide \"copy if not exists\".</p> <p>Supported values:</p> <ul> <li> <p><code>\"multipart\"</code>:</p> <p>Native Amazon S3 supports copy if not exists through a multipart upload where the upload copies an existing object and is completed only if the new object does not already exist.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> does not copy tags or attributes from the source object.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> makes only a best effort attempt to clean up the multipart upload if the copy operation fails. Consider using a lifecycle rule to automatically clean up abandoned multipart uploads.</p> </li> <li> <p><code>\"header:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;\"</code>:</p> <p>Some S3-compatible stores, such as Cloudflare R2, support copy if not exists semantics through custom headers.</p> <p>If set, <code>copy_if_not_exists</code> will perform a normal copy operation with the provided header pair, and expect the store to fail with <code>412 Precondition Failed</code> if the destination file already exists.</p> <p>For example <code>header: cf-copy-destination-if-none-match: *</code>, would set the header <code>cf-copy-destination-if-none-match</code> to <code>*</code>.</p> </li> <li> <p><code>\"header-with-status:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;:&lt;STATUS&gt;\"</code>:</p> <p>The same as the header variant above but allows custom status code checking, for object stores that return values other than 412.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>:</p> <p>The name of a DynamoDB table to use for coordination.</p> <p>The default timeout is used if not specified. This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_DEFAULT_REGION","title":"AWS_DEFAULT_REGION  <code>instance-attribute</code>","text":"<pre><code>AWS_DEFAULT_REGION: S3Regions | str\n</code></pre> <p>Default region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_DISABLE_TAGGING","title":"AWS_DISABLE_TAGGING  <code>instance-attribute</code>","text":"<pre><code>AWS_DISABLE_TAGGING: bool\n</code></pre> <p>Disable tagging objects. This can be desirable if not supported by the backing store.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_ENDPOINT","title":"AWS_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AWS_ENDPOINT: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_ENDPOINT_URL","title":"AWS_ENDPOINT_URL  <code>instance-attribute</code>","text":"<pre><code>AWS_ENDPOINT_URL: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_IMDSV1_FALLBACK","title":"AWS_IMDSV1_FALLBACK  <code>instance-attribute</code>","text":"<pre><code>AWS_IMDSV1_FALLBACK: bool\n</code></pre> <p>Fall back to ImdsV1</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_METADATA_ENDPOINT","title":"AWS_METADATA_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AWS_METADATA_ENDPOINT: str\n</code></pre> <p>Set the instance metadata endpoint</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_REGION","title":"AWS_REGION  <code>instance-attribute</code>","text":"<pre><code>AWS_REGION: S3Regions | str\n</code></pre> <p>Region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_REQUEST_PAYER","title":"AWS_REQUEST_PAYER  <code>instance-attribute</code>","text":"<pre><code>AWS_REQUEST_PAYER: bool\n</code></pre> <p>If <code>True</code>, enable operations on requester-pays buckets.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_S3_EXPRESS","title":"AWS_S3_EXPRESS  <code>instance-attribute</code>","text":"<pre><code>AWS_S3_EXPRESS: str\n</code></pre> <p>Enable Support for S3 Express One Zone</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SECRET_ACCESS_KEY","title":"AWS_SECRET_ACCESS_KEY  <code>instance-attribute</code>","text":"<pre><code>AWS_SECRET_ACCESS_KEY: str\n</code></pre> <p>Secret Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SERVER_SIDE_ENCRYPTION","title":"AWS_SERVER_SIDE_ENCRYPTION  <code>instance-attribute</code>","text":"<pre><code>AWS_SERVER_SIDE_ENCRYPTION: S3EncryptionAlgorithm | str\n</code></pre> <p>Type of encryption to use.</p> <p>If set, must be one of:</p> <ul> <li><code>\"AES256\"</code> (SSE-S3)</li> <li><code>\"aws:kms\"</code> (SSE-KMS)</li> <li><code>\"aws:kms:dsse\"</code> (DSSE-KMS)</li> <li><code>\"sse-c\"</code></li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SESSION_TOKEN","title":"AWS_SESSION_TOKEN  <code>instance-attribute</code>","text":"<pre><code>AWS_SESSION_TOKEN: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SKIP_SIGNATURE","title":"AWS_SKIP_SIGNATURE  <code>instance-attribute</code>","text":"<pre><code>AWS_SKIP_SIGNATURE: bool\n</code></pre> <p>If <code>True</code>, S3Store will not fetch credentials and will not sign requests.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SSE_BUCKET_KEY_ENABLED","title":"AWS_SSE_BUCKET_KEY_ENABLED  <code>instance-attribute</code>","text":"<pre><code>AWS_SSE_BUCKET_KEY_ENABLED: bool\n</code></pre> <p>If set to <code>True</code>, will use the bucket's default KMS key for server-side encryption. If set to <code>False</code>, will disable the use of the bucket's default KMS key for server-side encryption.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SSE_CUSTOMER_KEY_BASE64","title":"AWS_SSE_CUSTOMER_KEY_BASE64  <code>instance-attribute</code>","text":"<pre><code>AWS_SSE_CUSTOMER_KEY_BASE64: str\n</code></pre> <p>The base64 encoded, 256-bit customer encryption key to use for server-side encryption. If set, the server side encryption config value must be <code>\"sse-c\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_SSE_KMS_KEY_ID","title":"AWS_SSE_KMS_KEY_ID  <code>instance-attribute</code>","text":"<pre><code>AWS_SSE_KMS_KEY_ID: str\n</code></pre> <p>The KMS key ID to use for server-side encryption.</p> <p>If set, the server side encrypting config value must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_TOKEN","title":"AWS_TOKEN  <code>instance-attribute</code>","text":"<pre><code>AWS_TOKEN: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_UNSIGNED_PAYLOAD","title":"AWS_UNSIGNED_PAYLOAD  <code>instance-attribute</code>","text":"<pre><code>AWS_UNSIGNED_PAYLOAD: bool\n</code></pre> <p>Avoid computing payload checksum when calculating signature.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.AWS_VIRTUAL_HOSTED_STYLE_REQUEST","title":"AWS_VIRTUAL_HOSTED_STYLE_REQUEST  <code>instance-attribute</code>","text":"<pre><code>AWS_VIRTUAL_HOSTED_STYLE_REQUEST: bool\n</code></pre> <p>If virtual hosted style request has to be used.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.BUCKET","title":"BUCKET  <code>instance-attribute</code>","text":"<pre><code>BUCKET: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.BUCKET_NAME","title":"BUCKET_NAME  <code>instance-attribute</code>","text":"<pre><code>BUCKET_NAME: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.CHECKSUM_ALGORITHM","title":"CHECKSUM_ALGORITHM  <code>instance-attribute</code>","text":"<pre><code>CHECKSUM_ALGORITHM: S3ChecksumAlgorithm | str\n</code></pre> <p>Sets the checksum algorithm which has to be used for object integrity check during upload.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.CONDITIONAL_PUT","title":"CONDITIONAL_PUT  <code>instance-attribute</code>","text":"<pre><code>CONDITIONAL_PUT: str\n</code></pre> <p>Configure how to provide conditional put support</p> <p>Supported values:</p> <ul> <li> <p><code>\"etag\"</code> (default): Supported for S3-compatible stores that support conditional     put using the standard HTTP precondition headers <code>If-Match</code> and     <code>If-None-Match</code>.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>: The name of a DynamoDB table to use for coordination.</p> <p>This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.COPY_IF_NOT_EXISTS","title":"COPY_IF_NOT_EXISTS  <code>instance-attribute</code>","text":"<pre><code>COPY_IF_NOT_EXISTS: Literal['multipart'] | str\n</code></pre> <p>Configure how to provide \"copy if not exists\".</p> <p>Supported values:</p> <ul> <li> <p><code>\"multipart\"</code>:</p> <p>Native Amazon S3 supports copy if not exists through a multipart upload where the upload copies an existing object and is completed only if the new object does not already exist.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> does not copy tags or attributes from the source object.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> makes only a best effort attempt to clean up the multipart upload if the copy operation fails. Consider using a lifecycle rule to automatically clean up abandoned multipart uploads.</p> </li> <li> <p><code>\"header:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;\"</code>:</p> <p>Some S3-compatible stores, such as Cloudflare R2, support copy if not exists semantics through custom headers.</p> <p>If set, <code>copy_if_not_exists</code> will perform a normal copy operation with the provided header pair, and expect the store to fail with <code>412 Precondition Failed</code> if the destination file already exists.</p> <p>For example <code>header: cf-copy-destination-if-none-match: *</code>, would set the header <code>cf-copy-destination-if-none-match</code> to <code>*</code>.</p> </li> <li> <p><code>\"header-with-status:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;:&lt;STATUS&gt;\"</code>:</p> <p>The same as the header variant above but allows custom status code checking, for object stores that return values other than 412.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>:</p> <p>The name of a DynamoDB table to use for coordination.</p> <p>The default timeout is used if not specified. This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.DEFAULT_REGION","title":"DEFAULT_REGION  <code>instance-attribute</code>","text":"<pre><code>DEFAULT_REGION: S3Regions | str\n</code></pre> <p>Default region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.DISABLE_TAGGING","title":"DISABLE_TAGGING  <code>instance-attribute</code>","text":"<pre><code>DISABLE_TAGGING: bool\n</code></pre> <p>Disable tagging objects. This can be desirable if not supported by the backing store.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.ENDPOINT","title":"ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>ENDPOINT: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.ENDPOINT_URL","title":"ENDPOINT_URL  <code>instance-attribute</code>","text":"<pre><code>ENDPOINT_URL: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.IMDSV1_FALLBACK","title":"IMDSV1_FALLBACK  <code>instance-attribute</code>","text":"<pre><code>IMDSV1_FALLBACK: bool\n</code></pre> <p>Fall back to ImdsV1</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.METADATA_ENDPOINT","title":"METADATA_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>METADATA_ENDPOINT: str\n</code></pre> <p>Set the instance metadata endpoint</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.REGION","title":"REGION  <code>instance-attribute</code>","text":"<pre><code>REGION: S3Regions | str\n</code></pre> <p>Region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.REQUEST_PAYER","title":"REQUEST_PAYER  <code>instance-attribute</code>","text":"<pre><code>REQUEST_PAYER: bool\n</code></pre> <p>If <code>True</code>, enable operations on requester-pays buckets.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.S3_EXPRESS","title":"S3_EXPRESS  <code>instance-attribute</code>","text":"<pre><code>S3_EXPRESS: str\n</code></pre> <p>Enable Support for S3 Express One Zone</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.SECRET_ACCESS_KEY","title":"SECRET_ACCESS_KEY  <code>instance-attribute</code>","text":"<pre><code>SECRET_ACCESS_KEY: str\n</code></pre> <p>Secret Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.SESSION_TOKEN","title":"SESSION_TOKEN  <code>instance-attribute</code>","text":"<pre><code>SESSION_TOKEN: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.SKIP_SIGNATURE","title":"SKIP_SIGNATURE  <code>instance-attribute</code>","text":"<pre><code>SKIP_SIGNATURE: bool\n</code></pre> <p>If <code>True</code>, S3Store will not fetch credentials and will not sign requests.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.TOKEN","title":"TOKEN  <code>instance-attribute</code>","text":"<pre><code>TOKEN: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.UNSIGNED_PAYLOAD","title":"UNSIGNED_PAYLOAD  <code>instance-attribute</code>","text":"<pre><code>UNSIGNED_PAYLOAD: bool\n</code></pre> <p>Avoid computing payload checksum when calculating signature.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.VIRTUAL_HOSTED_STYLE_REQUEST","title":"VIRTUAL_HOSTED_STYLE_REQUEST  <code>instance-attribute</code>","text":"<pre><code>VIRTUAL_HOSTED_STYLE_REQUEST: bool\n</code></pre> <p>If virtual hosted style request has to be used.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.access_key_id","title":"access_key_id  <code>instance-attribute</code>","text":"<pre><code>access_key_id: str\n</code></pre> <p>AWS Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_access_key_id","title":"aws_access_key_id  <code>instance-attribute</code>","text":"<pre><code>aws_access_key_id: str\n</code></pre> <p>AWS Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_bucket","title":"aws_bucket  <code>instance-attribute</code>","text":"<pre><code>aws_bucket: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_bucket_name","title":"aws_bucket_name  <code>instance-attribute</code>","text":"<pre><code>aws_bucket_name: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_checksum_algorithm","title":"aws_checksum_algorithm  <code>instance-attribute</code>","text":"<pre><code>aws_checksum_algorithm: S3ChecksumAlgorithm | str\n</code></pre> <p>Sets the checksum algorithm which has to be used for object integrity check during upload.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_conditional_put","title":"aws_conditional_put  <code>instance-attribute</code>","text":"<pre><code>aws_conditional_put: str\n</code></pre> <p>Configure how to provide conditional put support</p> <p>Supported values:</p> <ul> <li> <p><code>\"etag\"</code> (default): Supported for S3-compatible stores that support conditional     put using the standard HTTP precondition headers <code>If-Match</code> and     <code>If-None-Match</code>.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>: The name of a DynamoDB table to use for coordination.</p> <p>This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_container_credentials_relative_uri","title":"aws_container_credentials_relative_uri  <code>instance-attribute</code>","text":"<pre><code>aws_container_credentials_relative_uri: str\n</code></pre> <p>Set the container credentials relative URI</p> <p>docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_copy_if_not_exists","title":"aws_copy_if_not_exists  <code>instance-attribute</code>","text":"<pre><code>aws_copy_if_not_exists: Literal['multipart'] | str\n</code></pre> <p>Configure how to provide \"copy if not exists\".</p> <p>Supported values:</p> <ul> <li> <p><code>\"multipart\"</code>:</p> <p>Native Amazon S3 supports copy if not exists through a multipart upload where the upload copies an existing object and is completed only if the new object does not already exist.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> does not copy tags or attributes from the source object.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> makes only a best effort attempt to clean up the multipart upload if the copy operation fails. Consider using a lifecycle rule to automatically clean up abandoned multipart uploads.</p> </li> <li> <p><code>\"header:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;\"</code>:</p> <p>Some S3-compatible stores, such as Cloudflare R2, support copy if not exists semantics through custom headers.</p> <p>If set, <code>copy_if_not_exists</code> will perform a normal copy operation with the provided header pair, and expect the store to fail with <code>412 Precondition Failed</code> if the destination file already exists.</p> <p>For example <code>header: cf-copy-destination-if-none-match: *</code>, would set the header <code>cf-copy-destination-if-none-match</code> to <code>*</code>.</p> </li> <li> <p><code>\"header-with-status:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;:&lt;STATUS&gt;\"</code>:</p> <p>The same as the header variant above but allows custom status code checking, for object stores that return values other than 412.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>:</p> <p>The name of a DynamoDB table to use for coordination.</p> <p>The default timeout is used if not specified. This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_default_region","title":"aws_default_region  <code>instance-attribute</code>","text":"<pre><code>aws_default_region: S3Regions | str\n</code></pre> <p>Default region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_disable_tagging","title":"aws_disable_tagging  <code>instance-attribute</code>","text":"<pre><code>aws_disable_tagging: bool\n</code></pre> <p>Disable tagging objects. This can be desirable if not supported by the backing store.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_endpoint","title":"aws_endpoint  <code>instance-attribute</code>","text":"<pre><code>aws_endpoint: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_endpoint_url","title":"aws_endpoint_url  <code>instance-attribute</code>","text":"<pre><code>aws_endpoint_url: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_imdsv1_fallback","title":"aws_imdsv1_fallback  <code>instance-attribute</code>","text":"<pre><code>aws_imdsv1_fallback: bool\n</code></pre> <p>Fall back to ImdsV1</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_metadata_endpoint","title":"aws_metadata_endpoint  <code>instance-attribute</code>","text":"<pre><code>aws_metadata_endpoint: str\n</code></pre> <p>Set the instance metadata endpoint</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_region","title":"aws_region  <code>instance-attribute</code>","text":"<pre><code>aws_region: S3Regions | str\n</code></pre> <p>Region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_request_payer","title":"aws_request_payer  <code>instance-attribute</code>","text":"<pre><code>aws_request_payer: bool\n</code></pre> <p>If <code>True</code>, enable operations on requester-pays buckets.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_s3_express","title":"aws_s3_express  <code>instance-attribute</code>","text":"<pre><code>aws_s3_express: bool\n</code></pre> <p>Enable Support for S3 Express One Zone</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_secret_access_key","title":"aws_secret_access_key  <code>instance-attribute</code>","text":"<pre><code>aws_secret_access_key: str\n</code></pre> <p>Secret Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_server_side_encryption","title":"aws_server_side_encryption  <code>instance-attribute</code>","text":"<pre><code>aws_server_side_encryption: S3EncryptionAlgorithm | str\n</code></pre> <p>Type of encryption to use.</p> <p>If set, must be one of:</p> <ul> <li><code>\"AES256\"</code> (SSE-S3)</li> <li><code>\"aws:kms\"</code> (SSE-KMS)</li> <li><code>\"aws:kms:dsse\"</code> (DSSE-KMS)</li> <li><code>\"sse-c\"</code></li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_session_token","title":"aws_session_token  <code>instance-attribute</code>","text":"<pre><code>aws_session_token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_skip_signature","title":"aws_skip_signature  <code>instance-attribute</code>","text":"<pre><code>aws_skip_signature: bool\n</code></pre> <p>If <code>True</code>, S3Store will not fetch credentials and will not sign requests.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_sse_bucket_key_enabled","title":"aws_sse_bucket_key_enabled  <code>instance-attribute</code>","text":"<pre><code>aws_sse_bucket_key_enabled: bool\n</code></pre> <p>If set to <code>True</code>, will use the bucket's default KMS key for server-side encryption. If set to <code>False</code>, will disable the use of the bucket's default KMS key for server-side encryption.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_sse_customer_key_base64","title":"aws_sse_customer_key_base64  <code>instance-attribute</code>","text":"<pre><code>aws_sse_customer_key_base64: str\n</code></pre> <p>The base64 encoded, 256-bit customer encryption key to use for server-side encryption. If set, the server side encryption config value must be <code>\"sse-c\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_sse_kms_key_id","title":"aws_sse_kms_key_id  <code>instance-attribute</code>","text":"<pre><code>aws_sse_kms_key_id: str\n</code></pre> <p>The KMS key ID to use for server-side encryption.</p> <p>If set, the server side encryption config value must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_token","title":"aws_token  <code>instance-attribute</code>","text":"<pre><code>aws_token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_unsigned_payload","title":"aws_unsigned_payload  <code>instance-attribute</code>","text":"<pre><code>aws_unsigned_payload: bool\n</code></pre> <p>Avoid computing payload checksum when calculating signature.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.aws_virtual_hosted_style_request","title":"aws_virtual_hosted_style_request  <code>instance-attribute</code>","text":"<pre><code>aws_virtual_hosted_style_request: bool\n</code></pre> <p>If virtual hosted style request has to be used.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.bucket_name","title":"bucket_name  <code>instance-attribute</code>","text":"<pre><code>bucket_name: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.checksum_algorithm","title":"checksum_algorithm  <code>instance-attribute</code>","text":"<pre><code>checksum_algorithm: S3ChecksumAlgorithm | str\n</code></pre> <p>Sets the checksum algorithm which has to be used for object integrity check during upload.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.conditional_put","title":"conditional_put  <code>instance-attribute</code>","text":"<pre><code>conditional_put: str\n</code></pre> <p>Configure how to provide conditional put support</p> <p>Supported values:</p> <ul> <li> <p><code>\"etag\"</code> (default): Supported for S3-compatible stores that support conditional     put using the standard HTTP precondition headers <code>If-Match</code> and     <code>If-None-Match</code>.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>: The name of a DynamoDB table to use for coordination.</p> <p>This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.copy_if_not_exists","title":"copy_if_not_exists  <code>instance-attribute</code>","text":"<pre><code>copy_if_not_exists: Literal['multipart'] | str\n</code></pre> <p>Configure how to provide \"copy if not exists\".</p> <p>Supported values:</p> <ul> <li> <p><code>\"multipart\"</code>:</p> <p>Native Amazon S3 supports copy if not exists through a multipart upload where the upload copies an existing object and is completed only if the new object does not already exist.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> does not copy tags or attributes from the source object.</p> <p>Warning</p> <p>When using this mode, <code>copy_if_not_exists</code> makes only a best effort attempt to clean up the multipart upload if the copy operation fails. Consider using a lifecycle rule to automatically clean up abandoned multipart uploads.</p> </li> <li> <p><code>\"header:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;\"</code>:</p> <p>Some S3-compatible stores, such as Cloudflare R2, support copy if not exists semantics through custom headers.</p> <p>If set, <code>copy_if_not_exists</code> will perform a normal copy operation with the provided header pair, and expect the store to fail with <code>412 Precondition Failed</code> if the destination file already exists.</p> <p>For example <code>header: cf-copy-destination-if-none-match: *</code>, would set the header <code>cf-copy-destination-if-none-match</code> to <code>*</code>.</p> </li> <li> <p><code>\"header-with-status:&lt;HEADER_NAME&gt;:&lt;HEADER_VALUE&gt;:&lt;STATUS&gt;\"</code>:</p> <p>The same as the header variant above but allows custom status code checking, for object stores that return values other than 412.</p> </li> <li> <p><code>\"dynamo:&lt;TABLE_NAME&gt;\"</code> or <code>\"dynamo:&lt;TABLE_NAME&gt;:&lt;TIMEOUT_MILLIS&gt;\"</code>:</p> <p>The name of a DynamoDB table to use for coordination.</p> <p>The default timeout is used if not specified. This will use the same region, credentials and endpoint as configured for S3.</p> </li> </ul>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.default_region","title":"default_region  <code>instance-attribute</code>","text":"<pre><code>default_region: S3Regions | str\n</code></pre> <p>Default region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.disable_tagging","title":"disable_tagging  <code>instance-attribute</code>","text":"<pre><code>disable_tagging: bool\n</code></pre> <p>Disable tagging objects. This can be desirable if not supported by the backing store.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.endpoint","title":"endpoint  <code>instance-attribute</code>","text":"<pre><code>endpoint: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.endpoint_url","title":"endpoint_url  <code>instance-attribute</code>","text":"<pre><code>endpoint_url: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.imdsv1_fallback","title":"imdsv1_fallback  <code>instance-attribute</code>","text":"<pre><code>imdsv1_fallback: bool\n</code></pre> <p>Fall back to ImdsV1</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.metadata_endpoint","title":"metadata_endpoint  <code>instance-attribute</code>","text":"<pre><code>metadata_endpoint: str\n</code></pre> <p>Set the instance metadata endpoint</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.region","title":"region  <code>instance-attribute</code>","text":"<pre><code>region: S3Regions | str\n</code></pre> <p>Region</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.request_payer","title":"request_payer  <code>instance-attribute</code>","text":"<pre><code>request_payer: bool\n</code></pre> <p>If <code>True</code>, enable operations on requester-pays buckets.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.s3_express","title":"s3_express  <code>instance-attribute</code>","text":"<pre><code>s3_express: bool\n</code></pre> <p>Enable Support for S3 Express One Zone</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.secret_access_key","title":"secret_access_key  <code>instance-attribute</code>","text":"<pre><code>secret_access_key: str\n</code></pre> <p>Secret Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.session_token","title":"session_token  <code>instance-attribute</code>","text":"<pre><code>session_token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.skip_signature","title":"skip_signature  <code>instance-attribute</code>","text":"<pre><code>skip_signature: bool\n</code></pre> <p>If <code>True</code>, S3Store will not fetch credentials and will not sign requests.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.unsigned_payload","title":"unsigned_payload  <code>instance-attribute</code>","text":"<pre><code>unsigned_payload: bool\n</code></pre> <p>Avoid computing payload checksum when calculating signature.</p>"},{"location":"api/store/aws/#obstore.store.S3ConfigInput.virtual_hosted_style_request","title":"virtual_hosted_style_request  <code>instance-attribute</code>","text":"<pre><code>virtual_hosted_style_request: bool\n</code></pre> <p>If virtual hosted style request has to be used.</p>"},{"location":"api/store/aws/#obstore.store.S3Config","title":"obstore.store.S3Config","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters returned from S3Store.config.</p> <p>Note that this is a strict subset of the keys allowed for input into the store, see S3ConfigInput.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import S3Config\n</code></pre>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_access_key_id","title":"aws_access_key_id  <code>instance-attribute</code>","text":"<pre><code>aws_access_key_id: str\n</code></pre> <p>AWS Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_bucket","title":"aws_bucket  <code>instance-attribute</code>","text":"<pre><code>aws_bucket: str\n</code></pre> <p>Bucket name</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_checksum_algorithm","title":"aws_checksum_algorithm  <code>instance-attribute</code>","text":"<pre><code>aws_checksum_algorithm: S3ChecksumAlgorithm | str\n</code></pre> <p>Sets the checksum algorithm which has to be used for object integrity check during upload.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_conditional_put","title":"aws_conditional_put  <code>instance-attribute</code>","text":"<pre><code>aws_conditional_put: str\n</code></pre> <p>See <code>S3ConfigInput.aws_conditional_put</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_container_credentials_relative_uri","title":"aws_container_credentials_relative_uri  <code>instance-attribute</code>","text":"<pre><code>aws_container_credentials_relative_uri: str\n</code></pre> <p>See <code>S3ConfigInput.aws_container_credentials_relative_uri</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_copy_if_not_exists","title":"aws_copy_if_not_exists  <code>instance-attribute</code>","text":"<pre><code>aws_copy_if_not_exists: Literal['multipart'] | str\n</code></pre> <p>See <code>S3ConfigInput.aws_copy_if_not_exists</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_default_region","title":"aws_default_region  <code>instance-attribute</code>","text":"<pre><code>aws_default_region: S3Regions | str\n</code></pre> <p>Default region</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_disable_tagging","title":"aws_disable_tagging  <code>instance-attribute</code>","text":"<pre><code>aws_disable_tagging: bool\n</code></pre> <p>Disable tagging objects. This can be desirable if not supported by the backing store.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_endpoint","title":"aws_endpoint  <code>instance-attribute</code>","text":"<pre><code>aws_endpoint: str\n</code></pre> <p>Sets custom endpoint for communicating with AWS S3.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_imdsv1_fallback","title":"aws_imdsv1_fallback  <code>instance-attribute</code>","text":"<pre><code>aws_imdsv1_fallback: bool\n</code></pre> <p>Fall back to ImdsV1</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_metadata_endpoint","title":"aws_metadata_endpoint  <code>instance-attribute</code>","text":"<pre><code>aws_metadata_endpoint: str\n</code></pre> <p>Set the instance metadata endpoint</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_region","title":"aws_region  <code>instance-attribute</code>","text":"<pre><code>aws_region: S3Regions | str\n</code></pre> <p>Region</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_request_payer","title":"aws_request_payer  <code>instance-attribute</code>","text":"<pre><code>aws_request_payer: bool\n</code></pre> <p>If <code>True</code>, enable operations on requester-pays buckets.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_s3_express","title":"aws_s3_express  <code>instance-attribute</code>","text":"<pre><code>aws_s3_express: bool\n</code></pre> <p>Enable Support for S3 Express One Zone</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_secret_access_key","title":"aws_secret_access_key  <code>instance-attribute</code>","text":"<pre><code>aws_secret_access_key: str\n</code></pre> <p>Secret Access Key</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_server_side_encryption","title":"aws_server_side_encryption  <code>instance-attribute</code>","text":"<pre><code>aws_server_side_encryption: S3EncryptionAlgorithm | str\n</code></pre> <p>See <code>S3ConfigInput.aws_server_side_encryption</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_session_token","title":"aws_session_token  <code>instance-attribute</code>","text":"<pre><code>aws_session_token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_skip_signature","title":"aws_skip_signature  <code>instance-attribute</code>","text":"<pre><code>aws_skip_signature: bool\n</code></pre> <p>If <code>True</code>, S3Store will not fetch credentials and will not sign requests.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_sse_bucket_key_enabled","title":"aws_sse_bucket_key_enabled  <code>instance-attribute</code>","text":"<pre><code>aws_sse_bucket_key_enabled: bool\n</code></pre> <p>If set to <code>True</code>, will use the bucket's default KMS key for server-side encryption. If set to <code>False</code>, will disable the use of the bucket's default KMS key for server-side encryption.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_sse_customer_key_base64","title":"aws_sse_customer_key_base64  <code>instance-attribute</code>","text":"<pre><code>aws_sse_customer_key_base64: str\n</code></pre> <p>The base64 encoded, 256-bit customer encryption key to use for server-side encryption. If set, the server side encryption config value must be <code>\"sse-c\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_sse_kms_key_id","title":"aws_sse_kms_key_id  <code>instance-attribute</code>","text":"<pre><code>aws_sse_kms_key_id: str\n</code></pre> <p>The KMS key ID to use for server-side encryption.</p> <p>If set, the server side encryption config value must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_token","title":"aws_token  <code>instance-attribute</code>","text":"<pre><code>aws_token: str\n</code></pre> <p>Token to use for requests (passed to underlying provider)</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_unsigned_payload","title":"aws_unsigned_payload  <code>instance-attribute</code>","text":"<pre><code>aws_unsigned_payload: bool\n</code></pre> <p>Avoid computing payload checksum when calculating signature.</p>"},{"location":"api/store/aws/#obstore.store.S3Config.aws_virtual_hosted_style_request","title":"aws_virtual_hosted_style_request  <code>instance-attribute</code>","text":"<pre><code>aws_virtual_hosted_style_request: bool\n</code></pre> <p>If virtual hosted style request has to be used.</p>"},{"location":"api/store/aws/#obstore.store.S3Credential","title":"obstore.store.S3Credential","text":"<p>               Bases: <code>TypedDict</code></p> <p>An S3 credential.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import S3Credential\n</code></pre>"},{"location":"api/store/aws/#obstore.store.S3Credential.access_key_id","title":"access_key_id  <code>instance-attribute</code>","text":"<pre><code>access_key_id: str\n</code></pre> <p>AWS access key ID.</p>"},{"location":"api/store/aws/#obstore.store.S3Credential.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: datetime | None\n</code></pre> <p>Expiry datetime of credential. The datetime should have time zone set.</p> <p>If None, the credential will never expire.</p>"},{"location":"api/store/aws/#obstore.store.S3Credential.secret_access_key","title":"secret_access_key  <code>instance-attribute</code>","text":"<pre><code>secret_access_key: str\n</code></pre> <p>AWS secret access key</p>"},{"location":"api/store/aws/#obstore.store.S3Credential.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: NotRequired[str | None]\n</code></pre> <p>AWS token.</p>"},{"location":"api/store/aws/#obstore.store.S3CredentialProvider","title":"obstore.store.S3CredentialProvider","text":"<p>               Bases: <code>Protocol</code></p> <p>A type hint for a synchronous or asynchronous callback to provide custom S3 credentials.</p> <p>This should be passed into the <code>credential_provider</code> parameter of <code>S3Store</code>.</p> <p>Examples:</p> <p>Return static credentials that don't expire: <pre><code>def get_credentials() -&gt; S3Credential:\n    return {\n        \"access_key_id\": \"...\",\n        \"secret_access_key\": \"...\",\n        \"token\": None,\n        \"expires_at\": None,\n    }\n</code></pre></p> <p>Return static credentials that are valid for 5 minutes: <pre><code>from datetime import datetime, timedelta, UTC\n\nasync def get_credentials() -&gt; S3Credential:\n    return {\n        \"access_key_id\": \"...\",\n        \"secret_access_key\": \"...\",\n        \"token\": None,\n        \"expires_at\": datetime.now(UTC) + timedelta(minutes=5),\n    }\n</code></pre></p> <p>A class-based credential provider with state:</p> <pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport boto3\nimport botocore.credentials\n\nif TYPE_CHECKING:\n    from obstore.store import S3Credential\n\n\nclass Boto3CredentialProvider:\n    credentials: botocore.credentials.Credentials\n\n    def __init__(self, session: boto3.session.Session) -&gt; None:\n        credentials = session.get_credentials()\n        if credentials is None:\n            raise ValueError(\"Received None from session.get_credentials\")\n\n        self.credentials = credentials\n\n    def __call__(self) -&gt; S3Credential:\n        frozen_credentials = self.credentials.get_frozen_credentials()\n        return {\n            \"access_key_id\": frozen_credentials.access_key,\n            \"secret_access_key\": frozen_credentials.secret_key,\n            \"token\": frozen_credentials.token,\n            \"expires_at\": None,\n        }\n</code></pre> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import S3CredentialProvider\n</code></pre>"},{"location":"api/store/aws/#obstore.store.S3CredentialProvider.__call__","title":"__call__  <code>staticmethod</code>","text":"<pre><code>__call__() -&gt; S3Credential | Coroutine[Any, Any, S3Credential]\n</code></pre> <p>Return an <code>S3Credential</code>.</p>"},{"location":"api/store/azure/","title":"Microsoft Azure","text":""},{"location":"api/store/azure/#obstore.store.AzureStore","title":"obstore.store.AzureStore","text":"<p>Interface to a Microsoft Azure Blob Storage container.</p> <p>All constructors will check for environment variables. All environment variables starting with <code>AZURE_</code> will be evaluated. Names must match keys from <code>AzureConfig</code>. Only upper-case environment variables are accepted.</p> <p>Some examples of variables extracted from environment:</p> <ul> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code>: storage account name</li> <li><code>AZURE_STORAGE_ACCOUNT_KEY</code>: storage account master key</li> <li><code>AZURE_STORAGE_ACCESS_KEY</code>: alias for <code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li><code>AZURE_STORAGE_CLIENT_ID</code> -&gt; client id for service principal authorization</li> <li><code>AZURE_STORAGE_CLIENT_SECRET</code> -&gt; client secret for service principal authorization</li> <li><code>AZURE_STORAGE_TENANT_ID</code> -&gt; tenant id used in oauth flows</li> </ul>"},{"location":"api/store/azure/#obstore.store.AzureStore.client_options","title":"client_options  <code>property</code>","text":"<pre><code>client_options: ClientConfig | None\n</code></pre> <p>Get the store's client configuration.</p>"},{"location":"api/store/azure/#obstore.store.AzureStore.config","title":"config  <code>property</code>","text":"<pre><code>config: AzureConfig\n</code></pre> <p>Get the underlying Azure config parameters.</p>"},{"location":"api/store/azure/#obstore.store.AzureStore.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix: str | None\n</code></pre> <p>Get the prefix applied to all operations in this store, if any.</p>"},{"location":"api/store/azure/#obstore.store.AzureStore.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig | None\n</code></pre> <p>Get the store's retry configuration.</p>"},{"location":"api/store/azure/#obstore.store.AzureStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    container: str | None = None,\n    *,\n    prefix: str | None = None,\n    config: AzureConfig | AzureConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: AzureCredentialProvider | None = None,\n    **kwargs: Unpack[AzureConfigInput],\n) -&gt; None\n</code></pre> <p>Construct a new AzureStore.</p> <p>Parameters:</p> <ul> <li> <code>container</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>the name of the container.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | None</code>)           \u2013            <p>A prefix within the bucket to use for all operations.</p> </li> <li> <code>config</code>               (<code>AzureConfig | AzureConfigInput | None</code>)           \u2013            <p>Azure Configuration. Values in this config will override values inferred from the url. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>AzureCredentialProvider | None</code>)           \u2013            <p>A callback to provide custom Azure credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[AzureConfigInput]</code>)           \u2013            <p>Azure configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>AzureStore</p> </li> </ul>"},{"location":"api/store/azure/#obstore.store.AzureStore.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    *,\n    prefix: str | None = None,\n    config: AzureConfig | AzureConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: AzureCredentialProvider | None = None,\n    **kwargs: Unpack[AzureConfigInput],\n) -&gt; AzureStore\n</code></pre> <p>Construct a new AzureStore with values populated from a well-known storage URL.</p> <p>The supported url schemes are:</p> <ul> <li><code>abfs[s]://&lt;container&gt;/&lt;path&gt;</code> (according to fsspec)</li> <li><code>abfs[s]://&lt;file_system&gt;@&lt;account_name&gt;.dfs.core.windows.net/&lt;path&gt;</code></li> <li><code>abfs[s]://&lt;file_system&gt;@&lt;account_name&gt;.dfs.fabric.microsoft.com/&lt;path&gt;</code></li> <li><code>az://&lt;container&gt;/&lt;path&gt;</code> (according to fsspec)</li> <li><code>adl://&lt;container&gt;/&lt;path&gt;</code> (according to fsspec)</li> <li><code>azure://&lt;container&gt;/&lt;path&gt;</code> (custom)</li> <li><code>https://&lt;account&gt;.dfs.core.windows.net</code></li> <li><code>https://&lt;account&gt;.blob.core.windows.net</code></li> <li><code>https://&lt;account&gt;.blob.core.windows.net/&lt;container&gt;</code></li> <li><code>https://&lt;account&gt;.dfs.fabric.microsoft.com</code></li> <li><code>https://&lt;account&gt;.dfs.fabric.microsoft.com/&lt;container&gt;</code></li> <li><code>https://&lt;account&gt;.blob.fabric.microsoft.com</code></li> <li><code>https://&lt;account&gt;.blob.fabric.microsoft.com/&lt;container&gt;</code></li> </ul> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>well-known storage URL.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | None</code>)           \u2013            <p>A prefix within the bucket to use for all operations.</p> </li> <li> <code>config</code>               (<code>AzureConfig | AzureConfigInput | None</code>)           \u2013            <p>Azure Configuration. Values in this config will override values inferred from the url. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>AzureCredentialProvider | None</code>)           \u2013            <p>A callback to provide custom Azure credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[AzureConfigInput]</code>)           \u2013            <p>Azure configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AzureStore</code>           \u2013            <p>AzureStore</p> </li> </ul>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput","title":"obstore.store.AzureConfigInput","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters for AzureStore.</p> <p>There are duplicates of many parameters, and parameters can be either upper or lower case. Not all parameters are required.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureConfigInput\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.ACCESS_KEY","title":"ACCESS_KEY  <code>instance-attribute</code>","text":"<pre><code>ACCESS_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.ACCOUNT_KEY","title":"ACCOUNT_KEY  <code>instance-attribute</code>","text":"<pre><code>ACCOUNT_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.ACCOUNT_NAME","title":"ACCOUNT_NAME  <code>instance-attribute</code>","text":"<pre><code>ACCOUNT_NAME: str\n</code></pre> <p>The name of the azure storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AUTHORITY_ID","title":"AUTHORITY_ID  <code>instance-attribute</code>","text":"<pre><code>AUTHORITY_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_AUTHORITY_ID","title":"AZURE_AUTHORITY_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_AUTHORITY_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_CLIENT_ID","title":"AZURE_CLIENT_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_CLIENT_ID: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_CLIENT_SECRET","title":"AZURE_CLIENT_SECRET  <code>instance-attribute</code>","text":"<pre><code>AZURE_CLIENT_SECRET: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_CONTAINER_NAME","title":"AZURE_CONTAINER_NAME  <code>instance-attribute</code>","text":"<pre><code>AZURE_CONTAINER_NAME: str\n</code></pre> <p>Container name</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_DISABLE_TAGGING","title":"AZURE_DISABLE_TAGGING  <code>instance-attribute</code>","text":"<pre><code>AZURE_DISABLE_TAGGING: bool\n</code></pre> <p>Disables tagging objects</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_ENDPOINT","title":"AZURE_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AZURE_ENDPOINT: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_FEDERATED_TOKEN_FILE","title":"AZURE_FEDERATED_TOKEN_FILE  <code>instance-attribute</code>","text":"<pre><code>AZURE_FEDERATED_TOKEN_FILE: str\n</code></pre> <p>File containing token for Azure AD workload identity federation</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_IDENTITY_ENDPOINT","title":"AZURE_IDENTITY_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AZURE_IDENTITY_ENDPOINT: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_MSI_ENDPOINT","title":"AZURE_MSI_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AZURE_MSI_ENDPOINT: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_MSI_RESOURCE_ID","title":"AZURE_MSI_RESOURCE_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_MSI_RESOURCE_ID: str\n</code></pre> <p>Msi resource id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_OBJECT_ID","title":"AZURE_OBJECT_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_OBJECT_ID: str\n</code></pre> <p>Object id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_SKIP_SIGNATURE","title":"AZURE_SKIP_SIGNATURE  <code>instance-attribute</code>","text":"<pre><code>AZURE_SKIP_SIGNATURE: bool\n</code></pre> <p>Skip signing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_ACCESS_KEY","title":"AZURE_STORAGE_ACCESS_KEY  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_ACCESS_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_ACCOUNT_KEY","title":"AZURE_STORAGE_ACCOUNT_KEY  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_ACCOUNT_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_ACCOUNT_NAME","title":"AZURE_STORAGE_ACCOUNT_NAME  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_ACCOUNT_NAME: str\n</code></pre> <p>The name of the azure storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_AUTHORITY_ID","title":"AZURE_STORAGE_AUTHORITY_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_AUTHORITY_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_CLIENT_ID","title":"AZURE_STORAGE_CLIENT_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_CLIENT_ID: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_CLIENT_SECRET","title":"AZURE_STORAGE_CLIENT_SECRET  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_CLIENT_SECRET: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_ENDPOINT","title":"AZURE_STORAGE_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_ENDPOINT: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_MASTER_KEY","title":"AZURE_STORAGE_MASTER_KEY  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_MASTER_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_SAS_KEY","title":"AZURE_STORAGE_SAS_KEY  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_SAS_KEY: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_SAS_TOKEN","title":"AZURE_STORAGE_SAS_TOKEN  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_SAS_TOKEN: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_TENANT_ID","title":"AZURE_STORAGE_TENANT_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_TENANT_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_TOKEN","title":"AZURE_STORAGE_TOKEN  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_TOKEN: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_STORAGE_USE_EMULATOR","title":"AZURE_STORAGE_USE_EMULATOR  <code>instance-attribute</code>","text":"<pre><code>AZURE_STORAGE_USE_EMULATOR: bool\n</code></pre> <p>Use object store with azurite storage emulator</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_TENANT_ID","title":"AZURE_TENANT_ID  <code>instance-attribute</code>","text":"<pre><code>AZURE_TENANT_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_USE_AZURE_CLI","title":"AZURE_USE_AZURE_CLI  <code>instance-attribute</code>","text":"<pre><code>AZURE_USE_AZURE_CLI: bool\n</code></pre> <p>Use azure cli for acquiring access token.</p> <p>Defaults to <code>True</code>.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.AZURE_USE_FABRIC_ENDPOINT","title":"AZURE_USE_FABRIC_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>AZURE_USE_FABRIC_ENDPOINT: bool\n</code></pre> <p>Use object store with url scheme account.dfs.fabric.microsoft.com</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.BEARER_TOKEN","title":"BEARER_TOKEN  <code>instance-attribute</code>","text":"<pre><code>BEARER_TOKEN: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.CLIENT_ID","title":"CLIENT_ID  <code>instance-attribute</code>","text":"<pre><code>CLIENT_ID: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.CLIENT_SECRET","title":"CLIENT_SECRET  <code>instance-attribute</code>","text":"<pre><code>CLIENT_SECRET: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.CONTAINER_NAME","title":"CONTAINER_NAME  <code>instance-attribute</code>","text":"<pre><code>CONTAINER_NAME: str\n</code></pre> <p>Container name</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.DISABLE_TAGGING","title":"DISABLE_TAGGING  <code>instance-attribute</code>","text":"<pre><code>DISABLE_TAGGING: bool\n</code></pre> <p>Disables tagging objects</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.ENDPOINT","title":"ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>ENDPOINT: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.FEDERATED_TOKEN_FILE","title":"FEDERATED_TOKEN_FILE  <code>instance-attribute</code>","text":"<pre><code>FEDERATED_TOKEN_FILE: str\n</code></pre> <p>File containing token for Azure AD workload identity federation</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.IDENTITY_ENDPOINT","title":"IDENTITY_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>IDENTITY_ENDPOINT: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.MASTER_KEY","title":"MASTER_KEY  <code>instance-attribute</code>","text":"<pre><code>MASTER_KEY: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.MSI_ENDPOINT","title":"MSI_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>MSI_ENDPOINT: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.MSI_RESOURCE_ID","title":"MSI_RESOURCE_ID  <code>instance-attribute</code>","text":"<pre><code>MSI_RESOURCE_ID: str\n</code></pre> <p>Msi resource id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.OBJECT_ID","title":"OBJECT_ID  <code>instance-attribute</code>","text":"<pre><code>OBJECT_ID: str\n</code></pre> <p>Object id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.SAS_KEY","title":"SAS_KEY  <code>instance-attribute</code>","text":"<pre><code>SAS_KEY: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.SAS_TOKEN","title":"SAS_TOKEN  <code>instance-attribute</code>","text":"<pre><code>SAS_TOKEN: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.SKIP_SIGNATURE","title":"SKIP_SIGNATURE  <code>instance-attribute</code>","text":"<pre><code>SKIP_SIGNATURE: bool\n</code></pre> <p>Skip signing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.TENANT_ID","title":"TENANT_ID  <code>instance-attribute</code>","text":"<pre><code>TENANT_ID: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.TOKEN","title":"TOKEN  <code>instance-attribute</code>","text":"<pre><code>TOKEN: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.USE_AZURE_CLI","title":"USE_AZURE_CLI  <code>instance-attribute</code>","text":"<pre><code>USE_AZURE_CLI: bool\n</code></pre> <p>Use azure cli for acquiring access token.</p> <p>Defaults to <code>True</code>.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.USE_EMULATOR","title":"USE_EMULATOR  <code>instance-attribute</code>","text":"<pre><code>USE_EMULATOR: bool\n</code></pre> <p>Use object store with azurite storage emulator</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.USE_FABRIC_ENDPOINT","title":"USE_FABRIC_ENDPOINT  <code>instance-attribute</code>","text":"<pre><code>USE_FABRIC_ENDPOINT: bool\n</code></pre> <p>Use object store with url scheme account.dfs.fabric.microsoft.com</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.access_key","title":"access_key  <code>instance-attribute</code>","text":"<pre><code>access_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.account_key","title":"account_key  <code>instance-attribute</code>","text":"<pre><code>account_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.account_name","title":"account_name  <code>instance-attribute</code>","text":"<pre><code>account_name: str\n</code></pre> <p>The name of the azure storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.authority_id","title":"authority_id  <code>instance-attribute</code>","text":"<pre><code>authority_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_authority_id","title":"azure_authority_id  <code>instance-attribute</code>","text":"<pre><code>azure_authority_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_client_id","title":"azure_client_id  <code>instance-attribute</code>","text":"<pre><code>azure_client_id: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_client_secret","title":"azure_client_secret  <code>instance-attribute</code>","text":"<pre><code>azure_client_secret: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_container_name","title":"azure_container_name  <code>instance-attribute</code>","text":"<pre><code>azure_container_name: str\n</code></pre> <p>Container name</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_disable_tagging","title":"azure_disable_tagging  <code>instance-attribute</code>","text":"<pre><code>azure_disable_tagging: bool\n</code></pre> <p>Disables tagging objects</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_endpoint","title":"azure_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_endpoint: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_federated_token_file","title":"azure_federated_token_file  <code>instance-attribute</code>","text":"<pre><code>azure_federated_token_file: str\n</code></pre> <p>File containing token for Azure AD workload identity federation</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_identity_endpoint","title":"azure_identity_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_identity_endpoint: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_msi_endpoint","title":"azure_msi_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_msi_endpoint: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_msi_resource_id","title":"azure_msi_resource_id  <code>instance-attribute</code>","text":"<pre><code>azure_msi_resource_id: str\n</code></pre> <p>Msi resource id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_object_id","title":"azure_object_id  <code>instance-attribute</code>","text":"<pre><code>azure_object_id: str\n</code></pre> <p>Object id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_skip_signature","title":"azure_skip_signature  <code>instance-attribute</code>","text":"<pre><code>azure_skip_signature: bool\n</code></pre> <p>Skip signing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_access_key","title":"azure_storage_access_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_access_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_account_key","title":"azure_storage_account_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_account_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_account_name","title":"azure_storage_account_name  <code>instance-attribute</code>","text":"<pre><code>azure_storage_account_name: str\n</code></pre> <p>The name of the azure storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_authority_id","title":"azure_storage_authority_id  <code>instance-attribute</code>","text":"<pre><code>azure_storage_authority_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_client_id","title":"azure_storage_client_id  <code>instance-attribute</code>","text":"<pre><code>azure_storage_client_id: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_client_secret","title":"azure_storage_client_secret  <code>instance-attribute</code>","text":"<pre><code>azure_storage_client_secret: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_endpoint","title":"azure_storage_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_storage_endpoint: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_master_key","title":"azure_storage_master_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_master_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_sas_key","title":"azure_storage_sas_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_sas_key: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_sas_token","title":"azure_storage_sas_token  <code>instance-attribute</code>","text":"<pre><code>azure_storage_sas_token: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_tenant_id","title":"azure_storage_tenant_id  <code>instance-attribute</code>","text":"<pre><code>azure_storage_tenant_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_token","title":"azure_storage_token  <code>instance-attribute</code>","text":"<pre><code>azure_storage_token: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_storage_use_emulator","title":"azure_storage_use_emulator  <code>instance-attribute</code>","text":"<pre><code>azure_storage_use_emulator: bool\n</code></pre> <p>Use object store with azurite storage emulator</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_tenant_id","title":"azure_tenant_id  <code>instance-attribute</code>","text":"<pre><code>azure_tenant_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_use_azure_cli","title":"azure_use_azure_cli  <code>instance-attribute</code>","text":"<pre><code>azure_use_azure_cli: bool\n</code></pre> <p>Use azure cli for acquiring access token.</p> <p>Defaults to <code>True</code>.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.azure_use_fabric_endpoint","title":"azure_use_fabric_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_use_fabric_endpoint: bool\n</code></pre> <p>Use object store with url scheme account.dfs.fabric.microsoft.com</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.bearer_token","title":"bearer_token  <code>instance-attribute</code>","text":"<pre><code>bearer_token: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.client_id","title":"client_id  <code>instance-attribute</code>","text":"<pre><code>client_id: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.client_secret","title":"client_secret  <code>instance-attribute</code>","text":"<pre><code>client_secret: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.container_name","title":"container_name  <code>instance-attribute</code>","text":"<pre><code>container_name: str\n</code></pre> <p>Container name</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.disable_tagging","title":"disable_tagging  <code>instance-attribute</code>","text":"<pre><code>disable_tagging: bool\n</code></pre> <p>Disables tagging objects</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.endpoint","title":"endpoint  <code>instance-attribute</code>","text":"<pre><code>endpoint: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.federated_token_file","title":"federated_token_file  <code>instance-attribute</code>","text":"<pre><code>federated_token_file: str\n</code></pre> <p>File containing token for Azure AD workload identity federation</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.identity_endpoint","title":"identity_endpoint  <code>instance-attribute</code>","text":"<pre><code>identity_endpoint: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.master_key","title":"master_key  <code>instance-attribute</code>","text":"<pre><code>master_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.msi_endpoint","title":"msi_endpoint  <code>instance-attribute</code>","text":"<pre><code>msi_endpoint: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.msi_resource_id","title":"msi_resource_id  <code>instance-attribute</code>","text":"<pre><code>msi_resource_id: str\n</code></pre> <p>Msi resource id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.object_id","title":"object_id  <code>instance-attribute</code>","text":"<pre><code>object_id: str\n</code></pre> <p>Object id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.sas_key","title":"sas_key  <code>instance-attribute</code>","text":"<pre><code>sas_key: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.sas_token","title":"sas_token  <code>instance-attribute</code>","text":"<pre><code>sas_token: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.skip_signature","title":"skip_signature  <code>instance-attribute</code>","text":"<pre><code>skip_signature: bool\n</code></pre> <p>Skip signing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.tenant_id","title":"tenant_id  <code>instance-attribute</code>","text":"<pre><code>tenant_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.use_azure_cli","title":"use_azure_cli  <code>instance-attribute</code>","text":"<pre><code>use_azure_cli: bool\n</code></pre> <p>Use azure cli for acquiring access token.</p> <p>Defaults to <code>True</code>.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.use_emulator","title":"use_emulator  <code>instance-attribute</code>","text":"<pre><code>use_emulator: bool\n</code></pre> <p>Use object store with azurite storage emulator</p>"},{"location":"api/store/azure/#obstore.store.AzureConfigInput.use_fabric_endpoint","title":"use_fabric_endpoint  <code>instance-attribute</code>","text":"<pre><code>use_fabric_endpoint: bool\n</code></pre> <p>Use object store with url scheme account.dfs.fabric.microsoft.com</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig","title":"obstore.store.AzureConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters returned from AzureStore.config.</p> <p>Note that this is a strict subset of the keys allowed for input into the store, see AzureConfigInput.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureConfig\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_container_name","title":"azure_container_name  <code>instance-attribute</code>","text":"<pre><code>azure_container_name: str\n</code></pre> <p>Container name</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_disable_tagging","title":"azure_disable_tagging  <code>instance-attribute</code>","text":"<pre><code>azure_disable_tagging: bool\n</code></pre> <p>Disables tagging objects</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_federated_token_file","title":"azure_federated_token_file  <code>instance-attribute</code>","text":"<pre><code>azure_federated_token_file: str\n</code></pre> <p>File containing token for Azure AD workload identity federation</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_msi_endpoint","title":"azure_msi_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_msi_endpoint: str\n</code></pre> <p>Endpoint to request a imds managed identity token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_msi_resource_id","title":"azure_msi_resource_id  <code>instance-attribute</code>","text":"<pre><code>azure_msi_resource_id: str\n</code></pre> <p>Msi resource id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_object_id","title":"azure_object_id  <code>instance-attribute</code>","text":"<pre><code>azure_object_id: str\n</code></pre> <p>Object id for use with managed identity authentication</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_skip_signature","title":"azure_skip_signature  <code>instance-attribute</code>","text":"<pre><code>azure_skip_signature: bool\n</code></pre> <p>Skip signing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_account_key","title":"azure_storage_account_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_account_key: str\n</code></pre> <p>Master key for accessing storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_account_name","title":"azure_storage_account_name  <code>instance-attribute</code>","text":"<pre><code>azure_storage_account_name: str\n</code></pre> <p>The name of the azure storage account</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_client_id","title":"azure_storage_client_id  <code>instance-attribute</code>","text":"<pre><code>azure_storage_client_id: str\n</code></pre> <p>Service principal client id for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_client_secret","title":"azure_storage_client_secret  <code>instance-attribute</code>","text":"<pre><code>azure_storage_client_secret: str\n</code></pre> <p>Service principal client secret for authorizing requests</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_endpoint","title":"azure_storage_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_storage_endpoint: str\n</code></pre> <p>Override the endpoint used to communicate with blob storage</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_sas_key","title":"azure_storage_sas_key  <code>instance-attribute</code>","text":"<pre><code>azure_storage_sas_key: str\n</code></pre> <p>Shared access signature.</p> <p>The signature is expected to be percent-encoded, <code>much</code>like they are provided in the azure storage explorer or azure portal.</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_tenant_id","title":"azure_storage_tenant_id  <code>instance-attribute</code>","text":"<pre><code>azure_storage_tenant_id: str\n</code></pre> <p>Tenant id used in oauth flows</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_token","title":"azure_storage_token  <code>instance-attribute</code>","text":"<pre><code>azure_storage_token: str\n</code></pre> <p>Bearer token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_storage_use_emulator","title":"azure_storage_use_emulator  <code>instance-attribute</code>","text":"<pre><code>azure_storage_use_emulator: bool\n</code></pre> <p>Use object store with azurite storage emulator</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_use_azure_cli","title":"azure_use_azure_cli  <code>instance-attribute</code>","text":"<pre><code>azure_use_azure_cli: bool\n</code></pre> <p>Use azure cli for acquiring access token</p>"},{"location":"api/store/azure/#obstore.store.AzureConfig.azure_use_fabric_endpoint","title":"azure_use_fabric_endpoint  <code>instance-attribute</code>","text":"<pre><code>azure_use_fabric_endpoint: bool\n</code></pre> <p>Use object store with url scheme account.dfs.fabric.microsoft.com</p>"},{"location":"api/store/azure/#obstore.store.AzureAccessKey","title":"obstore.store.AzureAccessKey","text":"<p>               Bases: <code>TypedDict</code></p> <p>A shared Azure Storage Account Key.</p> <p>learn.microsoft.com/en-us/rest/api/storageservices/authorize-with-shared-key</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureAccessKey\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureAccessKey.access_key","title":"access_key  <code>instance-attribute</code>","text":"<pre><code>access_key: str\n</code></pre> <p>Access key value.</p>"},{"location":"api/store/azure/#obstore.store.AzureAccessKey.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: datetime | None\n</code></pre> <p>Expiry datetime of credential. The datetime should have time zone set.</p> <p>If None, the credential will never expire.</p>"},{"location":"api/store/azure/#obstore.store.AzureSASToken","title":"obstore.store.AzureSASToken","text":"<p>               Bases: <code>TypedDict</code></p> <p>A shared access signature.</p> <p>learn.microsoft.com/en-us/rest/api/storageservices/delegate-access-with-shared-access-signature</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureSASToken\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureSASToken.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: datetime | None\n</code></pre> <p>Expiry datetime of credential. The datetime should have time zone set.</p> <p>If None, the credential will never expire.</p>"},{"location":"api/store/azure/#obstore.store.AzureSASToken.sas_token","title":"sas_token  <code>instance-attribute</code>","text":"<pre><code>sas_token: str | list[tuple[str, str]]\n</code></pre> <p>SAS token.</p>"},{"location":"api/store/azure/#obstore.store.AzureBearerToken","title":"obstore.store.AzureBearerToken","text":"<p>               Bases: <code>TypedDict</code></p> <p>An authorization token.</p> <p>learn.microsoft.com/en-us/rest/api/storageservices/authorize-with-azure-active-directory</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureBearerToken\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureBearerToken.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: datetime | None\n</code></pre> <p>Expiry datetime of credential. The datetime should have time zone set.</p> <p>If None, the credential will never expire.</p>"},{"location":"api/store/azure/#obstore.store.AzureBearerToken.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>Bearer token.</p>"},{"location":"api/store/azure/#obstore.store.AzureCredential","title":"obstore.store.AzureCredential  <code>module-attribute</code>","text":"<pre><code>AzureCredential: TypeAlias = AzureAccessKey | AzureSASToken | AzureBearerToken\n</code></pre> <p>A type alias for supported azure credentials to be returned from <code>AzureCredentialProvider</code>.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureCredential\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureCredentialProvider","title":"obstore.store.AzureCredentialProvider","text":"<p>               Bases: <code>Protocol</code></p> <p>A type hint for a synchronous or asynchronous callback to provide custom Azure credentials.</p> <p>This should be passed into the <code>credential_provider</code> parameter of <code>AzureStore</code>.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import AzureCredentialProvider\n</code></pre>"},{"location":"api/store/azure/#obstore.store.AzureCredentialProvider.__call__","title":"__call__  <code>staticmethod</code>","text":"<pre><code>__call__() -&gt; AzureCredential | Coroutine[Any, Any, AzureCredential]\n</code></pre> <p>Return an <code>AzureCredential</code>.</p>"},{"location":"api/store/config/","title":"Configuration","text":""},{"location":"api/store/config/#obstore.store.ClientConfig","title":"obstore.store.ClientConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>HTTP client configuration.</p> <p>For timeout values (<code>connect_timeout</code>, <code>http2_keep_alive_timeout</code>, <code>pool_idle_timeout</code>, and <code>timeout</code>), values can either be Python <code>timedelta</code> objects, or they can be \"human-readable duration strings\".</p> <p>The human-readable duration string is a concatenation of time spans. Where each time span is an integer number and a suffix. Supported suffixes:</p> <ul> <li><code>nsec</code>, <code>ns</code> -- nanoseconds</li> <li><code>usec</code>, <code>us</code> -- microseconds</li> <li><code>msec</code>, <code>ms</code> -- milliseconds</li> <li><code>seconds</code>, <code>second</code>, <code>sec</code>, <code>s</code></li> <li><code>minutes</code>, <code>minute</code>, <code>min</code>, <code>m</code></li> <li><code>hours</code>, <code>hour</code>, <code>hr</code>, <code>h</code></li> <li><code>days</code>, <code>day</code>, <code>d</code></li> <li><code>weeks</code>, <code>week</code>, <code>w</code></li> <li><code>months</code>, <code>month</code>, <code>M</code> -- defined as 30.44 days</li> <li><code>years</code>, <code>year</code>, <code>y</code> -- defined as 365.25 days</li> </ul> <p>For example:</p> <ul> <li><code>\"2h 37min\"</code></li> <li><code>\"32ms\"</code></li> </ul> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import ClientConfig\n</code></pre>"},{"location":"api/store/config/#obstore.store.ClientConfig.allow_http","title":"allow_http  <code>instance-attribute</code>","text":"<pre><code>allow_http: bool\n</code></pre> <p>Allow non-TLS, i.e. non-HTTPS connections.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.allow_invalid_certificates","title":"allow_invalid_certificates  <code>instance-attribute</code>","text":"<pre><code>allow_invalid_certificates: bool\n</code></pre> <p>Skip certificate validation on https connections.</p> <p>Warning</p> <p>You should think very carefully before using this method. If invalid certificates are trusted, any certificate for any site will be trusted for use. This includes expired certificates. This introduces significant vulnerabilities, and should only be used as a last resort or for testing</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.connect_timeout","title":"connect_timeout  <code>instance-attribute</code>","text":"<pre><code>connect_timeout: str | timedelta\n</code></pre> <p>Timeout for only the connect phase of a Client</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.default_content_type","title":"default_content_type  <code>instance-attribute</code>","text":"<pre><code>default_content_type: str\n</code></pre> <p>default <code>CONTENT_TYPE</code> for uploads</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.http1_only","title":"http1_only  <code>instance-attribute</code>","text":"<pre><code>http1_only: bool\n</code></pre> <p>Only use http1 connections.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.http2_keep_alive_interval","title":"http2_keep_alive_interval  <code>instance-attribute</code>","text":"<pre><code>http2_keep_alive_interval: str\n</code></pre> <p>Interval for HTTP2 Ping frames should be sent to keep a connection alive.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.http2_keep_alive_timeout","title":"http2_keep_alive_timeout  <code>instance-attribute</code>","text":"<pre><code>http2_keep_alive_timeout: str | timedelta\n</code></pre> <p>Timeout for receiving an acknowledgement of the keep-alive ping.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.http2_keep_alive_while_idle","title":"http2_keep_alive_while_idle  <code>instance-attribute</code>","text":"<pre><code>http2_keep_alive_while_idle: str\n</code></pre> <p>Enable HTTP2 keep alive pings for idle connections</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.http2_only","title":"http2_only  <code>instance-attribute</code>","text":"<pre><code>http2_only: bool\n</code></pre> <p>Only use http2 connections</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.pool_idle_timeout","title":"pool_idle_timeout  <code>instance-attribute</code>","text":"<pre><code>pool_idle_timeout: str | timedelta\n</code></pre> <p>The pool max idle timeout.</p> <p>This is the length of time an idle connection will be kept alive.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.pool_max_idle_per_host","title":"pool_max_idle_per_host  <code>instance-attribute</code>","text":"<pre><code>pool_max_idle_per_host: str\n</code></pre> <p>Maximum number of idle connections per host.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.proxy_url","title":"proxy_url  <code>instance-attribute</code>","text":"<pre><code>proxy_url: str\n</code></pre> <p>HTTP proxy to use for requests.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: str | timedelta\n</code></pre> <p>Request timeout.</p> <p>The timeout is applied from when the request starts connecting until the response body has finished.</p>"},{"location":"api/store/config/#obstore.store.ClientConfig.user_agent","title":"user_agent  <code>instance-attribute</code>","text":"<pre><code>user_agent: str\n</code></pre> <p>User-Agent header to be used by this client.</p>"},{"location":"api/store/config/#obstore.store.BackoffConfig","title":"obstore.store.BackoffConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Exponential backoff with jitter.</p> <p>See aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import BackoffConfig\n</code></pre>"},{"location":"api/store/config/#obstore.store.BackoffConfig.base","title":"base  <code>instance-attribute</code>","text":"<pre><code>base: int | float\n</code></pre> <p>The base of the exponential to use.</p> <p>Defaults to <code>2</code>.</p>"},{"location":"api/store/config/#obstore.store.BackoffConfig.init_backoff","title":"init_backoff  <code>instance-attribute</code>","text":"<pre><code>init_backoff: timedelta\n</code></pre> <p>The initial backoff duration.</p> <p>Defaults to 100 milliseconds.</p>"},{"location":"api/store/config/#obstore.store.BackoffConfig.max_backoff","title":"max_backoff  <code>instance-attribute</code>","text":"<pre><code>max_backoff: timedelta\n</code></pre> <p>The maximum backoff duration.</p> <p>Defaults to 15 seconds.</p>"},{"location":"api/store/config/#obstore.store.RetryConfig","title":"obstore.store.RetryConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>The configuration for how to respond to request errors.</p> <p>The following categories of error will be retried:</p> <ul> <li>5xx server errors</li> <li>Connection errors</li> <li>Dropped connections</li> <li>Timeouts for safe / read-only requests</li> </ul> <p>Requests will be retried up to some limit, using exponential backoff with jitter. See <code>BackoffConfig</code> for more information</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import RetryConfig\n</code></pre>"},{"location":"api/store/config/#obstore.store.RetryConfig.backoff","title":"backoff  <code>instance-attribute</code>","text":"<pre><code>backoff: BackoffConfig\n</code></pre> <p>The backoff configuration.</p> <p>Defaults to the values listed above if not provided.</p>"},{"location":"api/store/config/#obstore.store.RetryConfig.max_retries","title":"max_retries  <code>instance-attribute</code>","text":"<pre><code>max_retries: int\n</code></pre> <p>The maximum number of times to retry a request</p> <p>Set to 0 to disable retries.</p> <p>Defaults to 10.</p>"},{"location":"api/store/config/#obstore.store.RetryConfig.retry_timeout","title":"retry_timeout  <code>instance-attribute</code>","text":"<pre><code>retry_timeout: timedelta\n</code></pre> <p>The maximum length of time from the initial request after which no further retries will be attempted</p> <p>This not only bounds the length of time before a server error will be surfaced to the application, but also bounds the length of time a request's credentials must remain valid.</p> <p>As requests are retried without renewing credentials or regenerating request payloads, this number should be kept below 5 minutes to avoid errors due to expired credentials and/or request payloads.</p> <p>Defaults to 3 minutes.</p>"},{"location":"api/store/gcs/","title":"Google Cloud Storage","text":""},{"location":"api/store/gcs/#obstore.store.GCSStore","title":"obstore.store.GCSStore","text":"<p>Interface to Google Cloud Storage.</p> <p>All constructors will check for environment variables. All environment variables starting with <code>GOOGLE_</code> will be evaluated. Names must match keys from <code>GCSConfig</code>. Only upper-case environment variables are accepted.</p> <p>Some examples of variables extracted from environment:</p> <ul> <li><code>GOOGLE_SERVICE_ACCOUNT</code>: location of service account file</li> <li><code>GOOGLE_SERVICE_ACCOUNT_PATH</code>: (alias) location of service account file</li> <li><code>SERVICE_ACCOUNT</code>: (alias) location of service account file</li> <li><code>GOOGLE_SERVICE_ACCOUNT_KEY</code>: JSON serialized service account key</li> <li><code>GOOGLE_BUCKET</code>: bucket name</li> <li><code>GOOGLE_BUCKET_NAME</code>: (alias) bucket name</li> </ul> <p>If no credentials are explicitly provided, they will be sourced from the environment as documented here.</p>"},{"location":"api/store/gcs/#obstore.store.GCSStore.client_options","title":"client_options  <code>property</code>","text":"<pre><code>client_options: ClientConfig | None\n</code></pre> <p>Get the store's client configuration.</p>"},{"location":"api/store/gcs/#obstore.store.GCSStore.config","title":"config  <code>property</code>","text":"<pre><code>config: GCSConfig\n</code></pre> <p>Get the underlying GCS config parameters.</p>"},{"location":"api/store/gcs/#obstore.store.GCSStore.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix: str | None\n</code></pre> <p>Get the prefix applied to all operations in this store, if any.</p>"},{"location":"api/store/gcs/#obstore.store.GCSStore.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig | None\n</code></pre> <p>Get the store's retry configuration.</p>"},{"location":"api/store/gcs/#obstore.store.GCSStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    bucket: str | None = None,\n    *,\n    prefix: str | None = None,\n    config: GCSConfig | GCSConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: GCSCredentialProvider | None = None,\n    **kwargs: Unpack[GCSConfigInput],\n) -&gt; None\n</code></pre> <p>Construct a new GCSStore.</p> <p>Parameters:</p> <ul> <li> <code>bucket</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The GCS bucket to use.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | None</code>)           \u2013            <p>A prefix within the bucket to use for all operations.</p> </li> <li> <code>config</code>               (<code>GCSConfig | GCSConfigInput | None</code>)           \u2013            <p>GCS Configuration. Values in this config will override values inferred from the environment. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>GCSCredentialProvider | None</code>)           \u2013            <p>A callback to provide custom Google credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[GCSConfigInput]</code>)           \u2013            <p>GCS configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>GCSStore</p> </li> </ul>"},{"location":"api/store/gcs/#obstore.store.GCSStore.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    *,\n    prefix: str | None = None,\n    config: GCSConfig | GCSConfigInput | None = None,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n    credential_provider: GCSCredentialProvider | None = None,\n    **kwargs: Unpack[GCSConfigInput],\n) -&gt; GCSStore\n</code></pre> <p>Construct a new GCSStore with values populated from a well-known storage URL.</p> <p>The supported url schemes are:</p> <ul> <li><code>gs://&lt;bucket&gt;/&lt;path&gt;</code></li> </ul> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>well-known storage URL.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | None</code>)           \u2013            <p>A prefix within the bucket to use for all operations.</p> </li> <li> <code>config</code>               (<code>GCSConfig | GCSConfigInput | None</code>)           \u2013            <p>GCS Configuration. Values in this config will override values inferred from the url. Defaults to None.</p> </li> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> <li> <code>credential_provider</code>               (<code>GCSCredentialProvider | None</code>)           \u2013            <p>A callback to provide custom Google credentials.</p> </li> <li> <code>kwargs</code>               (<code>Unpack[GCSConfigInput]</code>)           \u2013            <p>GCS configuration values. Supports the same values as <code>config</code>, but as named keyword args.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GCSStore</code>           \u2013            <p>GCSStore</p> </li> </ul>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput","title":"obstore.store.GCSConfigInput","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters for GCSStore.</p> <p>There are duplicates of many parameters, and parameters can be either upper or lower case. Not all parameters are required.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import GCSConfigInput\n</code></pre>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.BUCKET","title":"BUCKET  <code>instance-attribute</code>","text":"<pre><code>BUCKET: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.BUCKET_NAME","title":"BUCKET_NAME  <code>instance-attribute</code>","text":"<pre><code>BUCKET_NAME: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_APPLICATION_CREDENTIALS","title":"GOOGLE_APPLICATION_CREDENTIALS  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_APPLICATION_CREDENTIALS: str\n</code></pre> <p>Application credentials path.</p> <p>See cloud.google.com/docs/authentication/provide-credentials-adc.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_BUCKET","title":"GOOGLE_BUCKET  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_BUCKET: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_BUCKET_NAME","title":"GOOGLE_BUCKET_NAME  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_BUCKET_NAME: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_SERVICE_ACCOUNT","title":"GOOGLE_SERVICE_ACCOUNT  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_SERVICE_ACCOUNT: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_SERVICE_ACCOUNT_KEY","title":"GOOGLE_SERVICE_ACCOUNT_KEY  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_SERVICE_ACCOUNT_KEY: str\n</code></pre> <p>The serialized service account key</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.GOOGLE_SERVICE_ACCOUNT_PATH","title":"GOOGLE_SERVICE_ACCOUNT_PATH  <code>instance-attribute</code>","text":"<pre><code>GOOGLE_SERVICE_ACCOUNT_PATH: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.SERVICE_ACCOUNT","title":"SERVICE_ACCOUNT  <code>instance-attribute</code>","text":"<pre><code>SERVICE_ACCOUNT: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.SERVICE_ACCOUNT_KEY","title":"SERVICE_ACCOUNT_KEY  <code>instance-attribute</code>","text":"<pre><code>SERVICE_ACCOUNT_KEY: str\n</code></pre> <p>The serialized service account key</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.SERVICE_ACCOUNT_PATH","title":"SERVICE_ACCOUNT_PATH  <code>instance-attribute</code>","text":"<pre><code>SERVICE_ACCOUNT_PATH: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.bucket_name","title":"bucket_name  <code>instance-attribute</code>","text":"<pre><code>bucket_name: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_application_credentials","title":"google_application_credentials  <code>instance-attribute</code>","text":"<pre><code>google_application_credentials: str\n</code></pre> <p>Application credentials path.</p> <p>See cloud.google.com/docs/authentication/provide-credentials-adc.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_bucket","title":"google_bucket  <code>instance-attribute</code>","text":"<pre><code>google_bucket: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_bucket_name","title":"google_bucket_name  <code>instance-attribute</code>","text":"<pre><code>google_bucket_name: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_service_account","title":"google_service_account  <code>instance-attribute</code>","text":"<pre><code>google_service_account: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_service_account_key","title":"google_service_account_key  <code>instance-attribute</code>","text":"<pre><code>google_service_account_key: str\n</code></pre> <p>The serialized service account key</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.google_service_account_path","title":"google_service_account_path  <code>instance-attribute</code>","text":"<pre><code>google_service_account_path: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.service_account","title":"service_account  <code>instance-attribute</code>","text":"<pre><code>service_account: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.service_account_key","title":"service_account_key  <code>instance-attribute</code>","text":"<pre><code>service_account_key: str\n</code></pre> <p>The serialized service account key</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfigInput.service_account_path","title":"service_account_path  <code>instance-attribute</code>","text":"<pre><code>service_account_path: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfig","title":"obstore.store.GCSConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration parameters returned from GCSStore.config.</p> <p>Note that this is a strict subset of the keys allowed for input into the store, see GCSConfigInput.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import GCSConfig\n</code></pre>"},{"location":"api/store/gcs/#obstore.store.GCSConfig.google_application_credentials","title":"google_application_credentials  <code>instance-attribute</code>","text":"<pre><code>google_application_credentials: str\n</code></pre> <p>Application credentials path.</p> <p>See cloud.google.com/docs/authentication/provide-credentials-adc.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfig.google_bucket","title":"google_bucket  <code>instance-attribute</code>","text":"<pre><code>google_bucket: str\n</code></pre> <p>Bucket name.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfig.google_service_account","title":"google_service_account  <code>instance-attribute</code>","text":"<pre><code>google_service_account: str\n</code></pre> <p>Path to the service account file.</p>"},{"location":"api/store/gcs/#obstore.store.GCSConfig.google_service_account_key","title":"google_service_account_key  <code>instance-attribute</code>","text":"<pre><code>google_service_account_key: str\n</code></pre> <p>The serialized service account key</p>"},{"location":"api/store/gcs/#obstore.store.GCSCredential","title":"obstore.store.GCSCredential","text":"<p>               Bases: <code>TypedDict</code></p> <p>A Google Cloud Storage Credential.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import GCSCredential\n</code></pre>"},{"location":"api/store/gcs/#obstore.store.GCSCredential.expires_at","title":"expires_at  <code>instance-attribute</code>","text":"<pre><code>expires_at: datetime | None\n</code></pre> <p>Expiry datetime of credential. The datetime should have time zone set.</p> <p>If None, the credential will never expire.</p>"},{"location":"api/store/gcs/#obstore.store.GCSCredential.token","title":"token  <code>instance-attribute</code>","text":"<pre><code>token: str\n</code></pre> <p>An HTTP bearer token.</p>"},{"location":"api/store/gcs/#obstore.store.GCSCredentialProvider","title":"obstore.store.GCSCredentialProvider","text":"<p>               Bases: <code>Protocol</code></p> <p>A type hint for a synchronous or asynchronous callback to provide custom Google Cloud Storage credentials.</p> <p>This should be passed into the <code>credential_provider</code> parameter of <code>GCSStore</code>.</p> <p>Not importable at runtime</p> <p>To use this type hint in your code, import it within a <code>TYPE_CHECKING</code> block:</p> <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from obstore.store import GCSCredentialProvider\n</code></pre>"},{"location":"api/store/gcs/#obstore.store.GCSCredentialProvider.__call__","title":"__call__  <code>staticmethod</code>","text":"<pre><code>__call__() -&gt; GCSCredential | Coroutine[Any, Any, GCSCredential]\n</code></pre> <p>Return a <code>GCSCredential</code>.</p>"},{"location":"api/store/http/","title":"HTTP","text":""},{"location":"api/store/http/#obstore.store.HTTPStore","title":"obstore.store.HTTPStore","text":"<p>Configure a connection to a generic HTTP server.</p> <p>Example</p> <p>Accessing the number of stars for a repo:</p> <pre><code>import json\n\nimport obstore as obs\nfrom obstore.store import HTTPStore\n\nstore = HTTPStore.from_url(\"https://api.github.com\")\nresp = obs.get(store, \"repos/developmentseed/obstore\")\ndata = json.loads(resp.bytes())\nprint(data[\"stargazers_count\"])\n</code></pre>"},{"location":"api/store/http/#obstore.store.HTTPStore.client_options","title":"client_options  <code>property</code>","text":"<pre><code>client_options: ClientConfig | None\n</code></pre> <p>Get the store's client configuration.</p>"},{"location":"api/store/http/#obstore.store.HTTPStore.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig | None\n</code></pre> <p>Get the store's retry configuration.</p>"},{"location":"api/store/http/#obstore.store.HTTPStore.url","title":"url  <code>property</code>","text":"<pre><code>url: str\n</code></pre> <p>Get the base url of this store.</p>"},{"location":"api/store/http/#obstore.store.HTTPStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    url: str,\n    *,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n) -&gt; None\n</code></pre> <p>Construct a new HTTPStore from a URL.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The base URL to use for the store.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>client_options</code>               (<code>ClientConfig | None</code>)           \u2013            <p>HTTP Client options. Defaults to None.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | None</code>)           \u2013            <p>Retry configuration. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>HTTPStore</p> </li> </ul>"},{"location":"api/store/http/#obstore.store.HTTPStore.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    *,\n    client_options: ClientConfig | None = None,\n    retry_config: RetryConfig | None = None,\n) -&gt; HTTPStore\n</code></pre> <p>Construct a new HTTPStore from a URL.</p> <p>This is an alias of <code>HTTPStore.__init__</code>.</p>"},{"location":"api/store/local/","title":"Local","text":""},{"location":"api/store/local/#obstore.store.LocalStore","title":"obstore.store.LocalStore","text":"<p>An ObjectStore interface to local filesystem storage.</p> <p>Can optionally be created with a directory prefix.</p> <pre><code>from pathlib import Path\n\nstore = LocalStore()\nstore = LocalStore(prefix=\"/path/to/directory\")\nstore = LocalStore(prefix=Path(\".\"))\n</code></pre>"},{"location":"api/store/local/#obstore.store.LocalStore.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix: Path | None\n</code></pre> <p>Get the prefix applied to all operations in this store, if any.</p>"},{"location":"api/store/local/#obstore.store.LocalStore.__init__","title":"__init__","text":"<pre><code>__init__(\n    prefix: str | Path | None = None,\n    *,\n    automatic_cleanup: bool = False,\n    mkdir: bool = False,\n) -&gt; None\n</code></pre> <p>Create a new LocalStore.</p> <p>Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Use the specified prefix applied to all paths. Defaults to <code>None</code>.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>automatic_cleanup</code>               (<code>bool</code>)           \u2013            <p>if <code>True</code>, enables automatic cleanup of empty directories when deleting files. Defaults to False.</p> </li> <li> <code>mkdir</code>               (<code>bool</code>)           \u2013            <p>if <code>True</code> and <code>prefix</code> is not <code>None</code>, the directory at <code>prefix</code> will attempt to be created. Note that this root directory will not be cleaned up, even if <code>automatic_cleanup</code> is <code>True</code>.</p> </li> </ul>"},{"location":"api/store/local/#obstore.store.LocalStore.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str, *, automatic_cleanup: bool = False, mkdir: bool = False\n) -&gt; LocalStore\n</code></pre> <p>Construct a new LocalStore from a <code>file://</code> URL.</p> <p>Examples:</p> <p>Construct a new store pointing to the root of your filesystem: <pre><code>url = \"file:///\"\nstore = LocalStore.from_url(url)\n</code></pre></p> <p>Construct a new store with a directory prefix: <pre><code>url = \"file:///Users/kyle/\"\nstore = LocalStore.from_url(url)\n</code></pre></p>"},{"location":"api/store/memory/","title":"Memory","text":""},{"location":"api/store/memory/#obstore.store.MemoryStore","title":"obstore.store.MemoryStore","text":"<p>A fully in-memory implementation of ObjectStore.</p> <p>Create a new in-memory store: <pre><code>store = MemoryStore()\n</code></pre></p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/02/10/releasing-obstore-04/","title":"Releasing obstore 0.4!","text":"<p>Obstore is the simplest, highest-throughput Python interface to Amazon S3, Google Cloud Storage, and Azure Storage, powered by Rust.</p> <p>This post gives an overview of what's new in obstore version 0.4.</p> <p>Refer to the changelog for all updates.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#easier-store-creation-with-from_url","title":"Easier store creation with <code>from_url</code>","text":"<p>There's a new top-level <code>obstore.store.from_url</code> function, which makes it dead-simple to create a store from a URL.</p> <p>Here's an example of using it to inspect data from the Sentinel-2 open data bucket. <code>from_url</code> automatically infers that this is an S3 path and constructs an <code>S3Store</code>, which we can pass to <code>obstore.list_with_delimiter</code> and <code>obstore.get</code>.</p> <pre><code>import obstore as obs\nfrom obstore.store import from_url\n\n# The base path within the bucket to \"mount\" to\nurl = \"s3://sentinel-cogs/sentinel-s2-l2a-cogs/12/S/UF/2022/6/S2A_12SUF_20220601_0_L2A\"\n\n# Pass in store-specific parameters as keyword arguments\n# Here, we pass `skip_signature=True` because it's a public bucket\nstore = from_url(url, region=\"us-west-2\", skip_signature=True)\n\n# Print filenames in this directory\nprint([meta[\"path\"] for meta in obs.list_with_delimiter(store)[\"objects\"]])\n# ['AOT.tif', 'B01.tif', 'B02.tif', 'B03.tif', 'B04.tif', 'B05.tif', 'B06.tif', 'B07.tif', 'B08.tif', 'B09.tif', 'B11.tif', 'B12.tif', 'B8A.tif', 'L2A_PVI.tif', 'S2A_12SUF_20220601_0_L2A.json', 'SCL.tif', 'TCI.tif', 'WVP.tif', 'granule_metadata.xml', 'thumbnail.jpg', 'tileinfo_metadata.json']\n\n# Download thumbnail\nwith open(\"thumbnail.jpg\", \"wb\") as f:\n    f.write(obs.get(store, \"thumbnail.jpg\").bytes())\n</code></pre> <p>And voil\u00e0, we have a thumbnail of the Grand Canyon from space:</p> <p></p> <p><code>from_url</code> also supports typing overloads. So your type checker will raise an error if you try to mix AWS-specific and Azure-specific configuration.</p> <p>Nevertheless, for best typing support, we still suggest using one of the store-specific <code>from_url</code> constructors (such as <code>S3Store.from_url</code>) if you know the protocol. Then your type checker can infer the type of the returned store.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#pickle-support","title":"Pickle support","text":"<p>One of obstore's initial integration targets is zarr-python, which needs to load large chunked N-dimensional arrays from object storage. In our early benchmarking, we've found that the obstore-based backend can cut data loading times in half as compared to the standard fsspec-based backend.</p> <p>However, Zarr is commonly used in distributed execution environments like Dask, which needs to be able to move store instances between workers. We've implemented pickle support for store classes to unblock this use case. Read our pickle documentation for more info.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#enhanced-loading-of-aws-credentials-provisional","title":"Enhanced loading of AWS credentials (provisional)","text":"<p>By default, each store class expects to find credential information either in environment variables or in passed-in arguments. In the case of AWS, that means the default constructors will not look in file-based credentials sources.</p> <p>The provisional <code>S3Store._from_native</code> constructor uses the official AWS Rust configuration crate to find credentials on the file system. This integration is expected to also automatically refresh temporary credentials before expiration.</p> <p>This API is provisional and may change in the future. If you have any feedback, please open an issue.</p> <p>Obstore version 0.5 is expected to improve on extensible credentials by enabling users to pass in arbitrary credentials in a sync or async function callback.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#return-arrow-data-from-list_with_delimiter","title":"Return Arrow data from <code>list_with_delimiter</code>","text":"<p>By default, the <code>obstore.list</code> and <code>obstore.list_with_delimiter</code> APIs return standard Python <code>dict</code>s. However, if you're listing a large bucket, the overhead of materializing all those Python objects can become significant.</p> <p><code>obstore.list</code> and <code>obstore.list_with_delimiter</code> now both support a <code>return_arrow</code> keyword parameter. If set to <code>True</code>, an Arrow <code>RecordBatch</code> or <code>Table</code> will be returned, which is both faster and more memory efficient.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#access-configuration-values-back-from-a-store","title":"Access configuration values back from a store","text":"<p>There are new attributes, such as <code>config</code>, <code>client_options</code>, and <code>retry_config</code> for accessing configuration parameters back from a store instance.</p> <p>This example uses an <code>S3Store</code> but the same behavior applies to <code>GCSStore</code> and <code>AzureStore</code> as well.</p> <pre><code>from obstore.store import S3Store\n\nstore = S3Store.from_url(\n    \"s3://ookla-open-data/parquet/performance/type=fixed/year=2024/quarter=1\",\n    region=\"us-west-2\",\n    skip_signature=True,\n)\nnew_store = S3Store(\n    config=store.config,\n    prefix=store.prefix,\n    client_options=store.client_options,\n    retry_config=store.retry_config,\n)\nassert store.config == new_store.config\nassert store.prefix == new_store.prefix\nassert store.client_options == new_store.client_options\nassert store.retry_config == new_store.retry_config\n</code></pre>"},{"location":"blog/2025/02/10/releasing-obstore-04/#open-remote-objects-as-file-like-readers-or-writers","title":"Open remote objects as file-like readers or writers","text":"<p>This version adds support for opening remote objects as a file-like reader or writer.</p> <pre><code>import os\n\nimport obstore as obs\nfrom obstore.store import MemoryStore\n\n# Create an in-memory store\nstore = MemoryStore()\n\n# Iteratively write to the file\nwith obs.open_writer(store, \"new_file.csv\") as writer:\n    writer.write(b\"col1,col2,col3\\n\")\n    writer.write(b\"a,1,True\\n\")\n    writer.write(b\"b,2,False\\n\")\n    writer.write(b\"c,3,True\\n\")\n\n\n# Open a reader from the file\nreader = obs.open_reader(store, \"new_file.csv\")\nfile_length = reader.seek(0, os.SEEK_END)\nprint(file_length) # 43\nreader.seek(0)\nbuf = reader.read()\nprint(buf)\n# Bytes(b\"col1,col2,col3\\na,1,True\\nb,2,False\\nc,3,True\\n\")\n</code></pre> <p>See <code>obstore.open_reader</code> and <code>obstore.open_writer</code> for more details. An async file-like reader and writer is also provided, see <code>obstore.open_reader_async</code> and <code>obstore.open_writer_async</code>.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#benchmarking","title":"Benchmarking","text":"<p>Benchmarking is still ongoing, but early results have been very promising and we've added documentation about our progress so far.</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#new-examples","title":"New examples","text":"<p>We've worked to update the documentation with more examples! We now have examples for how to use obstore with FastAPI, MinIO, and tqdm.</p> <p>We've also worked to consolidate introductory documentation into the \"user guide\".</p>"},{"location":"blog/2025/02/10/releasing-obstore-04/#all-updates","title":"All updates","text":"<p>Refer to the changelog for all updates.</p>"},{"location":"dev/DEVELOP/","title":"Contributor Documentation","text":""},{"location":"dev/DEVELOP/#prerequisites","title":"Prerequisites","text":"<p>Install uv and Rust.</p>"},{"location":"dev/DEVELOP/#layout","title":"Layout","text":"<ul> <li><code>pyo3-object_store/</code>: Logic for constructing <code>object_store</code> instances lives here, so that it can potentially be shared with other Rust-Python libraries in the future.</li> <li><code>obstore/</code>: The primary Python-facing bindings of the <code>obstore</code> library. This re-exports the classes defined in <code>pyo3-object_store</code>. It also adds the top-level functions that form the <code>obstore</code> API.</li> <li><code>pyo3-bytes</code>: A wrapper of <code>bytes::Bytes</code> that is used inside <code>obstore</code> for zero-copy buffer exchange between Rust and Python but also is intended to be reusable for other Rust-Python libraries.</li> </ul>"},{"location":"dev/DEVELOP/#developing-obstore","title":"Developing obstore","text":"<p>From the top-level directory, run</p> <pre><code>uv run maturin develop -m obstore/Cargo.toml\n</code></pre> <p>this will compile <code>obstore</code> and add it to the uv-managed Python environment.</p> <p>If you wish to do any benchmarking, run</p> <pre><code>uv run maturin develop -m obstore/Cargo.toml --release\n</code></pre> <p>to compile <code>obstore</code> with release optimizations turned on.</p>"},{"location":"dev/DEVELOP/#tests","title":"Tests","text":"<p>All obstore tests should go into the top-level <code>tests</code> directory.</p>"},{"location":"dev/DEVELOP/#publishing","title":"Publishing","text":"<p>Push a new tag to the main branch of the format <code>py-v*</code>. A new version will be published to PyPI automatically.</p>"},{"location":"dev/DEVELOP/#documentation-website","title":"Documentation website","text":"<p>The documentation website is generated with <code>mkdocs</code> and <code>mkdocs-material</code>. You can serve the docs website locally with</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>Publishing documentation happens automatically via CI when a new tag is published of the format <code>py-v*</code>. It can also be triggered manually through the Github Actions dashboard on this page. Note that publishing docs manually is not advised if there have been new code additions since the last release as the new functionality will be associated in the documentation with the tag of the previous release. In this case, prefer publishing a new patch or minor release, which will publish both a new Python package and the new documentation for it.</p>"},{"location":"dev/functional-api/","title":"Functional API Design Choice","text":"<p>Last edited 2025-02-04.</p> <p>See further discussion in this issue.</p> <p>Obstore intentionally presents its main API as top-level functions. E.g. users must use the top level <code>obstore.put</code> function:</p> <pre><code>import obstore as obs\nfrom obstore.store import AzureStore\n\nstore = AzureStore()\nobs.put(store, ....)\n</code></pre> <p>instead of a method on the store itself:</p> <pre><code>import obstore as obs\nfrom obstore.store import AzureStore\n\nstore = AzureStore()\nstore.put(....)\n</code></pre> <p>This page documents the design decisions for this API.</p>"},{"location":"dev/functional-api/#store-specific-vs-generic-api","title":"Store-specific vs generic API","text":"<p>This presents a nice separation of concerns, in my opinion, between store-specific properties and a generic API that works for every <code>ObjectStore</code>.</p> <p>Python store classes such as <code>S3Store</code> have a few properties to access the store-specific configuration, e.g. <code>S3Store.config</code> accesses the S3 credentials. Anything that's a property/method of the store class is specific to that type of store. Whereas any top-level method should work on any store equally well.</p>"},{"location":"dev/functional-api/#simpler-rust-code","title":"Simpler Rust code","text":"<p>On the Rust side, each Python class is a separate <code>struct</code>. A pyo3 <code>#[pyclass]</code> can't implement a trait, so the only way to implement the same methods on multiple Rust structs without copy-pasting is by having a macro. That isn't out of the question, however it does hamper extensibility, and having one and only one way to call commands is simpler to maintain.</p>"},{"location":"dev/functional-api/#simpler-middlewares","title":"Simpler Middlewares","text":"<p>The <code>PrefixStore</code> concept has since been taken out, in favor of natively handling store prefixes, but this argument still holds for other potential middlewares in the future.</p> <p>In developmentseed/obstore!117 we added a binding for <code>PrefixStore</code>.  Because we use object store classes functionally, we only needed 20 lines of Rust code: github.com/developmentseed/obstore/blob/b40d59b4e060ba4fd3dc69468b3ba7da1149758e/pyo3-object_store/src/prefix.rs#L10-L25</p> <p>If we exposed methods on an <code>S3Store</code>, then those methods would be lost whenever you apply a middleware around it, such as <code>PrefixStore(S3Store(...))</code>. So we'd have to ensure those same methods are also installed onto every middleware or other wrapper.</p>"},{"location":"dev/functional-api/#external-ffi-for-objectstore","title":"External FFI for ObjectStore","text":"<p>There was recently discussion on Discord about the merits of having a stable FFI for <code>ObjectStore</code>. If this comes to fruition in the future, then by having a functional API we could seamlessly use third party ObjectStore implementations or middlewares, with no Python overhead.</p> <p>I use a similar functional API in other Python bindings, especially in cases with zero-copy FFI, such as kylebarron.dev/geo-index/latest/api/rtree/#geoindex_rs.rtree.search (where the spatial index is passed in as the first argument instead) and kylebarron.dev/arro3/latest/api/compute/#arro3.compute.cast where the <code>cast</code> is not a method on the Arrow Array.</p>"},{"location":"dev/functional-api/#smaller-core-for-third-party-rust-bindings","title":"Smaller core for third-party Rust bindings","text":"<p>This repo has twin goals:</p> <ol> <li>Provide bindings to <code>object_store</code> for Python users who want a Python API.</li> <li>Make it easier for other Rust developers who are making Python bindings, who are using <code>object_store</code> on the Rust side already, and who want to expose <code>ObjectStore</code> bindings to Python in their own projects.</li> </ol> <p>The first goal is served by the <code>obstore</code> Python package and the second is served by the <code>pyo3-object_store</code> Rust crate. The latter provides builders for <code>S3Store</code>, <code>AzureStore</code>, <code>GCSStore</code>, which means that those third party Rust-Python bindings can have code as simple as:</p> <pre><code>#[pyfunction]\nfn use_object_store(store: PyObjectStore) {\n    let store: Arc&lt;dyn ObjectStore&gt; = store.into_inner();\n}\n</code></pre> <p>Those third party bindings don't need the Python bindings to perform arbitrary <code>get</code>, <code>list</code>, <code>put</code> from Python. Instead, they use this to access a raw <code>Arc&lt;dyn ObjectStore&gt;</code> from the Rust side.</p> <p>You'll notice that <code>S3Store</code>, <code>GCSStore</code>, and <code>AzureStore</code> aren't in the <code>obstore</code> library; they're in <code>pyo3-object_store</code>. We can't add methods to a pyclass from an external crate, so we couldn't leave those builders in <code>pyo3_object_store</code> while having the Python-facing operations live in <code>obstore</code>. Instead we'd have to put the entire content of the Python bindings in the <code>pyo3-object_store</code> crate. Then this would expose whatever class methods from the <code>obstore</code> Python API onto any external Rust-Python library that uses <code>pyo3-object_store</code>. I don't want to leak this abstraction nor make that public to other Rust consumers.</p>"},{"location":"dev/overridden-defaults/","title":"Overridden Defaults","text":"<p>In general, we wish to follow the upstream <code>object_store</code> as closely as possible, which should reduce the maintenance overhead here.</p> <p>However, there are occasionally places where we want to diverge from the upstream decision making, and we document those here.</p>"},{"location":"dev/overridden-defaults/#azure-cli","title":"Azure CLI","text":"<p>We always check for Azure CLI authentication as a fallback.</p> <p>If we stuck with the upstream <code>object_store</code> default, you would need to pass <code>use_azure_cli=True</code> to check for Azure CLI credentials.</p> <p>The Azure CLI is the second-to-last Azure authentication method checked checked. So this only changes the default behavior for people relying on instance authentication. For those people, they can still pass <code>use_azure_cli=False</code>.</p> <p>See upstream discussion here.</p>"},{"location":"dev/pickle/","title":"Pickle Implementation","text":"<p>Last edited 2025-02-04.</p> <p>Pickle support is important but hard to implement. It's important to support because it's commonly used from inside Dask and similar libraries to manage state across distributed workers.</p>"},{"location":"dev/pickle/#background","title":"Background","text":"<p>There are two ways to implement pickle support.</p> <ol> <li>Implementing <code>__getstate__</code> and <code>__setstate__</code>. The return value of <code>__getstate__</code> can be pretty much anything I think, and that gets passed back into <code>__setstate__</code> to be unpacked and set as the internal state.</li> <li>Implementing a constructor (tagged in Rust with <code>#[new]</code>) and <code>__getnewargs_ex__</code>. <code>__getnewargs_ex__</code> must return a tuple of <code>(args: tuple, kwargs: dict)</code>, which can be passed to the <code>#[new]</code> constructor. This can be simpler when you already have a <code>#[new]</code> implemented and when the parameters into that <code>#[new]</code> function are easily serializable. However it might require (I wonder if there's a way to recursively pickle things?)</li> </ol> <p>We can't extract the configuration out from a \"finished\" <code>object_store</code> instance like an <code>AmazonS3</code>. We could extract some configuration out of a builder instance like <code>AmazonS3Builder</code>, but then every time we use that class we'd have to call <code>build()</code> on the Rust side, which would probably significantly hurt performance. Therefore, the only(?) possible way to implement pickle support inside obstore is to persist the store configuration separately inside the <code>#[pyclass]</code>.</p>"},{"location":"dev/pickle/#implementation","title":"Implementation","text":"<ul> <li>Store configuration information a second time in the <code>#[pyclass]</code> of each store. (It's already implicitly stored by the underlying rust <code>ObjectStore</code> instance.)</li> <li>Handle prefixes automatically within each Python store class.</li> <li>Implement <code>__getnewargs_ex__</code>, returning the parameters passed to <code>#[new]</code></li> <li>Implement <code>IntoPyObject</code> for each configuration object: the store-specific config, client options, and retry options</li> </ul>"},{"location":"dev/pickle/#benefits","title":"Benefits","text":"<ul> <li>It should be relatively straightforward to implement for any of the raw stores: <code>S3Store</code>, <code>GCSStore</code>, <code>AzureStore</code>, <code>LocalStore</code>, and <code>MemoryStore</code>.</li> <li>We're already validating the <code>PyAmazonS3Config</code>, <code>PyClientOptions</code>, and <code>PyRetryConfig</code>, so it isn't that much extra work just to store those on the Rust class. So we can serialize them to Python objects in <code>__getnewargs_ex__</code> and then have Python automatically pass them to <code>#[new]</code>.</li> <li>Using <code>__getnewargs_ex__</code> means we don't need to add serde support; we can use <code>IntoPyObject</code> and <code>FromPyObject</code> for all (de)serialization and only have a single code path.</li> <li>We can persist all of the store-specific config, the client options, and the retry config, so the pickled instances should be exactly the same as the original instances.</li> <li>Supports any of the builder classmethods, e.g. <code>from_env</code>, <code>from_url</code>, etc,</li> <li>Most of the time, storing configuration information should be just a few strings. So ideally it'll increase memory usage only slightly, and won't affect runtime performance otherwise (assuming you reuse a store instead of creating a new one each time).</li> <li>Since we don't allow the store classes to be mutated after creation from Python, there's no risk of the two copies of the configuration getting out of sync.</li> </ul>"},{"location":"dev/pickle/#drawbacks","title":"Drawbacks","text":"<ul> <li>Because <code>url</code> has deferred parsing in the <code>object_store</code> builders, we need to special-case <code>url</code> handling. Naturally, passing <code>url</code> to a store with <code>with_url</code> means that <code>object_store</code> doesn't actually parse the URL until the <code>build()</code> method, and at that point we can no longer access config information from the built <code>AmazonS3</code>. Without special-casing this URL handling, pickling would fail for instances created from <code>from_url</code>.</li> </ul> <p>Therefore, we handle this by vendoring the small amount of URL parsing from upstream. So we apply the URL parsing onto our config <code>HashMap</code>s and then apply those to the builder. So our configs and those used by the raw stores stay in sync. See developmentseed/obstore!209. - Unclear how to support middleware, including <code>PrefixStore</code>, because those have to support an arbitrary wrapped object. Is there a way to recursively pickle and unpickle the thing it's wrapping?   - (Implemented) If we can't find a way to support pickling of arbitrary middleware, we could alternatively use a <code>PrefixStore</code> internally and automatically inside an <code>S3Store</code>, <code>GCSStore</code>, <code>AzureStore</code> (NOTE: we should maybe benchmark the overhead a <code>PrefixStore</code> causes, in case it's something we don't want to force on everyone? Well, if the <code>S3Store</code> stored an arbitrary <code>Arc&lt;dyn ObjectStore&gt;</code> then we could prefix when asked for and not prefix when not asked for, but maybe that would conflict with signing, if that requires a raw <code>object_store::AmazonS3</code> instance?). Alternatively, we could create the <code>PrefixStore</code> on demand, since it looks virtually free. - We don't currently implement pickle support for <code>MemoryStore</code>, as we don't have a way to serialize the memory state across workers.</p>"},{"location":"examples/fastapi/","title":"FastAPI","text":"<p>FastAPI is a modern, high-performance, web framework for building APIs with Python based on standard Python type hints.</p> <p>It's easy to integrate obstore with FastAPI routes, where you want to download a file from an object store and return it to the user.</p> <p>FastAPI has a <code>StreamingResponse</code>, which neatly integrates with <code>BytesStream</code> to stream the response to the user.</p> <p>Note</p> <p>This example is also available on Github if you'd like to test it out locally.</p> <p>First, import <code>fastapi</code> and <code>obstore</code> and create the FastAPI application.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\nimport obstore as obs\nfrom obstore.store import HTTPStore, S3Store\n\napp = FastAPI()\n</code></pre> <p>Next, we can add our route. Here, we create a simple route that fetches a small Parquet file from an HTTP url and returns it to the user.</p> <p>Passing <code>resp</code> directly to <code>StreamingResponse</code> calls <code>GetResult.stream()</code> under the hood and thus uses the default chunking behavior of <code>GetResult.stream()</code>.</p> <pre><code>@app.get(\"/example.parquet\")\nasync def download_example() -&gt; StreamingResponse:\n    store = HTTPStore.from_url(\"https://raw.githubusercontent.com\")\n    path = \"opengeospatial/geoparquet/refs/heads/main/examples/example.parquet\"\n\n    # Make the request. This only begins the download; it does not wait for the\n    # download to finish.\n    resp = await obs.get_async(store, path)\n    return StreamingResponse(resp)\n</code></pre> <p>You may also want to customize the chunking behavior of the async stream. To do this, call <code>GetResult.stream()</code> before passing to <code>StreamingResponse</code>.</p> <pre><code>@app.get(\"/large.parquet\")\nasync def large_example() -&gt; StreamingResponse:\n    # Example large Parquet file hosted in AWS open data\n    store = S3Store(\"ookla-open-data\", region=\"us-west-2\", skip_signature=True)\n    path = \"parquet/performance/type=fixed/year=2024/quarter=1/2024-01-01_performance_fixed_tiles.parquet\"\n\n    # Note: for large file downloads you may need to increase the timeout in\n    # the client configuration\n    resp = await obs.get_async(store, path)\n\n    # Example: Ensure the stream returns at least 5MB of data in each chunk.\n    return StreamingResponse(resp.stream(min_chunk_size=5 * 1024 * 1024))\n</code></pre> <p>Note that here FastAPI wraps <code>starlette.responses.StreamingResponse</code>. So any web server that uses Starlette for responses can use this same code.</p>"},{"location":"examples/minio/","title":"Minio","text":"<p>MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license. It's often used for testing or self-hosting S3-compatible storage.</p> <p>We can run minio locally using docker:</p> <pre><code>docker run -p 9000:9000 -p 9001:9001 \\\n    quay.io/minio/minio server /data --console-address \":9001\"\n</code></pre> <p><code>obstore</code> isn't able to create a bucket, so we need to do that manually. We can do that through the minio web UI. After running the above docker command, go to localhost:9001. Then log in with the credentials <code>minioadmin</code>, <code>minioadmin</code> for username and password. Then click \"Create a Bucket\" and create a bucket with the name <code>\"test-bucket\"</code>.</p> <p>Now we can create an <code>S3Store</code> to interact with minio:</p> <pre><code>import obstore as obs\nfrom obstore.store import S3Store\n\nstore = S3Store(\n    \"test-bucket\",\n    aws_endpoint=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    aws_virtual_hosted_style_request=False,\n    client_options={\"allow_http\": True},\n)\n\n# Add files\nobs.put(store, \"a.txt\", b\"foo\")\nobs.put(store, \"b.txt\", b\"bar\")\nobs.put(store, \"c/d.txt\", b\"baz\")\n\n# List files\nfiles = obs.list(store).collect()\nprint(files)\n\n# Download a file\nresp = obs.get(store, \"a.txt\")\nprint(resp.bytes())\n\n# Delete a file\nobs.delete(store, \"a.txt\")\n</code></pre> <p>There's a full example in the obstore repository.</p>"},{"location":"examples/tqdm/","title":"tqdm (Progress Bar)","text":"<p>tqdm provides an interactive progress bar for Python.</p> <p></p> <p>It's easy to wrap obstore downloads with a tqdm progress bar:</p> <pre><code>from obstore.store import HTTPStore\nfrom tqdm import tqdm\n\nstore = HTTPStore.from_url(\"https://ookla-open-data.s3.us-west-2.amazonaws.com\")\npath = \"parquet/performance/type=fixed/year=2019/quarter=1/2019-01-01_performance_fixed_tiles.parquet\"\nresponse = obs.get(store, path)\nfile_size = response.meta[\"size\"]\nwith tqdm(total=file_size) as pbar:\n    for bytes_chunk in response:\n        # Do something with buffer\n        pbar.update(len(bytes_chunk))\n</code></pre> <p>Or, if you're using the async API:</p> <pre><code>from obstore.store import HTTPStore\nfrom tqdm import tqdm\n\nstore = HTTPStore.from_url(\"https://ookla-open-data.s3.us-west-2.amazonaws.com\")\npath = \"parquet/performance/type=fixed/year=2019/quarter=1/2019-01-01_performance_fixed_tiles.parquet\"\nresponse = await obs.get_async(store, path)\nfile_size = response.meta[\"size\"]\nwith tqdm(total=file_size) as pbar:\n    async for bytes_chunk in response:\n        # Do something with buffer\n        pbar.update(len(bytes_chunk))\n</code></pre> <p>There's a full example in the obstore repository.</p>"},{"location":"troubleshooting/aws/","title":"Troubleshooting Amazon S3","text":""},{"location":"troubleshooting/aws/#region-required","title":"Region required","text":"<p>All requests to S3 must include the region. An error will occur on requests when you don't pass the correct region.</p> <p>For example, trying to list the <code>sentinel-cogs</code> open bucket without passing a region will fail:</p> <pre><code>import obstore as obs\nfrom obstore.store import S3Store\n\nstore = S3Store(\"sentinel-cogs\", skip_signature=True)\nnext(obs.list(store))\n</code></pre> <p>raises</p> <pre><code>GenericError: Generic S3 error: Error performing list request:\nReceived redirect without LOCATION, this normally indicates an incorrectly\nconfigured region\n</code></pre> <p>We can fix this by passing the correct region:</p> <pre><code>import obstore as obs\nfrom obstore.store import S3Store\n\nstore = S3Store(\"sentinel-cogs\", skip_signature=True, region=\"us-west-2\")\nnext(obs.list(store))\n</code></pre> <p>this prints:</p> <pre><code>[{'path': 'sentinel-s2-l2a-cogs/1/C/CV/2018/10/S2B_1CCV_20181004_0_L2A/AOT.tif',\n  'last_modified': datetime.datetime(2020, 9, 30, 20, 25, 56, tzinfo=datetime.timezone.utc),\n  'size': 50510,\n  'e_tag': '\"2e24c2ee324ea478f2f272dbd3f5ce69\"',\n  'version': None},\n...\n</code></pre>"},{"location":"troubleshooting/aws/#inferring-the-bucket-region","title":"Inferring the bucket region","text":"<p>Note that it's possible to infer the S3 bucket region from an arbitrary <code>HEAD</code> request.</p> <p>Here, we show an example of using <code>requests</code> to find the bucket region, but you can use any HTTP client:</p> <pre><code>import requests\n\ndef find_bucket_region(bucket_name: str) -&gt; str:\n    resp = requests.head(f\"https://{bucket_name}.s3.amazonaws.com\")\n    return resp.headers[\"x-amz-bucket-region\"]\n</code></pre> <p>Applying this to our previous example, we can use this to find the region of the <code>sentinel-cogs</code> bucket:</p> <pre><code>find_bucket_region(\"sentinel-cogs\")\n# 'us-west-2'\n</code></pre> <p>Or we can pass this directly into the region:</p> <pre><code>bucket_name = \"sentinel-cogs\"\nstore = S3Store(\n    bucket_name, skip_signature=True, region=find_bucket_region(bucket_name)\n)\n</code></pre> <p>Finding the bucket region in this way works both for public and non-public buckets.</p> <p>This <code>HEAD</code> request can also tell you if the bucket is public or not by checking the HTTP response code (accessible in <code>requests</code> via <code>resp.status_code</code>):</p> <ul> <li><code>200</code>: public bucket.</li> <li><code>403</code>: private bucket.</li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/release/","title":"Release","text":""}]}